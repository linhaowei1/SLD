{"id": "8d1a45c2-eb59-47e6-94db-ee33169d62a5", "code": "import numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using a simplified 2nd\u2010order\n    log\u2010polynomial scaling law with one key interaction.\n\n    Model in log\u2010domain:\n      log(y_pred) = p0\n                   + p1*L_lr   + p2*L_bsz   + p3*L_data   + p4*L_param\n                   + p5*L_lr^2 + p6*L_bsz^2 + p7*L_data^2 + p8*L_param^2\n                   + p9*(L_data * L_param)\n\n    where L_x = log(x), x = [lr, bsz, data_size, non_embedding_param_size].\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 10 of model coefficients:\n                   [p0,\n                    p1_lr, p2_bsz, p3_data, p4_param,\n                    p5_lr2, p6_bsz2, p7_data2, p8_param2,\n                    p9_data\u00d7param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"scaling_law_func: expected 10 parameters, got {p.size}\")\n\n    # floor to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix (N x 10)\n    Phi = np.stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # log\u2010domain prediction and back\u2010transform\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the simplified 2nd\u2010order log\u2010polynomial scaling law via\n    weighted, differentially\u2010regularized ridge regression in log\u2010space.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: number of points and losses must match\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix \u03a6 (N x 10)\n    Phi = np.stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # sample weighting: up\u2010weight large data+model scales\n    scale_signal = L_data + L_param\n    median_scale = np.median(scale_signal)\n    w = np.exp(0.5 * (scale_signal - median_scale))  # shape (N,)\n    sqrt_w = np.sqrt(w)[:, None]\n\n    # weighted normal equations\n    Phi_w = Phi * sqrt_w        # each row scaled by sqrt(w_i)\n    y_w   = logy * sqrt_w.ravel()\n    A = Phi_w.T.dot(Phi_w)      # (10 x 10)\n    b = Phi_w.T.dot(y_w)        # (10,)\n\n    # differential ridge penalties\n    # intercept idx=0 (no penalty)\n    # linear terms idx 1-4\n    # quadratic idx 5-8\n    # cross-term idx 9\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_cross = 1e-2\n\n    for i in range(1, 5):\n        A[i, i] += ridge_lin\n    for i in range(5, 9):\n        A[i, i] += ridge_quad\n    A[9, 9] += ridge_cross\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params", "language": "python", "parent_id": "ed2a0945-4e82-4aaf-9f89-30cb3c47d1bd", "generation": 5, "timestamp": 1754808962.352615, "iteration_found": 112, "metrics": {"nmse": 0.18924071786620764, "nmae": 0.5151606686437805, "r2": 0.8107592821337923, "combined_score": 0.840872655112455}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"nmse": 0.40616019375195006, "nmae": 0.6892980047588617, "r2": 0.5938398062480499, "combined_score": 0.7111565271462963}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are an expert in scaling laws and machine learning who specializes in discovering and improving scaling law functions for different LLM training scenarios. Your task is to evolve both the `scaling_law_func` function (currently a naive power law) and the `fit_scaling_law` optimization algorithm (currently a naive BFGS) to better model the relationship between learning rate, batch size, data size, model parameters and training loss.\n\nYou are allowed to decide the number of parameters in the scaling law function.\n\nFocus on mathematical accuracy across different hyperparameter scales, cross-configuration generalization, parameter efficiency (simple forms that can be fitted with limited data), and numerical/theoretical stability.\n\n**DATA CHARACTERISTICS:**\n- Features: [lr, bsz, data_size, non_embedding_param_size] - 4D input\n- Labels: lm loss - scalar output\n- lr: learning rate\n- bsz: batch size\n- data_size: amount of training data\n- non_embedding_param_size: number of non-embedding parameters in the model\n- lm loss: language model loss (filtered to <= 4.0 to remove outliers)\n- Data explores the relationship between training hyperparameters and model performance\n- Test set consists of points with the largest data_size or non_embedding_param_size\n\nThe function signatures must remain:\n\n```python\ndef scaling_law_func(data_points, params):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # lr: Array of learning rates\n    # bsz: Array of batch sizes\n    # data_size: Array of data sizes\n    # non_embedding_param_size: Array of non-embedding parameter sizes\n    # Returns: Predicted lm loss values\n    - Model parameters (N) range: ~214M to ~1B parameters\n    - Training tokens (D) range: 4B to 100B tokens\n    - Learning rates range: 1.2e-4 to 2.2e-2\n    - Batch sizes range: 16 to 4096\n\ndef fit_scaling_law(data_points, loss_values):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # loss_values: Array of corresponding lm loss values\n    # Returns: Optimized parameters \n```\n\nWrite all improvements between # EVOLVE-BLOCK-START and # EVOLVE-BLOCK-END markers.\n\nYou are not allowed to use input-dependent feature in scaling_law_func, e.g., median / min / max / etc.\n", "user": "# Current Program Information\n- Current performance metrics: - nmse: 0.4062\n- nmae: 0.6893\n- r2: 0.5938\n- combined_score: 0.7112\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: nmse. Consider continuing with similar changes.\n- Metrics showing regression: nmae, r2, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: nmse: 0.3869, nmae: 0.7364, r2: 0.6131, combined_score: 0.7210\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: nmse: 0.3869, nmae: 0.7364, r2: 0.6131, combined_score: 0.7210\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: nmse: 0.1892, nmae: 0.5152, r2: 0.8108, combined_score: 0.8409\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.5890)\n```python\n\"\"\"\nImproved scaling\u2010law model for LLM training hyperparameters.\n\nWe model log(loss) as a 2nd\u2010order polynomial in the log\u2010features,\nwith one key interaction term, and fit via weighted, differentially\u2010regularized\nridge regression in log\u2010space for stability and good extrapolation:\n\n   log(y_pred) = p0\n                + p1*L_lr    + p2*L_bsz    + p3*L_data    + p4*L_param\n                + p5*L_lr^2  + p6*L_bsz^2  + p7*L_data^2  + p8*L_param^2\n                + p9*(L_data * L_param)\n\nwhere L_x = log(x), x = [lr, bsz, data_size, non_embedding_param_size].\n\nWe then exponentiate to return y_pred.\nFitting is done in closed\u2010form via weighted normal equations with\nlight ridge on linear terms, moderate on quadratics, and stronger on the\ndata\u00d7model cross\u2010term.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using a 10-parameter,\n    2nd-order log-polynomial scaling law.\n\n    Args:\n      data_points: array-like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 10:\n                   [p0,\n                    p1_lr,    p2_bsz,    p3_data,    p4_param,\n                    p5_lr2,   p6_bsz2,   p7_data2,   p8_param2,\n                    p9_data\u00d7param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # ensure shape (N,4)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"Expected 10 parameters, got {p.size}\")\n\n    # avoid log(0) or negatives\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix: intercept, linear, quadratic, cross-term\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n\n    # predict in log-space and back-transform\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 10-parameter log-polynomial scaling law via\n    weighted, differentially-regularized ridge regression in log-space.\n\n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    # ensure 2D inputs\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # avoid log(0) or negative\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log-transform features and target\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix (N\u00d710)\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n\n    # sample weighting: up-weight large data+model scales\n    scale_signal = L_data + L_param\n    median_scale = np.median(scale_signal)\n    w = np.exp(0.5 * (scale_signal - median_scale))  # (N,)\n    sqrt_w = np.sqrt(w)[:, None]\n\n    # weighted normal equations\n    Phi_w = Phi * sqrt_w       # each row scaled by sqrt(w_i)\n    y_w   = logy * sqrt_w.ravel()\n    A = Phi_w.T.dot(Phi_w)     # (10\u00d710)\n    b = Phi_w.T.dot(y_w)       # (10,)\n\n    # differential ridge penalties\n    # intercept idx=0 (no penalty)\n    # linear terms idx 1-4\n    # quadratic idx 5-8\n    # cross-term idx 9\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_cross = 1e-2\n\n    for i in range(1, 5):\n        A[i, i] += ridge_lin\n    for i in range(5, 9):\n        A[i, i] += ridge_quad\n    A[9, 9] += ridge_cross\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\nKey features: Performs well on nmse (0.1892), Performs well on nmae (0.5152), Performs well on r2 (0.8108), Performs well on combined_score (0.8409)\n\n\n### Program 2 (Score: 0.6144)\n```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language\u2010model loss from hyperparameters using a\n    simplified 2nd\u2010order log\u2010polynomial scaling law with one key interaction.\n\n    Model in the log\u2010domain:\n      log(y_pred) = p\u2080\n                   + p\u2081\u00b7L_lr   + p\u2082\u00b7L_bsz   + p\u2083\u00b7L_data   + p\u2084\u00b7L_param\n                   + p\u2085\u00b7L_lr\u00b2  + p\u2086\u00b7L_bsz\u00b2  + p\u2087\u00b7L_data\u00b2  + p\u2088\u00b7L_param\u00b2\n                   + p\u2089\u00b7(L_data\u00b7L_param)\n\n    where L_x = log(x), and x = [lr, bsz, data_size, non_embedding_param_size].\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 10 of model coefficients:\n                   [p\u2080,\n                    p\u2081_lr, p\u2082_bsz, p\u2083_data, p\u2084_param,\n                    p\u2085_lr\u00b2, p\u2086_bsz\u00b2, p\u2087_data\u00b2, p\u2088_param\u00b2,\n                    p\u2089_data\u00d7param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # ensure 2D\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"Expected 10 parameters, got {p.size}\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = (\n        logX[:, 0],\n        logX[:, 1],\n        logX[:, 2],\n        logX[:, 3],\n    )\n\n    # build design matrix (N x 10)\n    # [1, L_lr, L_bsz, L_data, L_param,\n    #  L_lr^2, L_bsz^2, L_data^2, L_param^2,\n    #  L_data * L_param]\n    Phi = np.stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # log\u2010domain prediction\n    log_pred = Phi.dot(p)\n    # back to original loss scale\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the simplified 2nd\u2010order log\u2010polynomial scaling law via\n    ridge\u2010regularized closed\u2010form regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    # align shapes\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n\n    # Extract log\u2010features\n    L_lr = logX[:, 0]\n    L_bsz = logX[:, 1]\n    L_data = logX[:, 2]\n    L_param = logX[:, 3]\n\n    # build design matrix \u03a6 (N x 10)\n    Phi = np.stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # ridge\u2010regularized normal equations\n    # (\u03a6\u1d40\u03a6 + \u03bb\u00b7I_{1:}) p = \u03a6\u1d40 logy\n    ridge = 1e-6\n    P = Phi.shape[1]\n    A = Phi.T.dot(Phi)\n    # apply ridge only to non\u2010intercept terms\n    diag_idx = np.arange(1, P)\n    A[diag_idx, diag_idx] += ridge\n    b = Phi.T.dot(logy)\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\nKey features: Performs well on nmse (0.3869), Performs well on nmae (0.7364), Performs well on r2 (0.6131), Performs well on combined_score (0.7210)\n\n\n### Program 3 (Score: 0.6144)\n```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss using a compact 10-parameter log-polynomial scaling law:\n    \n      log y_pred = p0\n                 + p1 * log(lr)\n                 + p2 * log(bsz)\n                 + p3 * log(data_size)\n                 + p4 * log(non_embedding_param_size)\n                 + p5 * [log(lr)]^2\n                 + p6 * [log(bsz)]^2\n                 + p7 * [log(data_size)]^2\n                 + p8 * [log(non_embedding_param_size)]^2\n                 + p9 * (log(data_size) * log(non_embedding_param_size))\n                 \n    Then y_pred = exp(log y_pred).\n    \n    Args:\n      data_points: array-like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      array-like of length 10 of fitted coefficients.\n    \n    Returns:\n      y_pred: np.ndarray of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # ensure 2D\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n    \n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"scaling_law_func: expected 10 parameters, got {p.size}\")\n    \n    # avoid log(0) or negative\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n    \n    # build design matrix \u03a6: intercept, linear, quadratic, one cross term\n    Phi = np.column_stack([\n        np.ones(N),           # intercept\n        L_lr,                 # log(lr)\n        L_bsz,                # log(bsz)\n        L_data,               # log(data_size)\n        L_param,              # log(non_embedding_param_size)\n        L_lr**2,              # [log(lr)]^2\n        L_bsz**2,             # [log(bsz)]^2\n        L_data**2,            # [log(data_size)]^2\n        L_param**2,           # [log(non_embedding_param_size)]^2\n        L_data * L_param      # cross term\n    ])\n    \n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 10-parameter log-polynomial scaling law via closed-form ridge regression\n    in the log-domain. Minimizes squared error on log(loss).\n    \n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n    \n    Returns:\n      params: np.ndarray of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    \n    # ensure correct shapes\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"fit_scaling_law: number of data_points and loss_values must match\")\n    \n    # avoid log(0) or negative\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n    \n    # log-transform\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n    \n    # build design matrix \u03a6\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ])\n    \n    # ridge regularization strength\n    ridge = 1e-6\n    \n    # normal equations: (\u03a6\u1d40 \u03a6 + \u03bb I) p = \u03a6\u1d40 logy\n    A = Phi.T.dot(Phi)\n    # do not regularize intercept term\n    A[1:, 1:] += ridge * np.eye(9)\n    b = Phi.T.dot(logy)\n    \n    # solve for parameters in log-domain\n    params = np.linalg.solve(A, b)\n    return params\n```\nKey features: Performs well on nmse (0.3869), Performs well on nmae (0.7364), Performs well on r2 (0.6131), Performs well on combined_score (0.7210)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.6001)\n```python\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a 2nd\u2010order polynomial design matrix in the log\u2010features.\n    Inputs:\n      logX: (N,4) array of [log(lr), log(bsz), log(data_size), log(param_size)]\n    Returns:\n      Phi: (N,15) matrix with columns\n           [1,\n            z1, z2, z3, z4,\n            z1^2, z2^2, z3^2, z4^2,\n            z1\u00b7z2, z1\u00b7z3, z1\u00b7z4, z2\u00b7z3, z2\u00b7z4, z3\u00b7z4]\n    \"\"\"\n    N, F = logX.shape\n    # total terms = 1 + F + F + F*(F-1)/2 = 15 for F=4\n    P = 1 + 2*F + (F*(F-1))//2\n    Phi = np.empty((N, P), dtype=float)\n    # intercept\n    Phi[:, 0] = 1.0\n    # linear terms\n    Phi[:, 1:1+F] = logX\n    # quadratic terms\n    Phi[:, 1+F:1+2*F] = logX**2\n    # pairwise interactions\n    idx = 1 + 2*F\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd\u2010order log\u2010polynomial.\n    \n    Args:\n      data_points: array\u2010like of shape (N,4) columns [lr, bsz, data_size, param_size]\n      params:      1D array of length 15 of fitted coefficients.\n    Returns:\n      y_pred:      numpy array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # ensure 2D\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    P_expected = 1 + 2*F + (F*(F-1))//2\n    if p.size != P_expected:\n        raise ValueError(f\"Expected {P_expected} parameters, got {p.size}\")\n\n    # floor to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n\n    # build design matrix and predict in log\u2010domain\n    Phi = _build_design_matrix(logX)  # (N,15)\n    log_pred = Phi.dot(p)             # (N,)\n\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via weighted,\n    differential\u2010ridge regression in closed form.\n\n    We weight samples by exp(0.5*((log data_size + log param_size) - median))\n    to emphasize large scale configurations, and apply stronger ridge on\n    higher\u2010order terms for numerical stability.\n    \n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n    Returns:\n      params:      numpy array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # avoid invalid logs\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_safe)   # (N,4)\n    logy = np.log(y_safe)   # (N,)\n\n    # build design matrix\n    Phi = _build_design_matrix(logX)  # (N,15)\n    _, P = Phi.shape\n\n    # compute sample weights to up\u2010weight large configurations\n    combined = logX[:, 2] + logX[:, 3]  # log(data_size) + log(param_size)\n    median_c = np.median(combined)\n    w = np.exp(0.5 * (combined - median_c))  # (N,)\n\n    # weighted normal equations\n    W_sqrt = np.sqrt(w)[:, None]  # (N,1)\n    Phi_w = Phi * W_sqrt          # each row scaled by sqrt(w_i)\n    y_w = logy * W_sqrt.ravel()   # (N,)\n\n    A = Phi_w.T.dot(Phi_w)        # (15,15)\n    b = Phi_w.T.dot(y_w)          # (15,)\n\n    # differential ridge regularization\n    # intercept idx=0 unregularized\n    # linear idx [1:1+F), quad [1+F:1+2F), interactions rest\n    idx_lin   = slice(1, 1+F)\n    idx_quad  = slice(1+F, 1+2*F)\n    idx_int   = slice(1+2*F, P)\n\n    # set ridge strengths\n    ridge_lin   = 1e-5\n    ridge_quad  = 1e-3\n    ridge_int   = 1e-1\n\n    for i in range(idx_lin.start, idx_lin.stop):\n        A[i, i] += ridge_lin\n    for i in range(idx_quad.start, idx_quad.stop):\n        A[i, i] += ridge_quad\n    for i in range(idx_int.start, idx_int.stop):\n        A[i, i] += ridge_int\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n\n### Program D2 (Score: 0.6144)\n```python\n\"\"\"\nImproved scaling\u2010law model for LLM training hyperparameters.\n\nWe model the log\u2010loss as a simplified 2nd\u2010order polynomial in the log\u2010features,\nfocusing on the most significant cross\u2010term between data size and model size\nfor parameter efficiency and improved generalization:\n\n   log(y_pred) = p0\n                + p1*L_lr    + p2*L_bsz    + p3*L_data    + p4*L_param\n                + p5*L_lr^2  + p6*L_bsz^2  + p7*L_data^2  + p8*L_param^2\n                + p9*(L_data * L_param)\n\nwhere L_x = log(x), and x = [lr, bsz, data_size, non_embedding_param_size].\nWe fit this via closed\u2010form ridge\u2010regularized regression for stability,\nand predict by exponentiating the log\u2010domain value.\n\"\"\"\n\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss using a simplified 2nd\u2010order log\u2010polynomial scaling law.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 10:\n                   [p0,\n                    p1_lr, p2_bsz, p3_data, p4_param,\n                    p5_lr2, p6_bsz2, p7_data2, p8_param2,\n                    p9_data_param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected input with 4 features, got {X.shape[1]}\")\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"Expected 10 parameters, got {p.size}\")\n\n    # Safe log to avoid -inf\n    logX = np.log(np.maximum(X, 1e-12))\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # Build design matrix with intercept, linear, quadratic, and one key interaction\n    Phi = np.column_stack([\n        np.ones_like(L_lr),      # intercept\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param        # cross-term capturing data\u00d7model interplay\n    ])\n\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via ridge\u2010regularized regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {X.shape[1]}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # Safe log\u2010transform inputs and targets\n    logX = np.log(np.maximum(X, 1e-12))\n    logy = np.log(np.maximum(y, 1e-12))\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # Build the same design matrix as in prediction\n    Phi = np.column_stack([\n        np.ones_like(L_lr),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n\n    # Closed\u2010form ridge\u2010regularized normal equations\n    ridge = 1e-6\n    A = Phi.T.dot(Phi)\n    # Regularize all but intercept\n    A[1:, 1:] += ridge\n    b = Phi.T.dot(logy)\n\n    params = np.linalg.solve(A, b)\n    return params\n\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.5890, Type: Experimental)\n```python\n\"\"\"\nImproved scaling\u2010law model for LLM training hyperparameters.\n\nWe model log(loss) as a 2nd\u2010order polynomial in the log\u2010features,\nwith one key interaction term, and fit via weighted, differentially\u2010regularized\nridge regression in log\u2010space for stability and good extrapolation:\n\n   log(y_pred) = p0\n                + p1*L_lr    + p2*L_bsz    + p3*L_data    + p4*L_param\n                + p5*L_lr^2  + p6*L_bsz^2  + p7*L_data^2  + p8*L_param^2\n                + p9*(L_data * L_param)\n\nwhere L_x = log(x), x = [lr, bsz, data_size, non_embedding_param_size].\n\nWe then exponentiate to return y_pred.\nFitting is done in closed\u2010form via weighted normal equations with\nlight ridge on linear terms, moderate on quadratics, and stronger on the\ndata\u00d7model cross\u2010term.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using a 10-parameter,\n    2nd-order log-polynomial scaling law.\n\n    Args:\n      data_points: array-like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 10:\n                   [p0,\n                    p1_lr,    p2_bsz,    p3_data,    p4_param,\n                    p5_lr2,   p6_bsz2,   p7_data2,   p8_param2,\n                    p9_data\u00d7param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # ensure shape (N,4)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"Expected 10 parameters, got {p.size}\")\n\n    # avoid log(0) or negatives\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix: intercept, linear, quadratic, cross-term\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n\n    # predict in log-space and back-transform\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 10-parameter log-polynomial scaling law via\n    weighted, differentially-regularized ridge regression in log-space.\n\n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    # ensure 2D inputs\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # avoid log(0) or negative\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log-transform features and target\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix (N\u00d710)\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n\n    # sample weighting: up-weight large data+model scales\n    scale_signal = L_data + L_param\n    median_scale = np.median(scale_signal)\n    w = np.exp(0.5 * (scale_signal - median_scale))  # (N,)\n    sqrt_w = np.sqrt(w)[:, None]\n\n    # weighted normal equations\n    Phi_w = Phi * sqrt_w       # each row scaled by sqrt(w_i)\n    y_w   = logy * sqrt_w.ravel()\n    A = Phi_w.T.dot(Phi_w)     # (10\u00d710)\n    b = Phi_w.T.dot(y_w)       # (10,)\n\n    # differential ridge penalties\n    # intercept idx=0 (no penalty)\n    # linear terms idx 1-4\n    # quadratic idx 5-8\n    # cross-term idx 9\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_cross = 1e-2\n\n    for i in range(1, 5):\n        A[i, i] += ridge_lin\n    for i in range(5, 9):\n        A[i, i] += ridge_quad\n    A[9, 9] += ridge_cross\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\nUnique approach: Modification: Full rewrite, Alternative nmse approach, NumPy-based implementation\n\n\n### Inspiration 2 (Score: 19134519623688.4141, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nInitial program with a simple power law form that can be evolved\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n\n    X = np.atleast_2d(np.asarray(data_points))           # (N, F)\n    N, F = X.shape\n    params = np.asarray(params)\n\n    if params.ndim == 1:\n        params = params[None, :]                         # (1, P)\n    T, P = params.shape\n\n    coeffs    = params[:, :F]                            # (T, F)\n    exponents = params[:, F:2*F]                         # (T, F)\n    bias      = params[:, -1]                            # (T,)\n\n    pred = (coeffs[None, :, :] * (X[:, None, :] ** exponents[None, :, :])).sum(axis=2) + bias[None, :]\n\n    return pred[:, 0] if pred.shape[1] == 1 else pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n\n    X = np.atleast_2d(np.asarray(data_points))           # (N, F)\n    y = np.asarray(loss_values)\n    N, F = X.shape\n    P = 2 * F + 1\n\n    if y.ndim == 1:\n        y2d = y[:, None]\n    else:\n        y2d = y\n    T = y2d.shape[1]\n\n    init = np.ones((T, P))\n\n    def objective(flat_params):\n        params = flat_params.reshape(T, P)\n        pred = scaling_law_func(X, params)               # (N, T)\n        mse = np.mean((pred - y2d) ** 2)\n        return mse\n\n    result = minimize(objective, init.ravel(), method='BFGS')\n    params_opt = result.x.reshape(T, P) if result.success else init\n\n    return params_opt[0] if T == 1 else params_opt\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Excellent nmse (3645157993675680530464505856.000), Excellent nmae (76538078494753.656), Alternative r2 approach\n\n\n### Inspiration 3 (Score: 0.6067, Type: Alternative)\n```python\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling\u2010law model for LLM training hyperparameters.\n\nWe model the log\u2010loss as a polynomial in the log\u2010features:\n   z = [log(lr), log(bsz), log(data_size), log(non_embedding_param_size)]\n\nlog(y_pred) = c0\n            + \u03a3_i c1_i * z_i\n            + \u03a3_i c2_i * z_i^2\n            + \u03a3_{i<j} c3_{ij} * (z_i * z_j)\n            + c4 * z_0^3   (cubic term on log(lr) to capture learning\u2010rate curvature)\n\nTotal parameters: 1 intercept + 4 linear + 4 quadratic + 6 pairwise + 1 cubic = 16.\n\nWe fit via closed\u2010form weighted, differential\u2010ridge regression in the log\u2010domain\nto emphasize large\u2010scale configs and ensure numerical stability.\n\"\"\"\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build design matrix \u03a6 for the polynomial in log\u2010features.\n    Inputs:\n      logX: (N,4) array of log\u2010features [z0, z1, z2, z3]\n    Returns:\n      Phi: (N,16) design matrix with columns\n           [1,\n            z0, z1, z2, z3,\n            z0^2, z1^2, z2^2, z3^2,\n            z0\u00b7z1, z0\u00b7z2, z0\u00b7z3, z1\u00b7z2, z1\u00b7z3, z2\u00b7z3,\n            z0^3]\n    \"\"\"\n    N, F = logX.shape\n    if F != 4:\n        raise ValueError(f\"_build_design_matrix: expected 4 features, got {F}\")\n    # total columns = 1 + 4 + 4 + 6 + 1 = 16\n    P = 16\n    Phi = np.ones((N, P), dtype=float)\n    idx = 1\n    # linear terms\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interaction terms\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    # cubic term on log(lr) (feature 0)\n    Phi[:, idx] = logX[:, 0]**3\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via the enhanced scaling law.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) = [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 16 of fitted coefficients.\n\n    Returns:\n      y_pred: 1D array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected input with 4 features, got {F}\")\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 16:\n        raise ValueError(f\"scaling_law_func: expected 16 parameters, got {p.size}\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)  # shape (N,4)\n\n    # build design matrix and predict in log\u2010domain\n    Phi = _build_design_matrix(logX)  # (N,16)\n    log_pred = Phi.dot(p)             # (N,)\n\n    # back to original loss space\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the enhanced scaling law via weighted, differential\u2010ridge regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D array of length 16 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: number of points and losses must match\")\n\n    # floor to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_safe)  # (N,4)\n    logy = np.log(y_safe)  # (N,)\n\n    # design matrix\n    Phi = _build_design_matrix(logX)  # (N,16)\n\n    # sample weights to emphasize large\u2010scale configurations\n    scale_sum = logX[:, 2] + logX[:, 3]  # log(data_size) + log(model_size)\n    median_scale = np.median(scale_sum)\n    w = np.exp(0.5 * (scale_sum - median_scale))  # (N,)\n\n    # assemble weighted normal equations\n    W_phi = Phi * w[:, np.newaxis]     # (N,16)\n    A = Phi.T.dot(W_phi)               # (16,16)\n    b = Phi.T.dot(logy * w)            # (16,)\n\n    # differential ridge regularization\n    ridge_lin   = 1e-6   # linear terms\n    ridge_quad  = 1e-4   # quadratic terms\n    ridge_inter = 1e-2   # interaction terms\n    ridge_cubic = 1e-3   # cubic term on lr\n\n    # parameter index blocks\n    idx_lin   = range(1, 1+F)                                # 1..4\n    idx_quad  = range(1+F, 1+2*F)                            # 5..8\n    idx_inter = range(1+2*F, 1+2*F + (F*(F-1)//2))           # 9..14\n    idx_cubic = [A.shape[0]-1]                                # 15\n\n    # apply penalties (intercept at 0 left unregularized)\n    for i in idx_lin:\n        A[i, i] += ridge_lin\n    for i in idx_quad:\n        A[i, i] += ridge_quad\n    for i in idx_inter:\n        A[i, i] += ridge_inter\n    for i in idx_cubic:\n        A[i, i] += ridge_cubic\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, NumPy-based implementation, Comprehensive implementation\n\n\n### Inspiration 4 (Score: 0.5978, Type: Experimental)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling\u2010law model for LLM training hyperparameters.\n\nWe model the LM loss via a second\u2010order polynomial in the log\u2010domain of each feature:\n   log(loss) = c0\n             + sum_i ci * log(x_i)\n             + sum_i di * (log(x_i))^2\n             + sum_{i<j} e_{ij} * log(x_i)*log(x_j)\n\nwhere x = [lr, bsz, data_size, non_embedding_param_size].\n\nWe fit this via ridge\u2010regularized closed\u2010form regression for stability and efficiency.\n\"\"\"\nimport numpy as np\n\ndef _build_design_matrix(X):\n    \"\"\"\n    Build the design matrix \u03a6 for a second\u2010order log\u2010polynomial.\n    X: (N,4) array of positive features.\n    Returns: \u03a6 of shape (N, 15).\n    \"\"\"\n    N, F = X.shape\n    # Floor to avoid log(0)\n    X = np.maximum(X, 1e-12)\n    logX = np.log(X)  # (N,4)\n    \n    # Total parameters = 1 intercept + 4 linear + 4 quadratic + 6 interactions = 15\n    P = 1 + F + F + (F*(F-1))//2\n    Phi = np.ones((N, P), dtype=float)\n    \n    # Fill linear & quadratic & interaction terms\n    idx = 1\n    # linear terms\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd\u2010order log\u2010polynomial.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15.\n\n    Returns:\n      y_pred:      1D array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    \n    p = np.asarray(params, dtype=float).ravel()\n    P_exp = 1 + F + F + (F*(F-1))//2\n    if p.size != P_exp:\n        raise ValueError(f\"Expected {P_exp} parameters, got {p.size}\")\n    \n    Phi = _build_design_matrix(X)           # (N,15)\n    log_pred = Phi.dot(p)                   # (N,)\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via ridge\u2010regularized regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params:      1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n    \n    # Floor to avoid log(0) or negative\n    X = np.maximum(X, 1e-12)\n    y = np.maximum(y, 1e-12)\n    \n    # Log\u2010transform\n    logy = np.log(y)\n    Phi = _build_design_matrix(X)           # (N,15)\n    \n    # Closed\u2010form ridge regression: (\u03a6\u1d40\u03a6 + \u03bbI) p = \u03a6\u1d40 logy\n    ridge = 1e-4\n    P = Phi.shape[1]\n    A = Phi.T.dot(Phi)\n    # apply ridge on all but intercept\n    A[np.arange(1, P), np.arange(1, P)] += ridge\n    b = Phi.T.dot(logy)\n    \n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, NumPy-based implementation, Comprehensive implementation\n\n\n### Inspiration 5 (Score: 0.6797, Type: Alternative)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling\u2010law model for LLM training hyperparameters with learned asymptotic floor.\n\nWe model\n    y_pred = y_inf + exp( \u03a6(log X) \u00b7 p )\nwhere\n    \u03a6(log X) contains an intercept, first\u2010order, squared, and pairwise interaction\n    terms on the log of each feature.\nWe jointly learn the polynomial coefficients p and the floor y_inf by\na 1D golden\u2010section search over y_inf and closed\u2010form ridge regression\nfor p at each candidate.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language\u2010model loss from hyperparameters.\n\n    Args:\n      data_points: array of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 16:\n                   [y_inf,\n                    \u03b20,\n                    \u03b2_lr, \u03b2_bsz, \u03b2_data, \u03b2_param,\n                    \u03b3_lr2, \u03b3_bsz2, \u03b3_data2, \u03b3_param2,\n                    \u03b4_lr_bsz, \u03b4_lr_data, \u03b4_lr_param,\n                    \u03b4_bsz_data, \u03b4_bsz_param, \u03b4_data_param]\n\n    Returns:\n      y_pred: array of shape (N,) of predicted loss values.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # clamp to avoid log(0)\n    X = np.maximum(X, 1e-12)\n    logX = np.log(X)  # (N,4)\n    N, F = logX.shape\n\n    p_all = np.asarray(params, dtype=float).ravel()\n    if p_all.size != 1 + (1 + F + F + (F*(F-1))//2):\n        raise ValueError(f\"Expected 16 parameters but got {p_all.size}\")\n    y_inf = p_all[0]\n    p = p_all[1:]\n\n    # build design matrix \u03a6\n    P_expected = 1 + F + F + (F*(F-1))//2  # intercept + linear + squared + interactions\n    Phi = np.empty((N, P_expected), dtype=float)\n    # intercept\n    Phi[:, 0] = 1.0\n    # linear terms\n    Phi[:, 1:1+F] = logX\n    idx = 1 + F\n    # squared terms\n    for i in range(F):\n        Phi[:, idx] = logX[:, i]**2\n        idx += 1\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    log_term = Phi.dot(p)\n    y_pred = y_inf + np.exp(log_term)\n    return y_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the scaling law by jointly optimizing the floor y_inf via\n    golden\u2010section search and the polynomial coefficients via\n    closed\u2010form ridge regression.\n\n    Args:\n      data_points: array of shape (N,4)\n      loss_values: array of shape (N,)\n\n    Returns:\n      params: 1D array of learned parameters of length 16.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"Number of data points and losses must match.\")\n\n    # clamp inputs/outputs\n    X = np.maximum(X, 1e-12)\n    y = np.maximum(y, 1e-12)\n\n    N, F = X.shape\n    # build log\u2010feature matrix once\n    logX = np.log(X)\n    # design\u2010matrix dimension\n    P_poly = 1 + F + F + (F*(F-1))//2\n\n    def make_Phi(logX):\n        Phi = np.empty((logX.shape[0], P_poly), dtype=float)\n        Phi[:, 0] = 1.0\n        Phi[:, 1:1+F] = logX\n        idx = 1 + F\n        for i in range(F):\n            Phi[:, idx] = logX[:, i]**2\n            idx += 1\n        for i in range(F):\n            for j in range(i+1, F):\n                Phi[:, idx] = logX[:, i] * logX[:, j]\n                idx += 1\n        return Phi\n\n    Phi = make_Phi(logX)\n\n    # ridge penalty for polynomial terms (not intercept)\n    ridge = 1e-3\n\n    # residual MSE given y_inf candidate: solve for p then compute MSE.\n    def mse_for_yinf(y_inf):\n        z = y - y_inf\n        # require positivity\n        if np.any(z <= 0):\n            return np.inf\n        logz = np.log(z)\n        # solve (\u03a6\u1d40\u03a6 + ridge*I) p = \u03a6\u1d40 logz, but no penalty on intercept\n        A = Phi.T.dot(Phi)\n        # add ridge to diagonal\n        diag = np.arange(P_poly)\n        A[diag, diag] += ridge\n        A[0, 0] -= ridge  # no regularization on intercept\n        b = Phi.T.dot(logz)\n        try:\n            p = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            return np.inf\n        # compute predictions\n        log_term = Phi.dot(p)\n        y_pred = y_inf + np.exp(log_term)\n        return np.mean((y_pred - y)**2)\n\n    # golden\u2010section search for y_inf in [0, min(y)*0.9]\n    y_min = np.min(y)\n    a, b = 0.0, y_min * 0.9\n    gr = (np.sqrt(5) - 1) / 2\n    # initial interior points\n    c = b - gr*(b - a)\n    d = a + gr*(b - a)\n    fc = mse_for_yinf(c)\n    fd = mse_for_yinf(d)\n    for _ in range(25):\n        if fc > fd:\n            a = c\n            c = d\n            fc = fd\n            d = a + gr*(b - a)\n            fd = mse_for_yinf(d)\n        else:\n            b = d\n            d = c\n            fd = fc\n            c = b - gr*(b - a)\n            fc = mse_for_yinf(c)\n    # best y_inf\n    y_inf_opt = c if fc < fd else d\n\n    # re\u2010solve for polynomial at optimal y_inf\n    z = y - y_inf_opt\n    logz = np.log(z)\n    A = Phi.T.dot(Phi)\n    diag = np.arange(P_poly)\n    A[diag, diag] += ridge\n    A[0, 0] -= ridge\n    bvec = Phi.T.dot(logz)\n    p_opt = np.linalg.solve(A, bvec)\n\n    # pack parameters: [y_inf, ...polynomial coefficients...]\n    params = np.concatenate([[y_inf_opt], p_opt])\n    return params\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (0.990), Excellent nmae (1.216)\n\n\n\n# Current Program\n```python\nimport numpy as np\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a second\u2010order polynomial design matrix in the log\u2010domain.\n    Inputs:\n      logX: array of shape (N,4) with columns\n            [log(lr), log(bsz), log(data_size), log(non_embedding_param_size)]\n    Returns:\n      Phi: array of shape (N,15) with columns\n           [1,\n            logX_i,\n            (logX_i)^2,\n            logX_i * logX_j  for i<j]\n    \"\"\"\n    N, F = logX.shape\n    # total terms = 1 intercept + F linear + F quadratic + F*(F-1)/2 interactions\n    P = 1 + F + F + (F*(F-1))//2\n    Phi = np.empty((N, P), dtype=np.float64)\n\n    # intercept\n    Phi[:, 0] = 1.0\n\n    # linear terms\n    idx = 1\n    Phi[:, idx:idx+F] = logX\n    idx += F\n\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n\n    # pairwise interaction terms\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd\u2010order log\u2010polynomial.\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15 of fitted coefficients.\n    Returns:\n      y_pred:      numpy array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=np.float64)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=np.float64).ravel()\n    P_expected = 1 + F + F + (F*(F-1))//2\n    if p.size != P_expected:\n        raise ValueError(f\"Expected {P_expected} parameters, got {p.size}\")\n\n    # avoid log of zero or negatives\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n\n    # build design matrix and predict in log\u2010domain\n    Phi = _build_design_matrix(logX)   # shape (N,15)\n    log_pred = Phi.dot(p)              # shape (N,)\n\n    # back to original loss space\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via weighted,\n    differential\u2010ridge regression in closed form.\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n    Returns:\n      params: numpy array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=np.float64)\n    y = np.asarray(loss_values, dtype=np.float64).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor to avoid log issues\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_safe)    # shape (N,4)\n    logy = np.log(y_safe)    # shape (N,)\n\n    # design matrix\n    Phi = _build_design_matrix(logX)   # shape (N,15)\n    _, P = Phi.shape\n\n    # compute sample weights to emphasize large\u2010scale configs\n    # here we weight by combined size of data and params\n    weight_signal = logX[:, 2] + logX[:, 3]\n    median_ws = np.median(weight_signal)\n    w = np.exp(0.5 * (weight_signal - median_ws))   # shape (N,)\n\n    # build weighted normal equations\n    W_sqrt = np.sqrt(w)[:, None]      # shape (N,1)\n    Phi_w = Phi * W_sqrt              # each row scaled by sqrt(w_i)\n    y_w   = logy * W_sqrt.ravel()     # shape (N,)\n\n    A = Phi_w.T.dot(Phi_w)            # shape (15,15)\n    b = Phi_w.T.dot(y_w)              # shape (15,)\n\n    # differential ridge regularization\n    # intercept index = 0 (no regularization)\n    # linear terms     = indices 1..F\n    # quadratic terms  = indices F+1..2F\n    # interactions     = indices 2F+1..P-1\n    idx_lin_start   = 1\n    idx_lin_end     = 1 + F\n    idx_quad_start  = idx_lin_end\n    idx_quad_end    = idx_quad_start + F\n    idx_int_start   = idx_quad_end\n    idx_int_end     = P\n\n    # small ridge for linear, moderate for quad, stronger for interactions\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_inter = 1e-2\n\n    for i in range(idx_lin_start, idx_lin_end):\n        A[i, i] += ridge_lin\n    for i in range(idx_quad_start, idx_quad_end):\n        A[i, i] += ridge_quad\n    for i in range(idx_int_start, idx_int_end):\n        A[i, i] += ridge_inter\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using a simplified 2nd\u2010order\n    log\u2010polynomial scaling law with one key interaction.\n\n    Model in log\u2010domain:\n      log(y_pred) = p0\n                   + p1*L_lr   + p2*L_bsz   + p3*L_data   + p4*L_param\n                   + p5*L_lr^2 + p6*L_bsz^2 + p7*L_data^2 + p8*L_param^2\n                   + p9*(L_data * L_param)\n\n    where L_x = log(x), x = [lr, bsz, data_size, non_embedding_param_size].\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 10 of model coefficients:\n                   [p0,\n                    p1_lr, p2_bsz, p3_data, p4_param,\n                    p5_lr2, p6_bsz2, p7_data2, p8_param2,\n                    p9_data\u00d7param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"scaling_law_func: expected 10 parameters, got {p.size}\")\n\n    # floor to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix (N x 10)\n    Phi = np.stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # log\u2010domain prediction and back\u2010transform\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the simplified 2nd\u2010order log\u2010polynomial scaling law via\n    weighted, differentially\u2010regularized ridge regression in log\u2010space.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: number of points and losses must match\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix \u03a6 (N x 10)\n    Phi = np.stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # sample weighting: up\u2010weight large data+model scales\n    scale_signal = L_data + L_param\n    median_scale = np.median(scale_signal)\n    w = np.exp(0.5 * (scale_signal - median_scale))  # shape (N,)\n    sqrt_w = np.sqrt(w)[:, None]\n\n    # weighted normal equations\n    Phi_w = Phi * sqrt_w        # each row scaled by sqrt(w_i)\n    y_w   = logy * sqrt_w.ravel()\n    A = Phi_w.T.dot(Phi_w)      # (10 x 10)\n    b = Phi_w.T.dot(y_w)        # (10,)\n\n    # differential ridge penalties\n    # intercept idx=0 (no penalty)\n    # linear terms idx 1-4\n    # quadratic idx 5-8\n    # cross-term idx 9\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_cross = 1e-2\n\n    for i in range(1, 5):\n        A[i, i] += ridge_lin\n    for i in range(5, 9):\n        A[i, i] += ridge_quad\n    A[9, 9] += ridge_cross\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```"]}}}