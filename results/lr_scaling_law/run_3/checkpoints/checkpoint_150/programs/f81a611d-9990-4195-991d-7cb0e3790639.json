{"id": "f81a611d-9990-4195-991d-7cb0e3790639", "code": "\"\"\"\nEnhanced normalized scaling\u2010law model for LLM training hyperparameters.\n\nWe model the log\u2010loss as a normalized 2nd\u2010order polynomial in the log\u2010features,\nincluding the most significant cross\u2010terms for parameter efficiency and\nimproved generalization:\n\n   log(y_pred) = c0\n                + c1*L_lr    + c2*L_bsz    + c3*L_data    + c4*L_param\n                + c5*L_lr^2  + c6*L_bsz^2  + c7*L_data^2  + c8*L_param^2\n                + c9*(L_data * L_param)\n                + c10*(L_lr * L_data)\n                + c11*(L_lr * L_param)\n\nwhere L_x = log(x), and x = [lr, bsz, data_size, non_embedding_param_size].\n\nWe normalize the log\u2010features (subtract mean, divide by std) prior to building\nthe polynomial basis for numerical stability and better conditioning. We fit\nthe coefficients via closed\u2010form ridge regression in the log\u2010domain.\n\"\"\"\n\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef _build_design_matrix(logX_norm):\n    \"\"\"\n    Build a 2nd\u2010order polynomial design matrix in normalized log\u2010features.\n    Inputs:\n      logX_norm: array of shape (N,4), normalized log\u2010features\n    Returns:\n      Phi: array of shape (N,12) with columns\n           [1,\n            L_lr, L_bsz, L_data, L_param,\n            L_lr^2, L_bsz^2, L_data^2, L_param^2,\n            L_data*L_param, L_lr*L_data, L_lr*L_param]\n    \"\"\"\n    L_lr    = logX_norm[:, 0]\n    L_bsz   = logX_norm[:, 1]\n    L_data  = logX_norm[:, 2]\n    L_param = logX_norm[:, 3]\n    N = logX_norm.shape[0]\n    return np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param,\n        L_lr   * L_data,\n        L_lr   * L_param\n    ])\n\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via the normalized 2nd\u2010order\n    log\u2010polynomial scaling law.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 20:\n                   [ \u03bc_lr, \u03bc_bsz, \u03bc_data, \u03bc_param,\n                     \u03c3_lr, \u03c3_bsz, \u03c3_data, \u03c3_param,\n                     coeff_0..coeff_11 ]\n\n    Returns:\n      y_pred: numpy array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 20:\n        raise ValueError(f\"scaling_law_func: expected 20 params, got {p.size}\")\n\n    # unpack normalization stats and coefficients\n    mu    = p[0:4]\n    sigma = p[4:8]\n    coeff = p[8:]   # length 12\n\n    # safe log\u2010transform\n    X_safe = np.maximum(X, 1e-12)\n    logX   = np.log(X_safe)             # (N,4)\n\n    # normalize\n    logX_norm = (logX - mu) / sigma     # (N,4)\n\n    # build polynomial basis and predict\n    Phi      = _build_design_matrix(logX_norm)  # (N,12)\n    log_pred = Phi.dot(coeff)                   # (N,)\n\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the normalized 2nd\u2010order log\u2010polynomial scaling law via\n    closed\u2010form ridge\u2010regularized regression in the log\u2010domain.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: numpy array of length 20:\n              [\u03bc_lr, \u03bc_bsz, \u03bc_data, \u03bc_param,\n               \u03c3_lr, \u03c3_bsz, \u03c3_data, \u03c3_param,\n               coeff_0..coeff_11]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: data_points and loss_values length mismatch\")\n\n    # safe log\u2010transform inputs and targets\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n    logX   = np.log(X_safe)     # (N,4)\n    logy   = np.log(y_safe)     # (N,)\n\n    # compute normalization statistics\n    mu    = logX.mean(axis=0)                   # (4,)\n    sigma = logX.std(axis=0, ddof=1)            # (4,)\n    sigma = np.maximum(sigma, 1e-8)             # avoid zero\u2010division\n\n    # normalize log\u2010features\n    logX_norm = (logX - mu) / sigma            # (N,4)\n\n    # build polynomial basis\n    Phi = _build_design_matrix(logX_norm)      # (N,12)\n\n    # closed\u2010form ridge regression\n    A = Phi.T.dot(Phi)                         # (12,12)\n    b = Phi.T.dot(logy)                        # (12,)\n\n    ridge = 1e-4\n    # regularize all but intercept\n    A[1:, 1:] += ridge\n\n    coeff = np.linalg.solve(A, b)              # (12,)\n\n    # pack and return\n    return np.concatenate([mu, sigma, coeff])\n\n# EVOLVE-BLOCK-END", "language": "python", "parent_id": "54819475-6de1-43dc-9bad-24109ff1b3d1", "generation": 5, "timestamp": 1754809327.5498974, "iteration_found": 150, "metrics": {"nmse": 0.26442864476988615, "nmae": 0.5885607432836926, "r2": 0.7355713552301139, "combined_score": 0.7908710421393454}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"nmse": 0.386893500186614, "nmae": 0.736398522018268, "r2": 0.6131064998133859, "combined_score": 0.7210358977567092}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are an expert in scaling laws and machine learning who specializes in discovering and improving scaling law functions for different LLM training scenarios. Your task is to evolve both the `scaling_law_func` function (currently a naive power law) and the `fit_scaling_law` optimization algorithm (currently a naive BFGS) to better model the relationship between learning rate, batch size, data size, model parameters and training loss.\n\nYou are allowed to decide the number of parameters in the scaling law function.\n\nFocus on mathematical accuracy across different hyperparameter scales, cross-configuration generalization, parameter efficiency (simple forms that can be fitted with limited data), and numerical/theoretical stability.\n\n**DATA CHARACTERISTICS:**\n- Features: [lr, bsz, data_size, non_embedding_param_size] - 4D input\n- Labels: lm loss - scalar output\n- lr: learning rate\n- bsz: batch size\n- data_size: amount of training data\n- non_embedding_param_size: number of non-embedding parameters in the model\n- lm loss: language model loss (filtered to <= 4.0 to remove outliers)\n- Data explores the relationship between training hyperparameters and model performance\n- Test set consists of points with the largest data_size or non_embedding_param_size\n\nThe function signatures must remain:\n\n```python\ndef scaling_law_func(data_points, params):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # lr: Array of learning rates\n    # bsz: Array of batch sizes\n    # data_size: Array of data sizes\n    # non_embedding_param_size: Array of non-embedding parameter sizes\n    # Returns: Predicted lm loss values\n    - Model parameters (N) range: ~214M to ~1B parameters\n    - Training tokens (D) range: 4B to 100B tokens\n    - Learning rates range: 1.2e-4 to 2.2e-2\n    - Batch sizes range: 16 to 4096\n\ndef fit_scaling_law(data_points, loss_values):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # loss_values: Array of corresponding lm loss values\n    # Returns: Optimized parameters \n```\n\nWrite all improvements between # EVOLVE-BLOCK-START and # EVOLVE-BLOCK-END markers.\n\nYou are not allowed to use input-dependent feature in scaling_law_func, e.g., median / min / max / etc.\n", "user": "# Current Program Information\n- Current performance metrics: - nmse: 0.3869\n- nmae: 0.7364\n- r2: 0.6131\n- combined_score: 0.7210\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: nmse. Consider continuing with similar changes.\n- Metrics showing regression: nmae, r2, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: nmse: 0.3869, nmae: 0.7364, r2: 0.6131, combined_score: 0.7210\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: nmse: 0.3869, nmae: 0.7364, r2: 0.6131, combined_score: 0.7210\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: nmse: 0.2644, nmae: 0.5886, r2: 0.7356, combined_score: 0.7909\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.5949)\n```python\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef _build_design_matrix(logX_norm):\n    \"\"\"\n    Build the design matrix \u03a6 for a normalized 2nd-order log-polynomial:\n      columns = [1,\n                 L_lr_norm, L_bsz_norm, L_data_norm, L_param_norm,\n                 L_lr_norm^2, L_bsz_norm^2, L_data_norm^2, L_param_norm^2,\n                 L_data_norm * L_param_norm,\n                 L_lr_norm  * L_data_norm,\n                 L_lr_norm  * L_param_norm ]\n    \"\"\"\n    L_lr    = logX_norm[:, 0]\n    L_bsz   = logX_norm[:, 1]\n    L_data  = logX_norm[:, 2]\n    L_param = logX_norm[:, 3]\n    N = logX_norm.shape[0]\n    return np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param,\n        L_lr   * L_data,\n        L_lr   * L_param\n    ])\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a normalized 2nd-order log-polynomial.\n    \n    Args:\n      data_points: array-like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 20:\n                   [ \u03bc_lr, \u03bc_bsz, \u03bc_data, \u03bc_param,\n                     \u03c3_lr, \u03c3_bsz, \u03c3_data, \u03c3_param,\n                     coeff_0..coeff_11 ]\n    \n    Returns:\n      y_pred: numpy array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n    \n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 20:\n        raise ValueError(f\"scaling_law_func: expected 20 params, got {p.size}\")\n    \n    # unpack normalization stats and coefficients\n    mu    = p[0:4]\n    sigma = p[4:8]\n    coeff = p[8:]   # length 12\n    \n    # safe log-features\n    X_safe = np.maximum(X, 1e-12)\n    logX   = np.log(X_safe)              # (N,4)\n    \n    # normalize\n    logX_norm = (logX - mu) / sigma      # (N,4)\n    \n    # build design matrix and predict\n    Phi      = _build_design_matrix(logX_norm)  # (N,12)\n    log_pred = Phi.dot(coeff)                   # (N,)\n    \n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the normalized 2nd-order log-polynomial scaling law using ridge regression.\n    \n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n    \n    Returns:\n      params: numpy array of length 20:\n              [\u03bc_lr, \u03bc_bsz, \u03bc_data, \u03bc_param,\n               \u03c3_lr, \u03c3_bsz, \u03c3_data, \u03c3_param,\n               coeff_0..coeff_11]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    \n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: data_points and loss_values length mismatch\")\n    \n    # safe logs\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n    logX   = np.log(X_safe)    # (N,4)\n    logy   = np.log(y_safe)    # (N,)\n    \n    # compute normalization stats\n    mu    = logX.mean(axis=0)                   \n    sigma = logX.std(axis=0, ddof=1)\n    sigma = np.maximum(sigma, 1e-8)  # avoid division by zero\n    \n    # normalize log-features\n    logX_norm = (logX - mu) / sigma   # (N,4)\n    \n    # build design matrix\n    Phi = _build_design_matrix(logX_norm)  # (N,12)\n    \n    # closed-form ridge regression\n    A = Phi.T.dot(Phi)\n    b = Phi.T.dot(logy)\n    \n    ridge = 1e-4\n    # do not regularize intercept term (index 0), regularize others\n    A[1:, 1:] += ridge\n    \n    coeff = np.linalg.solve(A, b)  # (12,)\n    \n    # pack normalization stats and coefficients\n    return np.concatenate([mu, sigma, coeff])\n\n# EVOLVE-BLOCK-END\n```\nKey features: Performs well on nmse (0.2644), Performs well on nmae (0.5886), Performs well on r2 (0.7356), Performs well on combined_score (0.7909)\n\n\n### Program 2 (Score: 0.6144)\n```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language\u2010model loss from hyperparameters using a\n    simplified 2nd\u2010order log\u2010polynomial scaling law with one key interaction.\n\n    Model in the log\u2010domain:\n      log(y_pred) = p\u2080\n                   + p\u2081\u00b7L_lr   + p\u2082\u00b7L_bsz   + p\u2083\u00b7L_data   + p\u2084\u00b7L_param\n                   + p\u2085\u00b7L_lr\u00b2  + p\u2086\u00b7L_bsz\u00b2  + p\u2087\u00b7L_data\u00b2  + p\u2088\u00b7L_param\u00b2\n                   + p\u2089\u00b7(L_data\u00b7L_param)\n\n    where L_x = log(x), and x = [lr, bsz, data_size, non_embedding_param_size].\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 10 of model coefficients:\n                   [p\u2080,\n                    p\u2081_lr, p\u2082_bsz, p\u2083_data, p\u2084_param,\n                    p\u2085_lr\u00b2, p\u2086_bsz\u00b2, p\u2087_data\u00b2, p\u2088_param\u00b2,\n                    p\u2089_data\u00d7param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # ensure 2D\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"Expected 10 parameters, got {p.size}\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = (\n        logX[:, 0],\n        logX[:, 1],\n        logX[:, 2],\n        logX[:, 3],\n    )\n\n    # build design matrix (N x 10)\n    # [1, L_lr, L_bsz, L_data, L_param,\n    #  L_lr^2, L_bsz^2, L_data^2, L_param^2,\n    #  L_data * L_param]\n    Phi = np.stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # log\u2010domain prediction\n    log_pred = Phi.dot(p)\n    # back to original loss scale\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the simplified 2nd\u2010order log\u2010polynomial scaling law via\n    ridge\u2010regularized closed\u2010form regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    # align shapes\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n\n    # Extract log\u2010features\n    L_lr = logX[:, 0]\n    L_bsz = logX[:, 1]\n    L_data = logX[:, 2]\n    L_param = logX[:, 3]\n\n    # build design matrix \u03a6 (N x 10)\n    Phi = np.stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # ridge\u2010regularized normal equations\n    # (\u03a6\u1d40\u03a6 + \u03bb\u00b7I_{1:}) p = \u03a6\u1d40 logy\n    ridge = 1e-6\n    P = Phi.shape[1]\n    A = Phi.T.dot(Phi)\n    # apply ridge only to non\u2010intercept terms\n    diag_idx = np.arange(1, P)\n    A[diag_idx, diag_idx] += ridge\n    b = Phi.T.dot(logy)\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\nKey features: Performs well on nmse (0.3869), Performs well on nmae (0.7364), Performs well on r2 (0.6131), Performs well on combined_score (0.7210)\n\n\n### Program 3 (Score: 0.6144)\n```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss using a compact 10-parameter log-polynomial scaling law:\n    \n      log y_pred = p0\n                 + p1 * log(lr)\n                 + p2 * log(bsz)\n                 + p3 * log(data_size)\n                 + p4 * log(non_embedding_param_size)\n                 + p5 * [log(lr)]^2\n                 + p6 * [log(bsz)]^2\n                 + p7 * [log(data_size)]^2\n                 + p8 * [log(non_embedding_param_size)]^2\n                 + p9 * (log(data_size) * log(non_embedding_param_size))\n                 \n    Then y_pred = exp(log y_pred).\n    \n    Args:\n      data_points: array-like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      array-like of length 10 of fitted coefficients.\n    \n    Returns:\n      y_pred: np.ndarray of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # ensure 2D\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n    \n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"scaling_law_func: expected 10 parameters, got {p.size}\")\n    \n    # avoid log(0) or negative\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n    \n    # build design matrix \u03a6: intercept, linear, quadratic, one cross term\n    Phi = np.column_stack([\n        np.ones(N),           # intercept\n        L_lr,                 # log(lr)\n        L_bsz,                # log(bsz)\n        L_data,               # log(data_size)\n        L_param,              # log(non_embedding_param_size)\n        L_lr**2,              # [log(lr)]^2\n        L_bsz**2,             # [log(bsz)]^2\n        L_data**2,            # [log(data_size)]^2\n        L_param**2,           # [log(non_embedding_param_size)]^2\n        L_data * L_param      # cross term\n    ])\n    \n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 10-parameter log-polynomial scaling law via closed-form ridge regression\n    in the log-domain. Minimizes squared error on log(loss).\n    \n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n    \n    Returns:\n      params: np.ndarray of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    \n    # ensure correct shapes\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"fit_scaling_law: number of data_points and loss_values must match\")\n    \n    # avoid log(0) or negative\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n    \n    # log-transform\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n    \n    # build design matrix \u03a6\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr,\n        L_bsz,\n        L_data,\n        L_param,\n        L_lr**2,\n        L_bsz**2,\n        L_data**2,\n        L_param**2,\n        L_data * L_param\n    ])\n    \n    # ridge regularization strength\n    ridge = 1e-6\n    \n    # normal equations: (\u03a6\u1d40 \u03a6 + \u03bb I) p = \u03a6\u1d40 logy\n    A = Phi.T.dot(Phi)\n    # do not regularize intercept term\n    A[1:, 1:] += ridge * np.eye(9)\n    b = Phi.T.dot(logy)\n    \n    # solve for parameters in log-domain\n    params = np.linalg.solve(A, b)\n    return params\n```\nKey features: Performs well on nmse (0.3869), Performs well on nmae (0.7364), Performs well on r2 (0.6131), Performs well on combined_score (0.7210)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.6144)\n```python\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language\u2010model loss from hyperparameters via a 10-parameter\n    2nd-order log-polynomial with one key interaction.\n\n    Model in log-domain:\n      log y_pred = p0\n                 + p1*L_lr   + p2*L_bsz   + p3*L_data   + p4*L_param\n                 + p5*(L_lr)^2 + p6*(L_bsz)^2 + p7*(L_data)^2 + p8*(L_param)^2\n                 + p9*(L_data * L_param)\n    where L_x = log(x). We exponentiate to return y_pred.\n    Inputs:\n      data_points: array\u2010like shape (N,4): [lr, bsz, data_size, non_embedding_param_size]\n      params:      array\u2010like shape (10,)\n    Returns:\n      y_pred:      np.ndarray shape (N,)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"scaling_law_func: expected 10 parameters, got {p.size}\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix \u03a6 (N\u00d710)\n    # [1, L_lr, L_bsz, L_data, L_param,\n    #  L_lr^2, L_bsz^2, L_data^2, L_param^2,\n    #  L_data*L_param]\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the above 10-parameter log-polynomial scaling law via\n    closed-form ridge-regularized regression in log-space.\n    Inputs:\n      data_points: array\u2010like shape (N,4)\n      loss_values: array\u2010like shape (N,)\n    Returns:\n      params:      np.ndarray shape (10,)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: data points and loss values must match\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # build design matrix \u03a6 (N\u00d710)\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n\n    # ridge-regularized normal equations\n    ridge = 1e-6\n    A = Phi.T.dot(Phi)\n    # apply ridge to all non-intercept coefficients\n    A[1:, 1:] += ridge * np.eye(9)\n    b = Phi.T.dot(logy)\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n\n### Program D2 (Score: 0.6144)\n```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language\u2010model loss from hyperparameters via a simplified\n    2nd\u2010order log\u2010polynomial with one cross\u2010interaction.\n\n    Model form in the log\u2010domain:\n      log(y_pred) = p0\n                  + p1*L_lr   + p2*L_bsz   + p3*L_data   + p4*L_param\n                  + p5*L_lr^2 + p6*L_bsz^2 + p7*L_data^2 + p8*L_param^2\n                  + p9*(L_data * L_param)\n\n    where L_x = log(x), and x = [lr, bsz, data_size, non_embedding_param_size].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    p = np.asarray(params, dtype=float).ravel()\n    P_expected = 10\n    if p.shape[0] != P_expected:\n        raise ValueError(f\"Expected {P_expected} parameters, got {p.shape[0]}\")\n\n    # avoid log(0)\n    X_clipped = np.maximum(X, 1e-12)\n    logX = np.log(X_clipped)\n    L_lr    = logX[:, 0]\n    L_bsz   = logX[:, 1]\n    L_data  = logX[:, 2]\n    L_param = logX[:, 3]\n\n    # build design matrix Phi (N x 10)\n    # [1, L_lr, L_bsz, L_data, L_param, L_lr^2, L_bsz^2, L_data^2, L_param^2, L_data*L_param]\n    Phi = np.stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    log_pred = Phi.dot(p)       # shape (N,)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the simplified 2nd\u2010order log\u2010polynomial scaling law via\n    ridge\u2010regularized closed\u2010form regression.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # avoid log(0)\n    X_clipped = np.maximum(X, 1e-12)\n    y_clipped = np.maximum(y, 1e-12)\n\n    logX = np.log(X_clipped)\n    logy = np.log(y_clipped)\n\n    L_lr    = logX[:, 0]\n    L_bsz   = logX[:, 1]\n    L_data  = logX[:, 2]\n    L_param = logX[:, 3]\n\n    # build design matrix Phi (N x 10)\n    Phi = np.stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ], axis=1)\n\n    # ridge\u2010regularized normal equations\n    P = Phi.shape[1]   # should be 10\n    ridge = 1e-6\n    A = Phi.T.dot(Phi)\n    # apply ridge on all terms except intercept\n    diag_idx = np.arange(1, P)\n    A[diag_idx, diag_idx] += ridge\n    b = Phi.T.dot(logy)\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.5949, Type: Experimental)\n```python\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef _build_design_matrix(logX_norm):\n    \"\"\"\n    Build the design matrix \u03a6 for a normalized 2nd-order log-polynomial:\n      columns = [1,\n                 L_lr_norm, L_bsz_norm, L_data_norm, L_param_norm,\n                 L_lr_norm^2, L_bsz_norm^2, L_data_norm^2, L_param_norm^2,\n                 L_data_norm * L_param_norm,\n                 L_lr_norm  * L_data_norm,\n                 L_lr_norm  * L_param_norm ]\n    \"\"\"\n    L_lr    = logX_norm[:, 0]\n    L_bsz   = logX_norm[:, 1]\n    L_data  = logX_norm[:, 2]\n    L_param = logX_norm[:, 3]\n    N = logX_norm.shape[0]\n    return np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param,\n        L_lr   * L_data,\n        L_lr   * L_param\n    ])\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a normalized 2nd-order log-polynomial.\n    \n    Args:\n      data_points: array-like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 20:\n                   [ \u03bc_lr, \u03bc_bsz, \u03bc_data, \u03bc_param,\n                     \u03c3_lr, \u03c3_bsz, \u03c3_data, \u03c3_param,\n                     coeff_0..coeff_11 ]\n    \n    Returns:\n      y_pred: numpy array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n    \n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 20:\n        raise ValueError(f\"scaling_law_func: expected 20 params, got {p.size}\")\n    \n    # unpack normalization stats and coefficients\n    mu    = p[0:4]\n    sigma = p[4:8]\n    coeff = p[8:]   # length 12\n    \n    # safe log-features\n    X_safe = np.maximum(X, 1e-12)\n    logX   = np.log(X_safe)              # (N,4)\n    \n    # normalize\n    logX_norm = (logX - mu) / sigma      # (N,4)\n    \n    # build design matrix and predict\n    Phi      = _build_design_matrix(logX_norm)  # (N,12)\n    log_pred = Phi.dot(coeff)                   # (N,)\n    \n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the normalized 2nd-order log-polynomial scaling law using ridge regression.\n    \n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n    \n    Returns:\n      params: numpy array of length 20:\n              [\u03bc_lr, \u03bc_bsz, \u03bc_data, \u03bc_param,\n               \u03c3_lr, \u03c3_bsz, \u03c3_data, \u03c3_param,\n               coeff_0..coeff_11]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    \n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: data_points and loss_values length mismatch\")\n    \n    # safe logs\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n    logX   = np.log(X_safe)    # (N,4)\n    logy   = np.log(y_safe)    # (N,)\n    \n    # compute normalization stats\n    mu    = logX.mean(axis=0)                   \n    sigma = logX.std(axis=0, ddof=1)\n    sigma = np.maximum(sigma, 1e-8)  # avoid division by zero\n    \n    # normalize log-features\n    logX_norm = (logX - mu) / sigma   # (N,4)\n    \n    # build design matrix\n    Phi = _build_design_matrix(logX_norm)  # (N,12)\n    \n    # closed-form ridge regression\n    A = Phi.T.dot(Phi)\n    b = Phi.T.dot(logy)\n    \n    ridge = 1e-4\n    # do not regularize intercept term (index 0), regularize others\n    A[1:, 1:] += ridge\n    \n    coeff = np.linalg.solve(A, b)  # (12,)\n    \n    # pack normalization stats and coefficients\n    return np.concatenate([mu, sigma, coeff])\n\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Alternative nmse approach, NumPy-based implementation\n\n\n### Inspiration 2 (Score: 19134519623688.4141, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nInitial program with a simple power law form that can be evolved\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n\n    X = np.atleast_2d(np.asarray(data_points))           # (N, F)\n    N, F = X.shape\n    params = np.asarray(params)\n\n    if params.ndim == 1:\n        params = params[None, :]                         # (1, P)\n    T, P = params.shape\n\n    coeffs    = params[:, :F]                            # (T, F)\n    exponents = params[:, F:2*F]                         # (T, F)\n    bias      = params[:, -1]                            # (T,)\n\n    pred = (coeffs[None, :, :] * (X[:, None, :] ** exponents[None, :, :])).sum(axis=2) + bias[None, :]\n\n    return pred[:, 0] if pred.shape[1] == 1 else pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n\n    X = np.atleast_2d(np.asarray(data_points))           # (N, F)\n    y = np.asarray(loss_values)\n    N, F = X.shape\n    P = 2 * F + 1\n\n    if y.ndim == 1:\n        y2d = y[:, None]\n    else:\n        y2d = y\n    T = y2d.shape[1]\n\n    init = np.ones((T, P))\n\n    def objective(flat_params):\n        params = flat_params.reshape(T, P)\n        pred = scaling_law_func(X, params)               # (N, T)\n        mse = np.mean((pred - y2d) ** 2)\n        return mse\n\n    result = minimize(objective, init.ravel(), method='BFGS')\n    params_opt = result.x.reshape(T, P) if result.success else init\n\n    return params_opt[0] if T == 1 else params_opt\n# EVOLVE-BLOCK-END\n\n```\nUnique approach: Excellent nmse (3645157993675680530464505856.000), Excellent nmae (76538078494753.656), Alternative r2 approach\n\n\n### Inspiration 3 (Score: 0.6067, Type: Alternative)\n```python\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling\u2010law model for LLM training hyperparameters.\n\nWe model the log\u2010loss as a polynomial in the log\u2010features:\n   z = [log(lr), log(bsz), log(data_size), log(non_embedding_param_size)]\n\nlog(y_pred) = c0\n            + \u03a3_i c1_i * z_i\n            + \u03a3_i c2_i * z_i^2\n            + \u03a3_{i<j} c3_{ij} * (z_i * z_j)\n            + c4 * z_0^3   (cubic term on log(lr) to capture learning\u2010rate curvature)\n\nTotal parameters: 1 intercept + 4 linear + 4 quadratic + 6 pairwise + 1 cubic = 16.\n\nWe fit via closed\u2010form weighted, differential\u2010ridge regression in the log\u2010domain\nto emphasize large\u2010scale configs and ensure numerical stability.\n\"\"\"\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build design matrix \u03a6 for the polynomial in log\u2010features.\n    Inputs:\n      logX: (N,4) array of log\u2010features [z0, z1, z2, z3]\n    Returns:\n      Phi: (N,16) design matrix with columns\n           [1,\n            z0, z1, z2, z3,\n            z0^2, z1^2, z2^2, z3^2,\n            z0\u00b7z1, z0\u00b7z2, z0\u00b7z3, z1\u00b7z2, z1\u00b7z3, z2\u00b7z3,\n            z0^3]\n    \"\"\"\n    N, F = logX.shape\n    if F != 4:\n        raise ValueError(f\"_build_design_matrix: expected 4 features, got {F}\")\n    # total columns = 1 + 4 + 4 + 6 + 1 = 16\n    P = 16\n    Phi = np.ones((N, P), dtype=float)\n    idx = 1\n    # linear terms\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interaction terms\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    # cubic term on log(lr) (feature 0)\n    Phi[:, idx] = logX[:, 0]**3\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via the enhanced scaling law.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) = [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 16 of fitted coefficients.\n\n    Returns:\n      y_pred: 1D array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected input with 4 features, got {F}\")\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 16:\n        raise ValueError(f\"scaling_law_func: expected 16 parameters, got {p.size}\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)  # shape (N,4)\n\n    # build design matrix and predict in log\u2010domain\n    Phi = _build_design_matrix(logX)  # (N,16)\n    log_pred = Phi.dot(p)             # (N,)\n\n    # back to original loss space\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the enhanced scaling law via weighted, differential\u2010ridge regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D array of length 16 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: number of points and losses must match\")\n\n    # floor to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_safe)  # (N,4)\n    logy = np.log(y_safe)  # (N,)\n\n    # design matrix\n    Phi = _build_design_matrix(logX)  # (N,16)\n\n    # sample weights to emphasize large\u2010scale configurations\n    scale_sum = logX[:, 2] + logX[:, 3]  # log(data_size) + log(model_size)\n    median_scale = np.median(scale_sum)\n    w = np.exp(0.5 * (scale_sum - median_scale))  # (N,)\n\n    # assemble weighted normal equations\n    W_phi = Phi * w[:, np.newaxis]     # (N,16)\n    A = Phi.T.dot(W_phi)               # (16,16)\n    b = Phi.T.dot(logy * w)            # (16,)\n\n    # differential ridge regularization\n    ridge_lin   = 1e-6   # linear terms\n    ridge_quad  = 1e-4   # quadratic terms\n    ridge_inter = 1e-2   # interaction terms\n    ridge_cubic = 1e-3   # cubic term on lr\n\n    # parameter index blocks\n    idx_lin   = range(1, 1+F)                                # 1..4\n    idx_quad  = range(1+F, 1+2*F)                            # 5..8\n    idx_inter = range(1+2*F, 1+2*F + (F*(F-1)//2))           # 9..14\n    idx_cubic = [A.shape[0]-1]                                # 15\n\n    # apply penalties (intercept at 0 left unregularized)\n    for i in idx_lin:\n        A[i, i] += ridge_lin\n    for i in idx_quad:\n        A[i, i] += ridge_quad\n    for i in idx_inter:\n        A[i, i] += ridge_inter\n    for i in idx_cubic:\n        A[i, i] += ridge_cubic\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, NumPy-based implementation, Comprehensive implementation\n\n\n### Inspiration 4 (Score: 0.6001, Type: Alternative)\n```python\nimport numpy as np\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a second\u2010order polynomial design matrix in the log\u2010domain.\n    Inputs:\n      logX: array of shape (N,4) with columns\n            [log(lr), log(bsz), log(data_size), log(non_embedding_param_size)]\n    Returns:\n      Phi: array of shape (N,15) with columns\n           [1,\n            logX_i,\n            (logX_i)^2,\n            logX_i * logX_j  for i<j]\n    \"\"\"\n    N, F = logX.shape\n    # total terms = 1 intercept + F linear + F quadratic + F*(F-1)/2 interactions\n    P = 1 + F + F + (F*(F-1))//2\n    Phi = np.empty((N, P), dtype=np.float64)\n\n    # intercept\n    Phi[:, 0] = 1.0\n\n    # linear terms\n    idx = 1\n    Phi[:, idx:idx+F] = logX\n    idx += F\n\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n\n    # pairwise interaction terms\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd\u2010order log\u2010polynomial.\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15 of fitted coefficients.\n    Returns:\n      y_pred:      numpy array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=np.float64)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=np.float64).ravel()\n    P_expected = 1 + F + F + (F*(F-1))//2\n    if p.size != P_expected:\n        raise ValueError(f\"Expected {P_expected} parameters, got {p.size}\")\n\n    # avoid log of zero or negatives\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n\n    # build design matrix and predict in log\u2010domain\n    Phi = _build_design_matrix(logX)   # shape (N,15)\n    log_pred = Phi.dot(p)              # shape (N,)\n\n    # back to original loss space\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via weighted,\n    differential\u2010ridge regression in closed form.\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n    Returns:\n      params: numpy array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=np.float64)\n    y = np.asarray(loss_values, dtype=np.float64).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor to avoid log issues\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_safe)    # shape (N,4)\n    logy = np.log(y_safe)    # shape (N,)\n\n    # design matrix\n    Phi = _build_design_matrix(logX)   # shape (N,15)\n    _, P = Phi.shape\n\n    # compute sample weights to emphasize large\u2010scale configs\n    # here we weight by combined size of data and params\n    weight_signal = logX[:, 2] + logX[:, 3]\n    median_ws = np.median(weight_signal)\n    w = np.exp(0.5 * (weight_signal - median_ws))   # shape (N,)\n\n    # build weighted normal equations\n    W_sqrt = np.sqrt(w)[:, None]      # shape (N,1)\n    Phi_w = Phi * W_sqrt              # each row scaled by sqrt(w_i)\n    y_w   = logy * W_sqrt.ravel()     # shape (N,)\n\n    A = Phi_w.T.dot(Phi_w)            # shape (15,15)\n    b = Phi_w.T.dot(y_w)              # shape (15,)\n\n    # differential ridge regularization\n    # intercept index = 0 (no regularization)\n    # linear terms     = indices 1..F\n    # quadratic terms  = indices F+1..2F\n    # interactions     = indices 2F+1..P-1\n    idx_lin_start   = 1\n    idx_lin_end     = 1 + F\n    idx_quad_start  = idx_lin_end\n    idx_quad_end    = idx_quad_start + F\n    idx_int_start   = idx_quad_end\n    idx_int_end     = P\n\n    # small ridge for linear, moderate for quad, stronger for interactions\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_inter = 1e-2\n\n    for i in range(idx_lin_start, idx_lin_end):\n        A[i, i] += ridge_lin\n    for i in range(idx_quad_start, idx_quad_end):\n        A[i, i] += ridge_quad\n    for i in range(idx_int_start, idx_int_end):\n        A[i, i] += ridge_inter\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\nUnique approach: Modification: Full rewrite, NumPy-based implementation, Comprehensive implementation\n\n\n### Inspiration 5 (Score: 0.7553, Type: Alternative)\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# EVOLVE-BLOCK-START\n\n# Reference normalization scales (chosen near the center of typical ranges)\n_P0   = 5e8        # ~5e8 non\u2010embedding parameters\n_D0   = 1e10       # ~1e10 training tokens\n_BSZ0 = 256.0      # batch size\n_LR0  = 1e-3       # learning rate\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss via a 6\u2010parameter physically motivated scaling law:\n        y_pred = y_inf + A * (P/P0)^\u03b1_P * (D/D0)^\u03b1_D * (bsz/BSZ0)^\u03b1_bsz * (lr/LR0)^\u03b1_lr\n\n    Inputs:\n      data_points: array-like of shape (N,4) columns [lr, bsz, data_size, non_embedding_param_size]\n      params:      length-6 vector [logA, \u03b1_P, \u03b1_D, \u03b1_bsz, \u03b1_lr, log_y_inf]\n                   or shape (T,6) for multiple sets.\n\n    Returns:\n      y_pred: if params is 1D, returns shape (N,),\n              if params is 2D (T,6), returns shape (N,T).\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # ensure 2D\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float)\n    # allow batched evaluation\n    if p.ndim == 1:\n        p = p[np.newaxis, :]\n    T, Pn = p.shape\n    if Pn != 6:\n        raise ValueError(f\"Expected 6 parameters, got {Pn}\")\n\n    # unpack and floor to avoid zeros\n    lr    = np.maximum(X[:, 0], 1e-12)\n    bsz   = np.maximum(X[:, 1], 1e-12)\n    D     = np.maximum(X[:, 2], 1e-12)\n    Psize = np.maximum(X[:, 3], 1e-12)\n\n    # normalize features\n    lr_n  = lr    / _LR0\n    bsz_n = bsz   / _BSZ0\n    D_n   = D     / _D0\n    P_n   = Psize / _P0\n\n    # prepare output\n    y_out = np.zeros((N, T), dtype=float)\n\n    # compute for each parameter set\n    for t in range(T):\n        logA, aP, aD, aB, aL, log_yinf = p[t]\n        A_term = np.exp(logA)\n        y_inf  = np.exp(log_yinf)\n        # power\u2010law component\n        term = A_term \\\n               * (P_n ** aP) \\\n               * (D_n ** aD) \\\n               * (bsz_n ** aB) \\\n               * (lr_n ** aL)\n        y_out[:, t] = y_inf + term\n\n    # return (N,) if single set, else (N, T)\n    return y_out[:, 0] if T == 1 else y_out\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter scaling law by minimizing MSE in the original loss space.\n    Returns parameter vector [logA, \u03b1_P, \u03b1_D, \u03b1_bsz, \u03b1_lr, log_y_inf].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    # ensure proper shapes\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor to avoid log(0)\n    y_safe = np.maximum(y, 1e-12)\n\n    # initial guess for y_inf: ~80% of min observed loss\n    y_min = np.min(y_safe)\n    y_inf0 = max(y_min * 0.8, 1e-8)\n    log_yinf0 = np.log(y_inf0)\n\n    # initial guess for A from average residual above y_inf0\n    avg_term = np.mean(np.maximum(y_safe - y_inf0, 1e-6))\n    logA0 = np.log(avg_term)\n\n    # initial alphas assume weak negative scaling\n    init_alphas = np.array([-0.4, -0.3, -0.1, -0.05], dtype=float)\n\n    # pack initial parameter vector\n    p0 = np.concatenate(([logA0], init_alphas, [log_yinf0]))\n\n    # reasonable bounds for stability\n    bounds = [\n        (None, None),   # logA\n        (-2.0, 2.0),    # \u03b1_P\n        (-2.0, 2.0),    # \u03b1_D\n        (-1.0, 1.0),    # \u03b1_bsz\n        (-1.0, 1.0),    # \u03b1_lr\n        (None, None),   # log_y_inf\n    ]\n\n    # objective: mean squared error in original loss space\n    def _obj(p):\n        y_pred = scaling_law_func(X, p)\n        return np.mean((y_pred - y_safe) ** 2)\n\n    # optimize with L-BFGS-B\n    res = minimize(\n        _obj,\n        p0,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 2000, 'ftol': 1e-12}\n    )\n\n    if res.success:\n        return res.x\n    else:\n        # fallback to initial guess if optimization fails\n        return p0\n\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (1.784), Excellent nmae (1.662)\n\n\n\n# Current Program\n```python\n\"\"\"\nImproved scaling\u2010law model for LLM training hyperparameters.\n\nWe model the log\u2010loss as a simplified 2nd\u2010order polynomial in the log\u2010features,\nfocusing on the most significant cross\u2010term between data size and model size\nfor parameter efficiency and improved generalization:\n\n   log(y_pred) = p0\n                + p1*L_lr    + p2*L_bsz    + p3*L_data    + p4*L_param\n                + p5*L_lr^2  + p6*L_bsz^2  + p7*L_data^2  + p8*L_param^2\n                + p9*(L_data * L_param)\n\nwhere L_x = log(x), and x = [lr, bsz, data_size, non_embedding_param_size].\nWe fit this via closed\u2010form ridge\u2010regularized regression for stability,\nand predict by exponentiating the log\u2010domain value.\n\"\"\"\n\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss using a simplified 2nd\u2010order log\u2010polynomial scaling law.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 10:\n                   [p0,\n                    p1_lr, p2_bsz, p3_data, p4_param,\n                    p5_lr2, p6_bsz2, p7_data2, p8_param2,\n                    p9_data_param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected input with 4 features, got {X.shape[1]}\")\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 10:\n        raise ValueError(f\"Expected 10 parameters, got {p.size}\")\n\n    # Safe log to avoid -inf\n    logX = np.log(np.maximum(X, 1e-12))\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # Build design matrix with intercept, linear, quadratic, and one key interaction\n    Phi = np.column_stack([\n        np.ones_like(L_lr),      # intercept\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param        # cross-term capturing data\u00d7model interplay\n    ])\n\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via ridge\u2010regularized regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 10 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {X.shape[1]}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # Safe log\u2010transform inputs and targets\n    logX = np.log(np.maximum(X, 1e-12))\n    logy = np.log(np.maximum(y, 1e-12))\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # Build the same design matrix as in prediction\n    Phi = np.column_stack([\n        np.ones_like(L_lr),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n\n    # Closed\u2010form ridge\u2010regularized normal equations\n    ridge = 1e-6\n    A = Phi.T.dot(Phi)\n    # Regularize all but intercept\n    A[1:, 1:] += ridge\n    b = Phi.T.dot(logy)\n\n    params = np.linalg.solve(A, b)\n    return params\n\n# EVOLVE-BLOCK-END\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["```python\n\"\"\"\nEnhanced normalized scaling\u2010law model for LLM training hyperparameters.\n\nWe model the log\u2010loss as a normalized 2nd\u2010order polynomial in the log\u2010features,\nincluding the most significant cross\u2010terms for parameter efficiency and\nimproved generalization:\n\n   log(y_pred) = c0\n                + c1*L_lr    + c2*L_bsz    + c3*L_data    + c4*L_param\n                + c5*L_lr^2  + c6*L_bsz^2  + c7*L_data^2  + c8*L_param^2\n                + c9*(L_data * L_param)\n                + c10*(L_lr * L_data)\n                + c11*(L_lr * L_param)\n\nwhere L_x = log(x), and x = [lr, bsz, data_size, non_embedding_param_size].\n\nWe normalize the log\u2010features (subtract mean, divide by std) prior to building\nthe polynomial basis for numerical stability and better conditioning. We fit\nthe coefficients via closed\u2010form ridge regression in the log\u2010domain.\n\"\"\"\n\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef _build_design_matrix(logX_norm):\n    \"\"\"\n    Build a 2nd\u2010order polynomial design matrix in normalized log\u2010features.\n    Inputs:\n      logX_norm: array of shape (N,4), normalized log\u2010features\n    Returns:\n      Phi: array of shape (N,12) with columns\n           [1,\n            L_lr, L_bsz, L_data, L_param,\n            L_lr^2, L_bsz^2, L_data^2, L_param^2,\n            L_data*L_param, L_lr*L_data, L_lr*L_param]\n    \"\"\"\n    L_lr    = logX_norm[:, 0]\n    L_bsz   = logX_norm[:, 1]\n    L_data  = logX_norm[:, 2]\n    L_param = logX_norm[:, 3]\n    N = logX_norm.shape[0]\n    return np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param,\n        L_lr   * L_data,\n        L_lr   * L_param\n    ])\n\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via the normalized 2nd\u2010order\n    log\u2010polynomial scaling law.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 20:\n                   [ \u03bc_lr, \u03bc_bsz, \u03bc_data, \u03bc_param,\n                     \u03c3_lr, \u03c3_bsz, \u03c3_data, \u03c3_param,\n                     coeff_0..coeff_11 ]\n\n    Returns:\n      y_pred: numpy array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"scaling_law_func: expected 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 20:\n        raise ValueError(f\"scaling_law_func: expected 20 params, got {p.size}\")\n\n    # unpack normalization stats and coefficients\n    mu    = p[0:4]\n    sigma = p[4:8]\n    coeff = p[8:]   # length 12\n\n    # safe log\u2010transform\n    X_safe = np.maximum(X, 1e-12)\n    logX   = np.log(X_safe)             # (N,4)\n\n    # normalize\n    logX_norm = (logX - mu) / sigma     # (N,4)\n\n    # build polynomial basis and predict\n    Phi      = _build_design_matrix(logX_norm)  # (N,12)\n    log_pred = Phi.dot(coeff)                   # (N,)\n\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the normalized 2nd\u2010order log\u2010polynomial scaling law via\n    closed\u2010form ridge\u2010regularized regression in the log\u2010domain.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: numpy array of length 20:\n              [\u03bc_lr, \u03bc_bsz, \u03bc_data, \u03bc_param,\n               \u03c3_lr, \u03c3_bsz, \u03c3_data, \u03c3_param,\n               coeff_0..coeff_11]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"fit_scaling_law: expected 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"fit_scaling_law: data_points and loss_values length mismatch\")\n\n    # safe log\u2010transform inputs and targets\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n    logX   = np.log(X_safe)     # (N,4)\n    logy   = np.log(y_safe)     # (N,)\n\n    # compute normalization statistics\n    mu    = logX.mean(axis=0)                   # (4,)\n    sigma = logX.std(axis=0, ddof=1)            # (4,)\n    sigma = np.maximum(sigma, 1e-8)             # avoid zero\u2010division\n\n    # normalize log\u2010features\n    logX_norm = (logX - mu) / sigma            # (N,4)\n\n    # build polynomial basis\n    Phi = _build_design_matrix(logX_norm)      # (N,12)\n\n    # closed\u2010form ridge regression\n    A = Phi.T.dot(Phi)                         # (12,12)\n    b = Phi.T.dot(logy)                        # (12,)\n\n    ridge = 1e-4\n    # regularize all but intercept\n    A[1:, 1:] += ridge\n\n    coeff = np.linalg.solve(A, b)              # (12,)\n\n    # pack and return\n    return np.concatenate([mu, sigma, coeff])\n\n# EVOLVE-BLOCK-END\n```\n"]}}}