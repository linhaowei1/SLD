{"id": "8eaccccb-b40c-4c76-b12b-a9759af39a31", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a 2nd-order polynomial design matrix in log-features:\n      - intercept\n      - linear terms   (log x_i)\n      - quadratic terms (log x_i)^2\n      - pairwise interactions log x_i * log x_j for i<j\n    Input:\n      logX: (N,4) array\n    Output:\n      Phi:  (N,15) design matrix\n    \"\"\"\n    N, F = logX.shape\n    # number of terms: 1 + F + F + F*(F-1)/2 = 1 + 4 + 4 + 6 = 15\n    P = 1 + 2*F + (F*(F-1))//2\n    Phi = np.ones((N, P), dtype=float)\n    idx = 1\n    # linear terms\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd-order log-polynomial.\n\n    Args:\n      data_points: array-like of shape (N,4), columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15 of coefficients\n\n    Returns:\n      y_pred: 1D array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    # floor inputs to avoid log(0)\n    X = np.maximum(X, 1e-12)\n    logX = np.log(X)\n    # build design matrix\n    Phi = _build_design_matrix(logX)    # shape (N,15)\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != Phi.shape[1]:\n        raise ValueError(f\"Expected {Phi.shape[1]} parameters, got {p.size}\")\n    # predict in log-domain\n    logy_pred = Phi.dot(p)               # shape (N,)\n    return np.exp(logy_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd-order log-polynomial scaling law via closed-form\n    ridge-regularized regression with blockwise regularization.\n\n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n\n    Returns:\n      params: 1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor inputs/outputs to avoid log(0)\n    X = np.maximum(X, 1e-12)\n    y = np.maximum(y, 1e-12)\n\n    # log-transform\n    logX = np.log(X)    # (N,4)\n    logy = np.log(y)    # (N,)\n\n    # build design matrix and normal equations\n    Phi = _build_design_matrix(logX)   # (N,15)\n    P = Phi.shape[1]\n\n    # unweighted normal equations: A p = b\n    A = Phi.T.dot(Phi)                 # (15,15)\n    b = Phi.T.dot(logy)                # (15,)\n\n    # blockwise ridge regularization (exclude intercept at index 0)\n    # linear terms indices: 1..4, quadratic: 5..8, interactions: 9..14\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_inter = 1e-2\n\n    # linear block\n    lin_slice  = slice(1, 1+F)\n    A[lin_slice, lin_slice] += ridge_lin * np.eye(F)\n    # quadratic block\n    quad_slice = slice(1+F, 1+2*F)\n    A[quad_slice, quad_slice] += ridge_quad * np.eye(F)\n    # interaction block\n    inter_slice = slice(1+2*F, P)\n    K = P - (1+2*F)\n    A[inter_slice, inter_slice] += ridge_inter * np.eye(K)\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END", "language": "python", "parent_id": "3e391037-337a-41ed-99ba-3dae04ca1121", "generation": 3, "timestamp": 1754808402.4806101, "iteration_found": 77, "metrics": {"nmse": 0.4374383556793147, "nmae": 0.6955204211300365, "r2": 0.5625616443206853, "combined_score": 0.6956820068485045}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"nmse": 0.4061604030445895, "nmae": 0.6892982077613492, "r2": 0.5938395969554104, "combined_score": 0.7111564212978979}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are an expert in scaling laws and machine learning who specializes in discovering and improving scaling law functions for different LLM training scenarios. Your task is to evolve both the `scaling_law_func` function (currently a naive power law) and the `fit_scaling_law` optimization algorithm (currently a naive BFGS) to better model the relationship between learning rate, batch size, data size, model parameters and training loss.\n\nYou are allowed to decide the number of parameters in the scaling law function.\n\nFocus on mathematical accuracy across different hyperparameter scales, cross-configuration generalization, parameter efficiency (simple forms that can be fitted with limited data), and numerical/theoretical stability.\n\n**DATA CHARACTERISTICS:**\n- Features: [lr, bsz, data_size, non_embedding_param_size] - 4D input\n- Labels: lm loss - scalar output\n- lr: learning rate\n- bsz: batch size\n- data_size: amount of training data\n- non_embedding_param_size: number of non-embedding parameters in the model\n- lm loss: language model loss (filtered to <= 4.0 to remove outliers)\n- Data explores the relationship between training hyperparameters and model performance\n- Test set consists of points with the largest data_size or non_embedding_param_size\n\nThe function signatures must remain:\n\n```python\ndef scaling_law_func(data_points, params):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # lr: Array of learning rates\n    # bsz: Array of batch sizes\n    # data_size: Array of data sizes\n    # non_embedding_param_size: Array of non-embedding parameter sizes\n    # Returns: Predicted lm loss values\n    - Model parameters (N) range: ~214M to ~1B parameters\n    - Training tokens (D) range: 4B to 100B tokens\n    - Learning rates range: 1.2e-4 to 2.2e-2\n    - Batch sizes range: 16 to 4096\n\ndef fit_scaling_law(data_points, loss_values):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # loss_values: Array of corresponding lm loss values\n    # Returns: Optimized parameters \n```\n\nWrite all improvements between # EVOLVE-BLOCK-START and # EVOLVE-BLOCK-END markers.\n\nYou are not allowed to use input-dependent feature in scaling_law_func, e.g., median / min / max / etc.\n", "user": "# Current Program Information\n- Current performance metrics: - nmse: 0.4062\n- nmae: 0.6893\n- r2: 0.5938\n- combined_score: 0.7112\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: r2, combined_score. Consider continuing with similar changes.\n- Metrics showing regression: nmse, nmae. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: nmse: 0.4374, nmae: 0.6955, r2: 0.5626, combined_score: 0.6957\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: nmse: 0.4371, nmae: 0.6928, r2: 0.5629, combined_score: 0.6958\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: nmse: 0.4062, nmae: 0.6893, r2: 0.5938, combined_score: 0.7112\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.6001)\n```python\nimport numpy as np\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a 2nd\u2010order polynomial design matrix in log\u2010features:\n     - intercept\n     - linear terms (log x_i)\n     - quadratic terms (log x_i)^2\n     - pairwise interactions log x_i * log x_j for i<j\n    \"\"\"\n    N, F = logX.shape\n    # total columns = 1 + F + F + F*(F-1)/2\n    P = 1 + F + F + (F * (F - 1)) // 2\n    Phi = np.ones((N, P), dtype=float)\n    # linear terms\n    start = 1\n    Phi[:, start:start+F] = logX\n    # quadratic terms\n    start_q = start + F\n    Phi[:, start_q:start_q+F] = logX**2\n    # pairwise interactions\n    idx = start_q + F\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd\u2010order log\u2010polynomial.\n\n    Args:\n      data_points: array\u2010like of shape (N,4), columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15:\n                   [c0,\n                    c1_lr, c1_bsz, c1_data, c1_param,\n                    c2_lr, c2_bsz, c2_data, c2_param,\n                    c3_lr_bsz, c3_lr_data, c3_lr_param,\n                    c3_bsz_data, c3_bsz_param, c3_data_param]\n\n    Returns:\n      y_pred: 1D array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    p = np.asarray(params, dtype=float).ravel()\n    # Avoid invalid logs\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    # Build design matrix and predict\n    Phi = _build_design_matrix(logX)\n    if p.size != Phi.shape[1]:\n        raise ValueError(f\"Expected {Phi.shape[1]} parameters, got {p.size}\")\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via weighted,\n    differentially\u2010regularized ridge regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # Avoid zeros / negatives before log\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n\n    # Build design matrix (N x 15)\n    Phi = _build_design_matrix(logX)\n    P = Phi.shape[1]\n\n    # Sample weighting to emphasize large\u2010scale points\n    # w_i = exp( 0.5 * ((log data_size + log param_size) - median) )\n    ds = logX[:, 2]\n    ps = logX[:, 3]\n    prod = ds + ps\n    med = np.median(prod)\n    w = np.exp(0.5 * (prod - med))  # shape (N,)\n\n    # Form normal equations with weights\n    # A = Phi^T (W * Phi), b = Phi^T (W * logy)\n    W = w[:, None]  # (N,1)\n    A = Phi.T.dot(Phi * W)\n    b = Phi.T.dot(logy * w)\n\n    # Differential ridge: no penalty on intercept\n    #   small on linear, moderate on quadratic, stronger on interactions\n    regs = np.zeros(P, dtype=float)\n    # indices\n    lin_slice = slice(1, 1+F)\n    quad_slice = slice(1+F, 1+2*F)\n    interact_slice = slice(1+2*F, P)\n    # strengths\n    regs[lin_slice] = 1e-6\n    regs[quad_slice] = 1e-4\n    regs[interact_slice] = 1e-2\n    # Apply to diagonal of A\n    diag_idx = np.arange(P)\n    A[diag_idx, diag_idx] += regs\n\n    # Solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\nKey features: Performs well on nmse (0.4062), Performs well on nmae (0.6893), Performs well on r2 (0.5938), Performs well on combined_score (0.7112)\n\n\n### Program 2 (Score: 0.5972)\n```python\nimport numpy as np\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a design matrix \u03a6 for:\n      - intercept\n      - linear terms in each log\u2010feature\n      - squared terms in each log\u2010feature\n      - all pairwise products of log\u2010features\n      - a cubic term in the first feature (learning rate)\n    Input:\n      logX: (N,4) array of log\u2010features [log(lr), log(bsz), log(data_size), log(param_size)]\n    Returns:\n      Phi:  (N,P) design matrix where\n        P = 1      (intercept)\n          + 4      (linear)\n          + 4      (squared)\n          + 6      (pairwise interactions among 4 features)\n          + 1      (cubic lr)\n        = 16\n    \"\"\"\n    N, F = logX.shape\n    # number of columns = intercept + linear + squared + pairwise + cubic_lr\n    P = 1 + F + F + (F*(F-1))//2 + 1\n    Phi = np.ones((N, P), dtype=float)\n    idx = 1\n\n    # linear terms\n    Phi[:, idx:idx+F] = logX\n    idx += F\n\n    # squared terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    # cubic term for learning rate (feature 0)\n    Phi[:, idx] = logX[:, 0]**3\n\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a log\u2010polynomial + cubic\u2010lr model.\n    Args:\n      data_points: array\u2010like of shape (N,4): [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 16 of coefficients\n    Returns:\n      y_pred:      (N,) array of predicted LM losses\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected input with 4 features, got {X.shape[1]}\")\n\n    # floor to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n\n    # build design matrix and predict\n    Phi = _build_design_matrix(logX)\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != Phi.shape[1]:\n        raise ValueError(f\"Expected {Phi.shape[1]} parameters, got {p.size}\")\n\n    logy_pred = Phi.dot(p)\n    return np.exp(logy_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the above log\u2010polynomial + cubic\u2010lr scaling law via closed\u2010form ridge regression.\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n    Returns:\n      params:      1D array of length 16 of fitted coefficients\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor inputs/outputs to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n\n    # build design matrix\n    Phi = _build_design_matrix(logX)   # shape (N,16)\n    P = Phi.shape[1]\n\n    # Form normal equations\n    A = Phi.T.dot(Phi)\n    b = Phi.T.dot(logy)\n\n    # Ridge regularization (excluding intercept at index 0)\n    ridge = 1e-6\n    idx = np.arange(1, P)\n    A[idx, idx] += ridge\n\n    # solve for params\n    params = np.linalg.solve(A, b)\n    return params\n```\nKey features: Performs well on nmse (0.4371), Performs well on nmae (0.6928), Performs well on r2 (0.5629), Performs well on combined_score (0.6958)\n\n\n### Program 3 (Score: 0.5978)\n```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd-order log-polynomial model:\n      log y_pred = c0\n                 + \u03a3_i c1_i log x_i\n                 + \u03a3_i c2_i (log x_i)^2\n                 + \u03a3_{i<j} c3_{ij} (log x_i)(log x_j)\n    where x = [lr, bsz, data_size, non_embedding_param_size].\n\n    Args:\n      data_points: array-like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15:\n                   [c0,\n                    c1_lr, c1_bsz, c1_data, c1_param,\n                    c2_lr, c2_bsz, c2_data, c2_param,\n                    c3_lr_bsz, c3_lr_data, c3_lr_param,\n                    c3_bsz_data, c3_bsz_param, c3_data_param]\n\n    Returns:\n      y_pred: 1D array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, dtype=float).ravel()\n    P_expected = 1 + F + F + (F * (F - 1)) // 2\n    if p.size != P_expected:\n        raise ValueError(f\"Expected {P_expected} parameters, got {p.size}\")\n\n    # avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n\n    # build design matrix\n    Phi = _build_log_poly_features(logX)\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd-order log-polynomial scaling law via closed-form ridge regression.\n\n    We apply mild ridge on linear terms, moderate on quadratic, stronger on interactions\n    for numerical stability and better generalization.\n\n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n\n    Returns:\n      params: 1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor to avoid log of zero or negative\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    # log-transform\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n\n    # build design matrix\n    Phi = _build_log_poly_features(logX)\n    _, P = Phi.shape  # should be 15\n\n    # normal equations\n    A = Phi.T.dot(Phi)\n    b = Phi.T.dot(logy)\n\n    # indices for parameter blocks\n    idx_lin  = slice(1, 1 + F)\n    idx_quad = slice(1 + F, 1 + 2*F)\n    idx_inter= slice(1 + 2*F, P)\n\n    # ridge regularization strengths\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_inter = 1e-2\n\n    # apply ridge on diagonal\n    for i in range(idx_lin.start, idx_lin.stop):\n        A[i, i] += ridge_lin\n    for i in range(idx_quad.start, idx_quad.stop):\n        A[i, i] += ridge_quad\n    for i in range(idx_inter.start, idx_inter.stop):\n        A[i, i] += ridge_inter\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n\n\ndef _build_log_poly_features(logX):\n    \"\"\"\n    Construct the design matrix for a 2nd-order polynomial in log-features.\n\n    Input:\n      logX: array of shape (N,4)\n\n    Returns:\n      Phi: array of shape (N, 15)\n    \"\"\"\n    N, F = logX.shape\n    # total params: 1 intercept + 4 linear + 4 quadratic + 6 interactions = 15\n    P = 1 + F + F + (F*(F-1))//2\n    Phi = np.empty((N, P), dtype=float)\n\n    # intercept\n    Phi[:, 0] = 1.0\n\n    # linear terms\n    start = 1\n    Phi[:, start:start+F] = logX\n    start += F\n\n    # quadratic terms\n    Phi[:, start:start+F] = logX**2\n    start += F\n\n    # pairwise interactions\n    idx = start\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    return Phi\n```\nKey features: Performs well on nmse (0.4374), Performs well on nmae (0.6955), Performs well on r2 (0.5626), Performs well on combined_score (0.6957)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.5978)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law model for LLM training hyperparameters.\nWe fit a second\u2010order polynomial in the log\u2010domain of each feature,\nincluding quadratic terms and pairwise interactions, via ridge\u2010regularized\nclosed\u2010form regression for stability, efficiency, and improved accuracy.\n\nModel form:\n   log(y_pred) = c0\n               + sum_i c1_i * log(x_i)\n               + sum_i c2_i * (log(x_i))^2\n               + sum_{i<j} c3_{ij} * log(x_i)*log(x_j)\n\nwhere x = [lr, bsz, data_size, non_embedding_param_size],\nand y_pred is the predicted LM loss.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language\u2010model loss from hyperparameters via a 2nd\u2010order log\u2010polynomial.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15:\n                   [c0,\n                    c1_lr, c1_bsz, c1_data, c1_param,\n                    c2_lr, c2_bsz, c2_data, c2_param,\n                    c3_lr_bsz, c3_lr_data, c3_lr_param,\n                    c3_bsz_data, c3_bsz_param, c3_data_param]\n\n    Returns:\n      y_pred:      1D array of length N of predicted losses.\n    \"\"\"\n    X = np.asarray(data_points, float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, float).ravel()\n    # total parameters = 1 intercept + 4 linear + 4 quadratic + 6 interactions = 15\n    P_expected = 1 + F + F + (F*(F-1))//2\n    if p.shape[0] != P_expected:\n        raise ValueError(f\"Expected {P_expected} parameters, got {p.shape[0]}\")\n\n    # floor inputs to avoid log(0)\n    X_clipped = np.maximum(X, 1e-12)\n    logX = np.log(X_clipped)        # shape (N,4)\n\n    # build design matrix Phi\n    Phi = np.ones((N, P_expected), float)\n    # linear terms\n    idx = 1\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    # predict in log\u2010domain and exponentiate\n    log_pred = Phi.dot(p)           # shape (N,)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via ridge\u2010regularized regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params:      1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    y = np.asarray(loss_values, float).ravel()\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor to avoid log(0)\n    X_clipped = np.maximum(X, 1e-12)\n    y_clipped = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_clipped)        # shape (N,4)\n    logy = np.log(y_clipped)        # shape (N,)\n\n    # build design matrix Phi\n    P = 1 + F + F + (F*(F-1))//2     # 15\n    Phi = np.ones((N, P), float)\n    idx = 1\n    # linear\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    # ridge\u2010regularized normal equations\n    ridge = 1e-6\n    A = Phi.T.dot(Phi)\n    # apply ridge only to non\u2010intercept terms\n    A[np.arange(1, P), np.arange(1, P)] += ridge\n    b = Phi.T.dot(logy)\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n\n### Program D2 (Score: 0.5978)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nImproved scaling\u2010law discovery for LLM training hyperparameters.\n\nWe model the logarithm of the language modeling loss as a second\u2010order\npolynomial in the logarithms of each feature (learning rate, batch size,\ndata size, parameter size), including pairwise interactions:\n\n    log(loss) \u2248 c0\n               + \u2211_{i=1..4} c_i \u00b7 z_i\n               + \u2211_{i=1..4} d_i \u00b7 (z_i)^2\n               + \u2211_{1 \u2264 i < j \u2264 4} e_{ij} \u00b7 (z_i \u00b7 z_j)\n\nwhere z = [log(lr), log(bsz), log(data_size), log(non_embedding_param_size)].\n\nWe fit this model via closed\u2010form ridge\u2010regularized least squares\nin the log\u2010domain for numerical stability and strong generalization.\n\"\"\"\n\nimport numpy as np\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a design matrix \u03a6 for a second\u2010order polynomial in log\u2010features.\n    Input:\n      logX: (N,4) array of log\u2010transformed features.\n    Returns:\n      Phi: (N, 1 + 4 + 4 + 6) = (N,15) design matrix\n           [1, z1, z2, z3, z4, z1^2, z2^2, z3^2, z4^2,\n            z1\u00b7z2, z1\u00b7z3, z1\u00b7z4, z2\u00b7z3, z2\u00b7z4, z3\u00b7z4]\n    \"\"\"\n    N, F = logX.shape\n    # number of terms: intercept + 4 linear + 4 quadratic + 6 interactions = 15\n    P = 1 + F + F + (F*(F-1))//2\n    Phi = np.ones((N, P), dtype=float)\n    # linear terms\n    Phi[:, 1:1+F] = logX\n    # quadratic terms\n    Phi[:, 1+F:1+2*F] = logX**2\n    # pairwise interactions\n    idx = 1 + 2*F\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via the learned scaling law.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15 of fitted coefficients.\n\n    Returns:\n      y_pred:      1D array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected input with 4 features, got {X.shape[1]}\")\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != 15:\n        raise ValueError(f\"Expected 15 parameters, got {p.size}\")\n    # avoid log of zero or negative\n    X = np.maximum(X, 1e-12)\n    # log\u2010transform features\n    logX = np.log(X)\n    Phi = _build_design_matrix(logX)     # (N,15)\n    log_pred = Phi.dot(p)                # (N,)\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the second\u2010order log\u2010polynomial scaling law via ridge regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params:      1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Mismatch between number of points and loss values\")\n    # clip to avoid log of zero or negative\n    X = np.maximum(X, 1e-12)\n    y = np.maximum(y, 1e-12)\n    # log\u2010transform\n    logX = np.log(X)     # (N,4)\n    logy = np.log(y)     # (N,)\n    # build design matrix\n    Phi = _build_design_matrix(logX)  # (N,15)\n    # closed\u2010form ridge regression: solve (\u03a6\u1d40\u03a6 + \u03bb\u00b7diag)*p = \u03a6\u1d40 logy\n    ridge = 1e-4\n    P = Phi.shape[1]\n    A = Phi.T.dot(Phi)              # (15,15)\n    # apply ridge to all except intercept\n    A[1:,1:] += ridge * np.eye(P-1)\n    b = Phi.T.dot(logy)             # (15,)\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 2.2831, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nImproved, robust scaling law model for LLM training hyperparameters.\n\nWe model the asymptotic loss floor plus a single power\u2010law term\nin log\u2010space for numerical stability and parsimony:\n    y_pred = y_inf + A * P^{\u03b1_P} * D^{\u03b1_D} * (bsz * lr)^{\u03b1_E}\n\nwhere:\n    P   = non\u2010embedding parameter count\n    D   = data size (tokens)\n    bsz = batch size\n    lr  = learning rate\nand we fit the reparameterized vector:\n    p = [ logA, \u03b1_P, \u03b1_D, \u03b1_E, log_y_inf ]\n\nFitting is done by minimizing the mean squared error in log\u2010loss space:\n    MSE( log(y_pred) - log(y_true) ) \nwith simple box\u2010constraints to keep y_inf below the observed minimum.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via the fitted scaling law.\n    Args:\n      data_points: array\u2010like of shape (N,4): [lr, bsz, data_size, P_size]\n      params:      array of shape (5,) or (T,5): [logA, \u03b1_P, \u03b1_D, \u03b1_E, log_y_inf]\n    Returns:\n      y_pred: np.ndarray of shape (N,) if params is (5,),\n              or (N,T) if params is (T,5).\n    \"\"\"\n    X = np.asarray(data_points, float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input shape (N,4), got {X.shape}\")\n    # unpack features\n    lr    = X[:, 0]\n    bsz   = X[:, 1]\n    D     = X[:, 2]\n    Psize = X[:, 3]\n    # combine bsz and lr into a single effective exponent\n    E     = lr * bsz\n    # take logs of factors\n    logP = np.log(Psize)\n    logD = np.log(D)\n    logE = np.log(E + 1e-20)\n    # normalize params array\n    p = np.asarray(params, float)\n    if p.ndim == 1:\n        p = p.reshape(1, -1)\n    T, Pcount = p.shape\n    if Pcount != 5:\n        raise ValueError(f\"Expected params with 5 entries, got {Pcount}\")\n    # unpack parameter vectors\n    logA     = p[:, 0]   # shape (T,)\n    alpha_P  = p[:, 1]\n    alpha_D  = p[:, 2]\n    alpha_E  = p[:, 3]\n    log_yinf = p[:, 4]\n    # compute the power\u2010law term in log\u2010space: shape (N,T)\n    # log_term[n,t] = logA[t] + alpha_P[t]*logP[n] + alpha_D[t]*logD[n] + alpha_E[t]*logE[n]\n    # we broadcast: (N,1)+(1,T)*(N,1) etc.\n    log_term = (\n        logA[np.newaxis, :]\n        + np.outer(logP, alpha_P)\n        + np.outer(logD, alpha_D)\n        + np.outer(logE, alpha_E)\n    )  # shape (N,T)\n    # exponentiate the term\n    term = np.exp(log_term)\n    # exponentiate y_inf\n    y_inf = np.exp(log_yinf)[np.newaxis, :]  # shape (1,T)\n    # predicted loss\n    y_all = y_inf + term  # shape (N,T)\n    # return (N,) if single param vector\n    if y_all.shape[1] == 1:\n        return y_all[:, 0]\n    return y_all\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 5\u2010parameter scaling law by minimizing log\u2010MSE:\n        MSE( log(y_pred) - log(y_true) )\n    with simple box\u2010constraints on exponents and y_inf.\n    Returns optimized parameter vector [logA, \u03b1_P, \u03b1_D, \u03b1_E, log_y_inf].\n    \"\"\"\n    X = np.asarray(data_points, float)\n    y = np.asarray(loss_values, float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"Number of data points and loss values must match.\")\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected 4 features per data point, got {X.shape[1]}\")\n    # ensure y > 0\n    eps = 1e-8\n    y_safe = np.maximum(y, eps)\n    # compute logs of features once\n    lr    = X[:, 0]\n    bsz   = X[:, 1]\n    D     = X[:, 2]\n    Psize = X[:, 3]\n    E     = lr * bsz\n    logP = np.log(Psize + 1e-20)\n    logD = np.log(D + 1e-20)\n    logE = np.log(E + 1e-20)\n    # initial parameter guesses\n    min_y = np.min(y_safe)\n    p0 = np.array([\n        0.0,       # logA\n       -0.5,       # \u03b1_P\n       -0.3,       # \u03b1_D\n       -0.2,       # \u03b1_E\n        np.log(min_y * 0.05 + eps)  # log_y_inf (5% of min observed)\n    ], dtype=float)\n    # bounds: allow exponents in a reasonable range, keep y_inf below half of min(y)\n    bounds = [\n        (None, None),           # logA free\n        (-5.0, 5.0),            # \u03b1_P\n        (-5.0, 5.0),            # \u03b1_D\n        (-5.0, 5.0),            # \u03b1_E\n        (None, np.log(min_y * 0.5 + eps))  # log_y_inf \u2264 log(0.5 * min_y)\n    ]\n    # objective: mean squared error in log\u2010space\n    def objective(p):\n        logA, aP, aD, aE, log_yinf = p\n        # compute log_term\n        lt = (\n            logA\n            + aP * logP[:, None]\n            + aD * logD[:, None]\n            + aE * logE[:, None]\n        ).ravel()\n        # stable log\u2010sum\u2010exp for y_pred:\n        # log(y_inf + exp(lt)) = M + log(exp(log_yinf - M) + exp(lt - M))\n        M = np.maximum(log_yinf, lt)\n        logy_pred = M + np.log(np.exp(log_yinf - M) + np.exp(lt - M))\n        # residual in log-space\n        return np.mean((logy_pred - np.log(y_safe))**2)\n    # run L-BFGS-B\n    res = minimize(\n        objective,\n        p0,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 2000, 'ftol': 1e-12}\n    )\n    if res.success:\n        return res.x\n    else:\n        # fallback to initial\n        return p0.copy()\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (40.963), Excellent nmae (8.108)\n\n\n### Inspiration 2 (Score: 0.5978, Type: Experimental)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law model for LLM training hyperparameters.\nWe fit a second\u2010order polynomial in the log\u2010domain of each feature,\nincluding quadratic terms and pairwise interactions, via ridge\u2010regularized\nclosed\u2010form regression for stability, efficiency, and improved accuracy.\n\nModel form:\n   log(y_pred) = c0\n               + sum_i c1_i * log(x_i)\n               + sum_i c2_i * (log(x_i))^2\n               + sum_{i<j} c3_{ij} * log(x_i)*log(x_j)\n\nwhere x = [lr, bsz, data_size, non_embedding_param_size],\nand y_pred is the predicted LM loss.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language\u2010model loss from hyperparameters via a 2nd\u2010order log\u2010polynomial.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15:\n                   [c0,\n                    c1_lr, c1_bsz, c1_data, c1_param,\n                    c2_lr, c2_bsz, c2_data, c2_param,\n                    c3_lr_bsz, c3_lr_data, c3_lr_param,\n                    c3_bsz_data, c3_bsz_param, c3_data_param]\n\n    Returns:\n      y_pred:      1D array of length N of predicted losses.\n    \"\"\"\n    X = np.asarray(data_points, float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n\n    p = np.asarray(params, float).ravel()\n    # total parameters = 1 intercept + 4 linear + 4 quadratic + 6 interactions = 15\n    P_expected = 1 + F + F + (F*(F-1))//2\n    if p.shape[0] != P_expected:\n        raise ValueError(f\"Expected {P_expected} parameters, got {p.shape[0]}\")\n\n    # floor inputs to avoid log(0)\n    X_clipped = np.maximum(X, 1e-12)\n    logX = np.log(X_clipped)        # shape (N,4)\n\n    # build design matrix Phi\n    Phi = np.ones((N, P_expected), float)\n    # linear terms\n    idx = 1\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    # predict in log\u2010domain and exponentiate\n    log_pred = Phi.dot(p)           # shape (N,)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via ridge\u2010regularized regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params:      1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    y = np.asarray(loss_values, float).ravel()\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor to avoid log(0)\n    X_clipped = np.maximum(X, 1e-12)\n    y_clipped = np.maximum(y, 1e-12)\n\n    # log\u2010transform\n    logX = np.log(X_clipped)        # shape (N,4)\n    logy = np.log(y_clipped)        # shape (N,)\n\n    # build design matrix Phi\n    P = 1 + F + F + (F*(F-1))//2     # 15\n    Phi = np.ones((N, P), float)\n    idx = 1\n    # linear\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n\n    # ridge\u2010regularized normal equations\n    ridge = 1e-6\n    A = Phi.T.dot(Phi)\n    # apply ridge only to non\u2010intercept terms\n    A[np.arange(1, P), np.arange(1, P)] += ridge\n    b = Phi.T.dot(logy)\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, NumPy-based implementation, Comprehensive implementation\n\n\n### Inspiration 3 (Score: 0.7554, Type: Alternative)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nEvolved scaling\u2010law discovery for LLM training hyperparameters.\n\nModel form:\n    y_pred = y_inf + A * P^aP * D^aD * bsz^aB * lr^aL\n\nWe parameterize:\n    p = [logA, aP, aD, aB, aL, log_yinf]\n\nWe initialize by subtracting an initial y_inf guess and fitting\na linear least\u2010squares in log\u2010space, then refine all parameters\nvia L-BFGS-B with analytic gradients.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language modeling loss from hyperparameters.\n    data_points: (N,4) array columns [lr, bsz, data_size, non_embedding_param_size]\n    params:      array of length 6 or shape (T,6):\n                 [logA, aP, aD, aB, aL, log_yinf]\n    Returns:\n      y_pred: shape (N,) if single param set or (N,T) for multiple sets.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[None, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected 4 features (lr,bsz,D,P), got {F}\")\n    # unpack features\n    lr    = X[:, 0]\n    bsz   = X[:, 1]\n    D     = X[:, 2]\n    Psize = X[:, 3]\n\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        p = p[None, :]\n    T, Pcount = p.shape\n    if Pcount != 6:\n        raise ValueError(f\"Expected 6 parameters, got {Pcount}\")\n\n    # Precompute logs\n    log_lr   = np.log(np.maximum(lr, 1e-12))\n    log_bsz  = np.log(np.maximum(bsz, 1.0))\n    log_D    = np.log(np.maximum(D, 1.0))\n    log_P    = np.log(np.maximum(Psize, 1.0))\n\n    y_all = np.zeros((N, T), dtype=float)\n    for t in range(T):\n        logA, aP, aD, aB, aL, log_yinf = p[t]\n        # log\u2010term for the power law piece\n        log_term = (logA\n                    + aP * log_P\n                    + aD * log_D\n                    + aB * log_bsz\n                    + aL * log_lr)\n        term = np.exp(log_term)\n        y_inf = np.exp(log_yinf)\n        y_all[:, t] = y_inf + term\n\n    return y_all[:, 0] if T == 1 else y_all\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter scaling law by minimizing MSE on observed losses.\n    Returns optimized parameter vector [logA, aP, aD, aB, aL, log_yinf].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X[None, :]\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"Number of data points and loss values must match.\")\n\n    # 1) Initial guess for y_inf and linear LS on log\u2010space\n    y_min = max(np.min(y), 1e-8)\n    y_inf0 = y_min * 0.5\n\n    # prevent zeros before logging\n    lr    = X[:, 0]; bsz  = X[:, 1]\n    D     = X[:, 2]; Psize = X[:, 3]\n    log_lr   = np.log(np.maximum(lr, 1e-12))\n    log_bsz  = np.log(np.maximum(bsz, 1.0))\n    log_D    = np.log(np.maximum(D, 1.0))\n    log_P    = np.log(np.maximum(Psize, 1.0))\n\n    # subtract floor and take logs\n    y_tilde = np.maximum(y - y_inf0, 1e-8)\n    log_y   = np.log(y_tilde)\n\n    # Design matrix for [logA, aP, aD, aB, aL]\n    M = np.vstack([\n        np.ones_like(log_y),\n        log_P,\n        log_D,\n        log_bsz,\n        log_lr\n    ]).T\n    # solve least squares\n    beta, *_ = np.linalg.lstsq(M, log_y, rcond=None)\n    # beta = [logA, aP, aD, aB, aL]\n    p0 = np.concatenate([beta, [np.log(y_inf0)]])\n\n    # 2) refine by minimizing MSE with analytic gradient\n    # bounds: keep exponents and logs in reasonable range\n    lb = [-20.0, -10.0, -10.0, -10.0, -10.0, np.log(1e-8)]\n    ub = [ 20.0,  10.0,  10.0,  10.0,  10.0, np.log(y_min)]\n    bounds = list(zip(lb, ub))\n\n    # reuse logs computed above inside objective\n    def obj_and_grad(p):\n        logA, aP, aD, aB, aL, log_yinf = p\n        y_inf = np.exp(log_yinf)\n        # power\u2010law term\n        log_term = (logA\n                    + aP * log_P\n                    + aD * log_D\n                    + aB * log_bsz\n                    + aL * log_lr)\n        term = np.exp(log_term)\n        y_pred = y_inf + term\n        r = y_pred - y\n        mse = np.mean(r * r)\n\n        # gradients\n        Ninv2 = 2.0 / float(y.size)\n        g_logA      = Ninv2 * np.sum(r * term)\n        g_aP        = Ninv2 * np.sum(r * term * log_P)\n        g_aD        = Ninv2 * np.sum(r * term * log_D)\n        g_aB        = Ninv2 * np.sum(r * term * log_bsz)\n        g_aL        = Ninv2 * np.sum(r * term * log_lr)\n        g_log_yinf  = Ninv2 * np.sum(r * y_inf)\n\n        grad = np.array([\n            g_logA,\n            g_aP,\n            g_aD,\n            g_aB,\n            g_aL,\n            g_log_yinf\n        ], dtype=float)\n        return mse, grad\n\n    result = minimize(\n        fun=lambda p: obj_and_grad(p)[0],\n        x0=p0,\n        jac=lambda p: obj_and_grad(p)[1],\n        bounds=bounds,\n        method=\"L-BFGS-B\",\n        options={\"maxiter\":1000, \"ftol\":1e-12}\n    )\n\n    if result.success:\n        return result.x\n    else:\n        # fallback to linear\u2010LS initial guess\n        return p0\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (1.785), Excellent nmae (1.663)\n\n\n### Inspiration 4 (Score: 0.7552, Type: Alternative)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Normalization constants to keep feature scales ~1\nP0   = 1e8     # ~1e8 non\u2010embedding parameters\nD0   = 1e10    # ~1e10 training tokens\nBSZ0 = 512.0   # batch size\nLR0  = 1e-3    # learning rate\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    6\u2010parameter power\u2010law with asymptotic floor:\n      y_pred = y_inf + A * (P/P0)^a_P * (D/D0)^a_D * (bsz/BSZ0)^a_bsz * (lr/LR0)^a_lr\n\n    data_points: array (N,4) columns [lr, bsz, data_size, non_embedding_param_size]\n    params:      array shape (6,) or (T,6): [logA, a_P, a_D, a_bsz, a_lr, log_yinf]\n    Returns:\n      y_pred: shape (N,) if single param row, or (N,T) for multiple.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected (N,4) input, got {X.shape}\")\n    # normalize features\n    lr   = X[:, 0] / LR0\n    bsz  = X[:, 1] / BSZ0\n    D    = X[:, 2] / D0\n    P    = X[:, 3] / P0\n\n    p = np.atleast_2d(np.asarray(params, dtype=float))\n    if p.shape[1] != 6:\n        raise ValueError(f\"Expected params shape (_,6), got {p.shape}\")\n    # unpack parameters\n    logA, aP, aD, aB, aL, log_yinf = p.T\n    A     = np.exp(logA)\n    y_inf = np.exp(log_yinf)\n\n    # compute term via broadcasting: shape (N,1) ** (1,T) \u2192 (N,T)\n    term = (\n        (P[:, None] ** aP[None, :]) *\n        (D[:, None] ** aD[None, :]) *\n        (bsz[:, None] ** aB[None, :]) *\n        (lr[:, None] ** aL[None, :]) *\n        (A[None, :])\n    )\n    y_pred = y_inf[None, :] + term\n\n    # if only one parameter set, return (N,)\n    return y_pred[:, 0] if y_pred.shape[1] == 1 else y_pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter scaling law by:\n      (1) estimating y_inf via the min observed loss,\n      (2) doing linear least\u2010squares on log(y \u2212 y_inf),\n      (3) refining with L-BFGS-B to minimize MSE in original space.\n\n    Returns: array [logA, a_P, a_D, a_bsz, a_lr, log_yinf]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4 or y.shape[0] != N:\n        raise ValueError(\"Mismatch between data_points shape and loss_values length.\")\n\n    # ensure positivity\n    y = np.maximum(y, 1e-12)\n    y_min = np.min(y)\n\n    # 1) Initial floor estimate\n    y_inf0 = y_min * 0.9\n\n    # 2) Linearized power\u2010law fit on log(y - y_inf0)\n    z = y - y_inf0\n    if np.any(z <= 0):\n        z += (abs(np.min(z)) + 1e-12)\n    log_z = np.log(z)\n\n    # normalize features for linear fit\n    lr   = X[:, 0] / LR0\n    bsz  = X[:, 1] / BSZ0\n    D    = X[:, 2] / D0\n    P    = X[:, 3] / P0\n\n    log_lr  = np.log(lr)\n    log_bsz = np.log(bsz)\n    log_D   = np.log(D)\n    log_P   = np.log(P)\n\n    # design matrix: [1, log_P, log_D, log_bsz, log_lr]\n    M = np.stack([np.ones(N), log_P, log_D, log_bsz, log_lr], axis=1)\n    sol, *_ = np.linalg.lstsq(M, log_z, rcond=None)\n    logA0, aP0, aD0, aB0, aL0 = sol\n\n    # pack initial params\n    p0 = np.array([logA0, aP0, aD0, aB0, aL0, np.log(y_inf0)], dtype=float)\n\n    # bounds for stable optimization\n    bounds = [\n        (None, None),                 # logA\n        (-5.0, 5.0),                  # a_P\n        (-5.0, 5.0),                  # a_D\n        (-5.0, 5.0),                  # a_bsz\n        (-5.0, 5.0),                  # a_lr\n        (np.log(1e-12), np.log(y_min))# log_yinf\n    ]\n\n    # objective: MSE in original loss space\n    def _obj(p):\n        y_pred = scaling_law_func(X, p)\n        return np.mean((y_pred - y) ** 2)\n\n    res = minimize(\n        _obj,\n        p0,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 1000, 'ftol': 1e-12}\n    )\n\n    return res.x if res.success else p0\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (1.783), Excellent nmae (1.662)\n\n\n### Inspiration 5 (Score: 0.6387, Type: Alternative)\n```python\n\"\"\"\nImproved scaling\u2010law model for LLM training hyperparameters.\n\nWe model the LM loss as an additive floor plus an exponential of a\n2nd\u2010order polynomial in the log\u2010features:\n\n    y_pred = y_inf + exp( p0\n                         + p1*L_lr    + p2*L_bsz    + p3*L_data    + p4*L_param\n                         + p5*L_lr^2  + p6*L_bsz^2  + p7*L_data^2  + p8*L_param^2\n                         + p9*(L_data * L_param) )\n\nwhere L_x = log(x), x = [lr, bsz, data_size, non_embedding_param_size],\nand y_inf is a learned asymptotic loss floor.  We fit y_inf by\nalternating closed\u2010form ridge regression for the log\u2010term coefficients\nand a moment\u2010matching update for the floor.\n\"\"\"\n\nimport numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss with an additive floor and a 2nd\u2010order log\u2010polynomial.\n\n    Args:\n      data_points: array\u2010like of shape (N,4) columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 11:\n                   [y_inf,\n                    p0,\n                    p1_lr, p2_bsz, p3_data, p4_param,\n                    p5_lr2, p6_bsz2, p7_data2, p8_param2,\n                    p9_data_param]\n\n    Returns:\n      y_pred: 1D numpy array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    p_all = np.asarray(params, dtype=float).ravel()\n    if p_all.size != 11:\n        raise ValueError(f\"Expected 11 parameters, got {p_all.size}\")\n    # unpack floor and polynomial coefficients\n    y_inf = p_all[0]\n    p = p_all[1:]\n\n    # clamp to avoid log(0)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n\n    # Build design matrix: intercept, linear, quadratic, one interaction\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])\n    log_term = Phi.dot(p)\n    return y_inf + np.exp(log_term)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the scaling law (floor + log\u2010polynomial) by alternating\n    ridge regression for the polynomial and a simple moment update\n    for the floor.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D numpy array of length 11:\n              [y_inf,\n               p0,\n               p1_lr, p2_bsz, p3_data, p4_param,\n               p5_lr2, p6_bsz2, p7_data2, p8_param2,\n               p9_data_param]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # build design matrix once (for the exp\u2010term)\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    L_lr, L_bsz, L_data, L_param = logX.T\n    Phi = np.column_stack([\n        np.ones(N),\n        L_lr, L_bsz, L_data, L_param,\n        L_lr**2, L_bsz**2, L_data**2, L_param**2,\n        L_data * L_param\n    ])  # shape (N,10)\n\n    # initialize y_inf near 90% of min observed loss\n    y_inf = max(np.min(y) * 0.9, 0.0)\n    ridge = 1e-6\n    eps = 1e-12\n\n    # alternate: solve for p given y_inf, then update y_inf by mean residual\n    for _ in range(4):\n        # residual above floor\n        z = y - y_inf\n        # ensure positivity for log\n        z = np.maximum(z, eps)\n        logz = np.log(z)\n        # closed\u2010form ridge regression for p: minimize ||Phi p - logz||^2 + ridge||p[1:]||^2\n        A = Phi.T.dot(Phi)\n        # regularize all but the intercept term p0\n        A[1:, 1:] += ridge\n        b = Phi.T.dot(logz)\n        p = np.linalg.solve(A, b)\n        # update floor by mean of residuals\n        term = np.exp(Phi.dot(p))\n        y_inf = np.mean(y - term)\n        # enforce non-negative floor\n        if y_inf < 0:\n            y_inf = 0.0\n\n    # pack parameters\n    params = np.empty(11, dtype=float)\n    params[0] = y_inf\n    params[1:] = p\n    return params\n\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmae (0.907), NumPy-based implementation\n\n\n\n# Current Program\n```python\nimport numpy as np\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a 2nd\u2010order polynomial design matrix in log\u2010features:\n     - intercept\n     - linear terms (log x_i)\n     - quadratic terms (log x_i)^2\n     - pairwise interactions log x_i * log x_j for i<j\n    \"\"\"\n    N, F = logX.shape\n    # total columns = 1 + F + F + F*(F-1)/2\n    P = 1 + F + F + (F * (F - 1)) // 2\n    Phi = np.ones((N, P), dtype=float)\n    # linear terms\n    start = 1\n    Phi[:, start:start+F] = logX\n    # quadratic terms\n    start_q = start + F\n    Phi[:, start_q:start_q+F] = logX**2\n    # pairwise interactions\n    idx = start_q + F\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd\u2010order log\u2010polynomial.\n\n    Args:\n      data_points: array\u2010like of shape (N,4), columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15:\n                   [c0,\n                    c1_lr, c1_bsz, c1_data, c1_param,\n                    c2_lr, c2_bsz, c2_data, c2_param,\n                    c3_lr_bsz, c3_lr_data, c3_lr_param,\n                    c3_bsz_data, c3_bsz_param, c3_data_param]\n\n    Returns:\n      y_pred: 1D array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    p = np.asarray(params, dtype=float).ravel()\n    # Avoid invalid logs\n    X_safe = np.maximum(X, 1e-12)\n    logX = np.log(X_safe)\n    # Build design matrix and predict\n    Phi = _build_design_matrix(logX)\n    if p.size != Phi.shape[1]:\n        raise ValueError(f\"Expected {Phi.shape[1]} parameters, got {p.size}\")\n    log_pred = Phi.dot(p)\n    return np.exp(log_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd\u2010order log\u2010polynomial scaling law via weighted,\n    differentially\u2010regularized ridge regression.\n\n    Args:\n      data_points: array\u2010like of shape (N,4)\n      loss_values: array\u2010like of shape (N,)\n\n    Returns:\n      params: 1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    if y.size != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # Avoid zeros / negatives before log\n    X_safe = np.maximum(X, 1e-12)\n    y_safe = np.maximum(y, 1e-12)\n\n    logX = np.log(X_safe)\n    logy = np.log(y_safe)\n\n    # Build design matrix (N x 15)\n    Phi = _build_design_matrix(logX)\n    P = Phi.shape[1]\n\n    # Sample weighting to emphasize large\u2010scale points\n    # w_i = exp( 0.5 * ((log data_size + log param_size) - median) )\n    ds = logX[:, 2]\n    ps = logX[:, 3]\n    prod = ds + ps\n    med = np.median(prod)\n    w = np.exp(0.5 * (prod - med))  # shape (N,)\n\n    # Form normal equations with weights\n    # A = Phi^T (W * Phi), b = Phi^T (W * logy)\n    W = w[:, None]  # (N,1)\n    A = Phi.T.dot(Phi * W)\n    b = Phi.T.dot(logy * w)\n\n    # Differential ridge: no penalty on intercept\n    #   small on linear, moderate on quadratic, stronger on interactions\n    regs = np.zeros(P, dtype=float)\n    # indices\n    lin_slice = slice(1, 1+F)\n    quad_slice = slice(1+F, 1+2*F)\n    interact_slice = slice(1+2*F, P)\n    # strengths\n    regs[lin_slice] = 1e-6\n    regs[quad_slice] = 1e-4\n    regs[interact_slice] = 1e-2\n    # Apply to diagonal of A\n    diag_idx = np.arange(P)\n    A[diag_idx, diag_idx] += regs\n\n    # Solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _build_design_matrix(logX):\n    \"\"\"\n    Build a 2nd-order polynomial design matrix in log-features:\n      - intercept\n      - linear terms   (log x_i)\n      - quadratic terms (log x_i)^2\n      - pairwise interactions log x_i * log x_j for i<j\n    Input:\n      logX: (N,4) array\n    Output:\n      Phi:  (N,15) design matrix\n    \"\"\"\n    N, F = logX.shape\n    # number of terms: 1 + F + F + F*(F-1)/2 = 1 + 4 + 4 + 6 = 15\n    P = 1 + 2*F + (F*(F-1))//2\n    Phi = np.ones((N, P), dtype=float)\n    idx = 1\n    # linear terms\n    Phi[:, idx:idx+F] = logX\n    idx += F\n    # quadratic terms\n    Phi[:, idx:idx+F] = logX**2\n    idx += F\n    # pairwise interactions\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX[:, i] * logX[:, j]\n            idx += 1\n    return Phi\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a 2nd-order log-polynomial.\n\n    Args:\n      data_points: array-like of shape (N,4), columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 15 of coefficients\n\n    Returns:\n      y_pred: 1D array of length N of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected input with 4 features, got {F}\")\n    # floor inputs to avoid log(0)\n    X = np.maximum(X, 1e-12)\n    logX = np.log(X)\n    # build design matrix\n    Phi = _build_design_matrix(logX)    # shape (N,15)\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != Phi.shape[1]:\n        raise ValueError(f\"Expected {Phi.shape[1]} parameters, got {p.size}\")\n    # predict in log-domain\n    logy_pred = Phi.dot(p)               # shape (N,)\n    return np.exp(logy_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 2nd-order log-polynomial scaling law via closed-form\n    ridge-regularized regression with blockwise regularization.\n\n    Args:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n\n    Returns:\n      params: 1D array of length 15 of fitted coefficients.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    N, F = X.shape\n    if F != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got {F}\")\n    if y.shape[0] != N:\n        raise ValueError(\"Number of data points and loss values must match\")\n\n    # floor inputs/outputs to avoid log(0)\n    X = np.maximum(X, 1e-12)\n    y = np.maximum(y, 1e-12)\n\n    # log-transform\n    logX = np.log(X)    # (N,4)\n    logy = np.log(y)    # (N,)\n\n    # build design matrix and normal equations\n    Phi = _build_design_matrix(logX)   # (N,15)\n    P = Phi.shape[1]\n\n    # unweighted normal equations: A p = b\n    A = Phi.T.dot(Phi)                 # (15,15)\n    b = Phi.T.dot(logy)                # (15,)\n\n    # blockwise ridge regularization (exclude intercept at index 0)\n    # linear terms indices: 1..4, quadratic: 5..8, interactions: 9..14\n    ridge_lin   = 1e-6\n    ridge_quad  = 1e-4\n    ridge_inter = 1e-2\n\n    # linear block\n    lin_slice  = slice(1, 1+F)\n    A[lin_slice, lin_slice] += ridge_lin * np.eye(F)\n    # quadratic block\n    quad_slice = slice(1+F, 1+2*F)\n    A[quad_slice, quad_slice] += ridge_quad * np.eye(F)\n    # interaction block\n    inter_slice = slice(1+2*F, P)\n    K = P - (1+2*F)\n    A[inter_slice, inter_slice] += ridge_inter * np.eye(K)\n\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END\n```"]}}}