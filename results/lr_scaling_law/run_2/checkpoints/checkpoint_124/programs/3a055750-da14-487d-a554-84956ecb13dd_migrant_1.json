{"id": "3a055750-da14-487d-a554-84956ecb13dd_migrant_1", "code": "\"\"\"\nSecond\u2010order log\u2010domain polynomial scaling law with feature normalization\nand adaptive ridge\u2010regularized closed\u2010form fitting for numerical stability\nand cross\u2010configuration generalization.\n\"\"\"\nimport numpy as np\n\n# Precomputed log\u2010domain minima and maxima for features:\n#   [learning_rate, batch_size, data_size, non_embedding_param_size]\n_LOG_F_MINS = np.log(np.array([\n    1.2e-4,   # lr min\n    16.0,     # batch size min\n    4e9,      # data size min\n    2.14e8    # non-embedding parameter size min\n], dtype=float))\n_LOG_F_MAXS = np.log(np.array([\n    2.2e-2,   # lr max\n    4096.0,   # batch size max\n    1e11,     # data size max\n    1e9       # non-embedding parameter size max\n], dtype=float))\n_LOG_F_RANGES = _LOG_F_MAXS - _LOG_F_MINS  # used to normalize to [0,1]\n\ndef _build_design_matrix(logX_norm):\n    \"\"\"\n    Build a design matrix \u03a6 for a second\u2010order polynomial in the normalized\n    log\u2010domain. Columns:\n      [1,\n       logX_norm_i for each feature (4),\n       (logX_norm_i)^2 for each feature (4),\n       logX_norm_i * logX_norm_j for all i<j (6)\n      ]\n    \"\"\"\n    N, F = logX_norm.shape\n    # total parameters = 1 + F linear + F squared + F*(F-1)/2 interactions\n    P = 1 + F + F + (F*(F-1))//2\n    Phi = np.empty((N, P), dtype=float)\n    # intercept\n    Phi[:, 0] = 1.0\n    # linear terms\n    Phi[:, 1:1+F] = logX_norm\n    # squared terms\n    start_sq = 1 + F\n    Phi[:, start_sq:start_sq+F] = logX_norm**2\n    # pairwise interactions\n    idx = start_sq + F\n    for i in range(F):\n        for j in range(i+1, F):\n            Phi[:, idx] = logX_norm[:, i] * logX_norm[:, j]\n            idx += 1\n    return Phi\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language\u2010model loss from hyperparameters via a normalized\n    second\u2010order log\u2010domain polynomial.\n    Inputs:\n      data_points: array-like of shape (N,4) for [lr, bsz, data_size, non_embed_param_size]\n      params:      array-like of length 15 (intercept + linear + squared + interactions)\n    Returns:\n      preds: array of shape (N,) of predicted LM loss values\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    # support single-sample input\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points of shape (N,4), got {X.shape}\")\n    # floor to avoid log(0)\n    X = np.maximum(X, 1e-12)\n    # log-transform & normalize to [0,1]\n    logX = np.log(X)\n    logX_norm = (logX - _LOG_F_MINS) / _LOG_F_RANGES\n    # clip small numerical drift\n    logX_norm = np.clip(logX_norm, 0.0, 1.0)\n    # build design matrix\n    Phi = _build_design_matrix(logX_norm)  # shape (N,15)\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size != Phi.shape[1]:\n        raise ValueError(f\"Expected {Phi.shape[1]} params, got {p.size}\")\n    # predict in log-domain\n    logy_pred = Phi.dot(p)                 # shape (N,)\n    return np.exp(logy_pred)\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the normalized second\u2010order polynomial scaling law via closed\u2010form\n    ridge\u2010regularized least squares with adaptive regularization.\n    Inputs:\n      data_points: array-like of shape (N,4)\n      loss_values: array-like of shape (N,)\n    Returns:\n      params:      array of length 15 (intercept + linear + squared + interactions)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    # support single-sample input\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points of shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.size:\n        raise ValueError(\"Number of data points and loss values must match\")\n    # floor to avoid log(0)\n    X = np.maximum(X, 1e-12)\n    y = np.maximum(y, 1e-12)\n    # log-transform features & target\n    logX = np.log(X)\n    logX_norm = (logX - _LOG_F_MINS) / _LOG_F_RANGES\n    logX_norm = np.clip(logX_norm, 0.0, 1.0)\n    logy = np.log(y)\n    # build design matrix\n    Phi = _build_design_matrix(logX_norm)  # shape (N,15)\n    # normal equations\n    A = Phi.T.dot(Phi)                     # shape (15,15)\n    b = Phi.T.dot(logy)                    # shape (15,)\n    # adaptive ridge: \u03bb = 1e-6 * trace(A) / P\n    Pdim = A.shape[0]\n    ridge = 1e-6 * np.trace(A) / Pdim\n    # penalize all but intercept\n    A[np.diag_indices(Pdim)] += ridge\n    A[0, 0] -= ridge\n    # solve for parameters\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END", "language": "python", "parent_id": "3a055750-da14-487d-a554-84956ecb13dd", "generation": 4, "timestamp": 1754806559.104806, "iteration_found": 0, "metrics": {"nmse": 0.009529812593006361, "nmae": 0.10319057777458805, "r2": 0.9904701874069937, "combined_score": 0.990560147432864}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"nmse": 0.43745284046690064, "nmae": 0.6955378006319771, "r2": 0.5625471595330993, "combined_score": 0.6956749966664568}, "island": 1, "migrant": true}, "artifacts_json": null, "artifact_dir": null}