{"id": "d657963a-a403-46de-a1ba-c0d5e1c0b732", "code": "import numpy as np\n\n# Try to import a robust optimizer; otherwise we'll fall back to the OLS initialization\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Floor\u2010augmented power\u2010law scaling for LM loss:\n      log_pred = logaddexp(a0 + w\u00b7logX, u_floor)\n      loss_pred = exp(log_pred)\n\n    Args:\n      data_points: array of shape (N,4), columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:       1D array of length 6 = [a0, w1, w2, w3, w4, u_floor],\n                    or 2D array of shape (T,6) for T parallel parameter sets.\n\n    Returns:\n      If params is (6,), returns (N,) predicted losses.\n      If params is (T,6), returns (N,T) predicted losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # Clip to avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    # ensure we have shape (T,6)\n    if p.ndim == 1:\n        if p.shape[0] != 6:\n            raise ValueError(f\"Expected params of length 6, got {p.shape[0]}\")\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n\n    # Unpack parameters\n    a0     = p[:, 0]       # shape (T,)\n    weights= p[:, 1:5]     # shape (T,4)\n    u_floor= p[:, 5]       # shape (T,)\n\n    # Compute un\u2010floored log\u2010prediction: (N,T)\n    log_term = logX.dot(weights.T) + a0[None, :]\n    # Apply the floor in log\u2010space\n    pred_log = np.logaddexp(log_term, u_floor[None, :])\n    loss_pred = np.exp(pred_log)\n\n    # If only one parameter set, return a 1D array\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter floor\u2010augmented power\u2010law by minimizing\n      MSE(log_pred, log(y)) + \u03bb * ||params||^2\n    in the log\u2010domain with L-BFGS-B (if available), otherwise return the\n    OLS\u2010initialized parameters.\n\n    Args:\n      data_points: array of shape (N,4)\n      loss_values: array of shape (N,)\n\n    Returns:\n      params: 1D array of length 6 = [a0, w1, w2, w3, w4, u_floor]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # Clip and take logs\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)   # (N,4)\n    logy = np.log(yc)   # (N,)\n\n    N = X.shape[0]\n\n    # 1) OLS init for a0 and w\n    A_lin = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    lin_p, *_ = np.linalg.lstsq(A_lin, logy, rcond=None)\n    a0_0    = lin_p[0]\n    w0      = lin_p[1:]         # (4,)\n\n    # 2) Floor init: a bit below the minimum observed log\u2010loss\n    min_logy = np.min(logy)\n    # set floor ~ 5% of min(y) in linear space => log(floor) = log(min_y) + log(0.05)\n    u0 = min_logy + np.log(0.05)\n\n    # Initial parameter vector\n    p0 = np.concatenate([[a0_0], w0, [u0]])  # shape (6,)\n\n    # Regularization strength\n    lambda_reg = 1e-3\n\n    # Objective: MSE in log domain + ridge\n    def _objective(p):\n        a0 = p[0]\n        w  = p[1:5]\n        uf = p[5]\n        lt = logX.dot(w) + a0                  # (N,)\n        pll = np.logaddexp(lt, uf)             # (N,)\n        res = pll - logy\n        mse = np.mean(res * res)\n        reg = lambda_reg * np.sum(p * p)\n        return mse + reg\n\n    # If optimizer is available, run L-BFGS-B\n    if _HAVE_MINIMIZE:\n        res = minimize(\n            _objective, p0,\n            method=\"L-BFGS-B\",\n            options={\"maxiter\": 500, \"ftol\": 1e-9}\n        )\n        if res.success:\n            return res.x\n\n    # Fallback to initial guess\n    return p0", "language": "python", "parent_id": "70917201-9871-4702-89a9-9c1522c34008", "generation": 2, "timestamp": 1754802325.7827623, "iteration_found": 63, "metrics": {"nmse": 4.5451550302496075, "nmae": 2.545778469878871, "r2": -3.5451550302496075, "combined_score": 0.18033760905598817}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"nmse": 112.9881536359387, "nmae": 14.128417620739429, "r2": -111.9881536359387, "combined_score": 0.008772841458541842}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are an expert in scaling laws and machine learning who specializes in discovering and improving scaling law functions for different LLM training scenarios. Your task is to evolve both the `scaling_law_func` function (currently a naive power law) and the `fit_scaling_law` optimization algorithm (currently a naive BFGS) to better model the relationship between learning rate, batch size, data size, model parameters and training loss.\n\nFocus on mathematical accuracy across different hyperparameter scales, cross-configuration generalization, parameter efficiency (simple forms that can be fitted with limited data), and numerical/theoretical stability.\n\n**DATA CHARACTERISTICS:**\n- Features: [lr, bsz, data_size, non_embedding_param_size] - 4D input\n- Labels: lm loss - scalar output\n- lr: learning rate\n- bsz: batch size\n- data_size: amount of training data\n- non_embedding_param_size: number of non-embedding parameters in the model\n- lm loss: language model loss (filtered to <= 4.0 to remove outliers)\n- Data explores the relationship between training hyperparameters and model performance\n- Test set consists of points with the largest data_size or non_embedding_param_size\n\nThe function signatures must remain:\n\n```python\ndef scaling_law_func(data_points, params):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # lr: Array of learning rates\n    # bsz: Array of batch sizes\n    # data_size: Array of data sizes\n    # non_embedding_param_size: Array of non-embedding parameter sizes\n    # Returns: Predicted lm loss values\n    - Model parameters (N) range: ~214M to ~1B parameters\n    - Training tokens (D) range: 4B to 100B tokens\n    - Learning rates range: 1.2e-4 to 2.2e-2\n    - Batch sizes range: 16 to 4096\n\ndef fit_scaling_law(data_points, loss_values):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # loss_values: Array of corresponding lm loss values\n    # Returns: Optimized parameters \n```\n\nWrite all improvements between # EVOLVE-BLOCK-START and # EVOLVE-BLOCK-END markers.\n\nYou are not allowed to use input-dependent feature in scaling_law_func, e.g., median / min / max / etc.\n", "user": "# Current Program Information\n- Current performance metrics: - nmse: 112.9882\n- nmae: 14.1284\n- r2: -111.9882\n- combined_score: 0.0088\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: nmse, nmae. Consider continuing with similar changes.\n- Metrics showing regression: r2, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: nmse: 43.2172, nmae: 8.6380, r2: -42.2172, combined_score: 0.0226\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: nmse: 7.2824, nmae: 3.0438, r2: -6.2824, combined_score: 0.1207\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: nmse: 4.5452, nmae: 2.5458, r2: -3.5452, combined_score: 0.1803\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9315)\n```python\nimport numpy as np\n\n# Try to import a robust optimizer; otherwise fall back to log\u2010linear with floor init\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language model loss from hyperparameters using a floor\u2010augmented\n    power\u2010law in log\u2010space:\n      log_pred = logsumexp(intercept + w\u00b7logX, u_floor)\n      loss = exp(log_pred)\n    where u_floor = log(floor) ensures floor >= 0.\n    Inputs:\n      data_points: np.ndarray (N,4) of positive hyperparameters\n                   columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:      shape (5,) or (T,5):\n                   params[...,0]    = intercept\n                   params[...,1:5]  = weights w_i\n                   params[...,5]    = u_floor (log floor)\n    Returns:\n      preds: shape (N,) if params is (5,), or (N,T) if params is (T,5)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # Avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # (N,4)\n\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        # our model has 6 parameters: intercept + 4 weights + 1 u_floor\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n    intercept = p[:, 0]           # (T,)\n    weights   = p[:, 1:5]         # (T,4)\n    u_floor   = p[:, 5]           # (T,)\n\n    # compute log_term = intercept + w\u00b7logX, shape (N,T)\n    log_term = logX.dot(weights.T) + intercept[None, :]\n    # pred_log = logsumexp(log_term, u_floor)\n    # numpy.logaddexp does exactly log(exp(a)+exp(b))\n    pred_log = np.logaddexp(log_term, u_floor[None, :])\n    loss_pred = np.exp(pred_log)  # (N,T)\n\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the floor\u2010augmented power\u2010law model by minimizing MSE in log\u2010loss domain:\n      minimize mean((log_pred - log(y))^2) + \u03bb||params||^2\n    where\n      log_pred = logaddexp(intercept + w\u00b7logX, u_floor)\n    Returns:\n      params: np.ndarray shape (6,) = [intercept, w1, w2, w3, w4, u_floor]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # prepare log\u2010features and log\u2010targets\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)   # (N,4)\n    logy = np.log(yc)   # (N,)\n\n    N = X.shape[0]\n    F = 4\n\n    # initial linear fit on log\u2010loss to get intercept and weights\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    w_lin, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    intercept0 = w_lin[0]\n    weights0   = w_lin[1:]\n    # initialize floor small fraction of min loss\n    y_min = np.min(yc)\n    floor0 = max(y_min * 0.05, eps)\n    u0 = np.log(floor0)\n    p0 = np.concatenate([[intercept0], weights0, [u0]])  # shape (6,)\n\n    # objective: MSE in log domain + ridge on all params\n    lambda_reg = 1e-3\n    def _objective(p):\n        # p: (6,)\n        b = p[0]\n        w = p[1:1+F]\n        uf = p[1+F]\n        # log_term: (N,)\n        lt = logX.dot(w) + b\n        pll = np.logaddexp(lt, uf)         # (N,)\n        res = pll - logy\n        mse = np.mean(res*res)\n        reg = lambda_reg * np.sum(p*p)\n        return mse + reg\n\n    # optimize\n    if _HAVE_MINIMIZE:\n        res = minimize(_objective, p0,\n                       method='L-BFGS-B',\n                       options={'maxiter':500, 'ftol':1e-9})\n        if res.success:\n            return res.x\n        else:\n            return p0\n    else:\n        # fallback: no optimizer\n        return p0\n# EVOLVE-BLOCK-END\n```\nKey features: Performs well on nmse (4.5452), Performs well on nmae (2.5458), Performs well on r2 (-3.5452), Performs well on combined_score (0.1803)\n\n\n### Program 2 (Score: 1.0411)\n```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using an 8\u2010parameter log\u2010law:\n      log L = a0\n            + a1\u00b7log(N)\n            + a2\u00b7log(D)\n            + a3\u00b7log(B)\n            + a4\u00b7log(\u03b7)\n            + a5\u00b7[log(\u03b7)]^2\n            + a6\u00b7[log(\u03b7)\u00b7log(B)]\n            + a7\u00b7[log(N)\u00b7log(D)]\n\n    Inputs:\n      data_points: array of shape (N,4), columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:       1D array of length 8 = [a0, a1, ..., a7]\n\n    Returns:\n      preds: array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    p = np.asarray(params, dtype=float).ravel()\n    if p.shape[0] != 8:\n        raise ValueError(f\"Expected params length 8, got {p.shape[0]}\")\n\n    # Clip to avoid log(0)\n    eps = 1e-12\n    X_clipped = np.clip(X, eps, None)\n    # Extract and log\u2010transform features\n    log_lr, log_bsz, log_D, log_N = np.log(X_clipped).T\n\n    # Build design matrix F (N \u00d7 8)\n    F = np.column_stack([\n        np.ones_like(log_lr),        # intercept\n        log_N,                       # model size\n        log_D,                       # data size\n        log_bsz,                     # batch size\n        log_lr,                      # learning rate\n        log_lr**2,                   # (log \u03b7)^2\n        log_lr * log_bsz,            # interaction \u03b7\u00b7B\n        log_N * log_D                # interaction N\u00b7D\n    ])\n\n    # Compute log\u2010loss and exponentiate\n    log_pred = F.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 8\u2010parameter log\u2010law by ridge\u2010regularized least squares in log\u2010space:\n      minimize ||F\u00b7params \u2013 log(y)||^2 + \u03bb||params||^2\n\n    where F columns = [1, log N, log D, log B, log \u03b7, (log \u03b7)^2, log \u03b7\u00b7log B, log N\u00b7log D].\n\n    Inputs:\n      data_points: array of shape (N,4)\n      loss_values: array of shape (N,)\n\n    Returns:\n      params: 1D array of length 8\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError(\"loss_values must be a 1D array of same length as data_points\")\n\n    # Clip to avoid log(0)\n    eps = 1e-12\n    X_clipped = np.clip(X, eps, None)\n    y_clipped = np.clip(y, eps, None)\n\n    # Log\u2010transform features and target\n    log_lr, log_bsz, log_D, log_N = np.log(X_clipped).T\n    log_y = np.log(y_clipped)\n\n    # Build design matrix F (N \u00d7 8)\n    F = np.column_stack([\n        np.ones_like(log_lr),\n        log_N,\n        log_D,\n        log_bsz,\n        log_lr,\n        log_lr**2,\n        log_lr * log_bsz,\n        log_N * log_D\n    ])\n\n    # Normal equations with ridge regularization\n    FtF = F.T.dot(F)\n    # Regularization strength: small fraction of average diagonal\n    avg_diag = np.trace(FtF) / FtF.shape[0]\n    lam = 1e-5 * avg_diag\n    reg = lam * np.eye(FtF.shape[0])\n\n    rhs = F.T.dot(log_y)\n    params = np.linalg.solve(FtF + reg, rhs)\n    return params\n```\nKey features: Performs well on nmse (7.2824), Performs well on nmae (3.0438), Performs well on r2 (-6.2824), Performs well on combined_score (0.1207)\n\n\n### Program 3 (Score: 2.4152)\n```python\nimport numpy as np\n\n# Try to import a robust optimizer; otherwise fall back to closed\u2010form init\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Floor\u2010augmented power\u2010law scaling:\n      log_pred = logaddexp(a0 + w\u00b7logX, u_floor)\n      loss_pred = exp(log_pred)\n    where\n      X = [lr, bsz, data_size, non_embedding_param_size]\n      params = [a0, w1, w2, w3, w4, u_floor]\n    Supports broadcasting over multiple param sets.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    # ensure shape (T,6)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n    a0     = p[:, 0]       # (T,)\n    weights= p[:, 1:5]     # (T,4)\n    ufloor = p[:, 5]       # (T,)\n\n    # compute a0 + w\u00b7logX, shape (N,T)\n    log_term = logX.dot(weights.T) + a0[None, :]\n    # apply floor in log\u2010space\n    pred_log = np.logaddexp(log_term, ufloor[None, :])\n    loss_pred = np.exp(pred_log)\n\n    # if single param set, return (N,)\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6-parameter floor\u2010augmented power\u2010law by minimizing\n      MSE(log_pred, log(y)) + \u03bb||params||^2\n    in log\u2010domain with L-BFGS-B.\n    Returns params = [a0, w1, w2, w3, w4, u_floor].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # clip to avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)       # (N,4)\n    logy = np.log(yc)       # (N,)\n\n    N = X.shape[0]\n    # 1) Linear least\u2010squares init for a0 and w\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    lin_p, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    a0_0    = lin_p[0]\n    w0      = lin_p[1:]         # (4,)\n\n    # 2) Floor init: a bit below minimum observed log-loss\n    min_logy = np.min(logy)\n    # floor roughly at 1/10 of min loss in log\u2010space\n    uf0 = min_logy - np.log(10.0)\n\n    # initial 6-vector\n    p0 = np.concatenate([[a0_0], w0, [uf0]])  # shape (6,)\n\n    # small regularization to prevent over\u2010fitting\n    lambda_reg = 1e-4\n\n    # objective in log\u2010domain\n    def _obj(p):\n        a0 = p[0]\n        w  = p[1:5]\n        uf = p[5]\n        lt = logX.dot(w) + a0           # (N,)\n        pll= np.logaddexp(lt, uf)       # (N,)\n        res= pll - logy\n        mse= np.mean(res * res)\n        reg= lambda_reg * np.sum(p * p)\n        return mse + reg\n\n    # bounds: keep u_floor \u2264 min_logy for stability\n    bounds = [(None, None)] * 5 + [(None, min_logy)]\n\n    if _HAVE_MINIMIZE:\n        res = minimize(\n            _obj, p0,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter':1000, 'ftol':1e-10}\n        )\n        if res.success:\n            return res.x\n    # fallback to initial guess if optimizer unavailable or fails\n    return p0\n# EVOLVE-BLOCK-END\n```\nKey features: Performs well on nmse (43.2172), Performs well on nmae (8.6380), Performs well on r2 (-42.2172), Performs well on combined_score (0.0226)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 3.5631)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nExtended quadratic\u2013interaction log\u2010law for LLM training loss:\n  log\u2009L = a0\n        + a1\u00b7log(N)\n        + a2\u00b7log(D)\n        + a3\u00b7log(B)\n        + a4\u00b7[log(B)]^2\n        + a5\u00b7log(\u03b7)\n        + a6\u00b7[log(\u03b7)]^2\n        + a7\u00b7[log(\u03b7)\u00b7log(B)]\n\nwhere\n  \u03b7 = learning rate\n  B = batch size\n  D = data_size (tokens)\n  N = non_embedding_param_size\n\nWe fit via ridge\u2010regularized least squares in log\u2010space for numerical stability,\nwith no penalty on the intercept and small penalty on other coefficients.\nSupports both single and parallel parameter sets.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using the 8\u2010parameter\n    quadratic\u2013interaction log\u2010law.\n\n    Args:\n      data_points: array of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 8, or 2D array of shape (T,8)\n\n    Returns:\n      If params.ndim==1: (N,) array of predicted losses.\n      If params.ndim==2: (N,T) array of predicted losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # unpack and safe\u2010clip\n    lr   = np.clip(X[:, 0], 1e-12, None)\n    bsz  = np.clip(X[:, 1], 1e-12, None)\n    D    = np.clip(X[:, 2], 1e-12, None)\n    Np   = np.clip(X[:, 3], 1e-12, None)\n\n    # logs\n    log_lr   = np.log(lr)\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_Np   = np.log(Np)\n\n    # build design matrix (N \u00d7 8)\n    A = np.column_stack([\n        np.ones_like(log_lr),           # intercept\n        log_Np,                         # model size effect\n        log_D,                          # data size effect\n        log_bsz,                        # batch size linear\n        log_bsz**2,                     # batch size quadratic\n        log_lr,                         # learning rate linear\n        log_lr**2,                      # learning rate quadratic\n        log_lr * log_bsz               # cross term\n    ])  # shape (N,8)\n\n    p = np.asarray(params, dtype=float)\n    # Single parameter vector\n    if p.ndim == 1:\n        if p.shape[0] != 8:\n            raise ValueError(f\"Expected params length 8, got {p.shape[0]}\")\n        log_pred = A.dot(p)            # shape (N,)\n        return np.exp(log_pred)\n\n    # Multiple parameter sets in parallel\n    if p.ndim == 2:\n        T, M = p.shape\n        if M != 8:\n            raise ValueError(f\"Expected params.shape[1]==8, got {M}\")\n        log_pred = A.dot(p.T)          # shape (N,T)\n        return np.exp(log_pred)\n\n    raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 8\u2010parameter quadratic\u2013interaction log\u2010law by solving\n    a ridge\u2010regularized least squares in log\u2010space:\n\n      minimize ||A\u00b7w \u2013 log(y)||^2 + w\u1d40\u00b7R\u00b7w\n\n    where A columns =\n      [1,\n       log(N),\n       log(D),\n       log(B),\n       (log(B))^2,\n       log(\u03b7),\n       (log(\u03b7))^2,\n       log(\u03b7)\u00b7log(B)]\n\n    and R = diag([0, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb]) (no penalty on intercept).\n\n    Returns:\n      params: 1D array of length 8 = [a0, a1, \u2026, a7]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n\n    # unpack and safe\u2010clip\n    lr   = np.clip(X[:, 0], 1e-12, None)\n    bsz  = np.clip(X[:, 1], 1e-12, None)\n    D    = np.clip(X[:, 2], 1e-12, None)\n    Np   = np.clip(X[:, 3], 1e-12, None)\n    y_clipped = np.clip(y, 1e-12, None)\n\n    # logs\n    log_lr   = np.log(lr)\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_Np   = np.log(Np)\n    log_y    = np.log(y_clipped)\n\n    # build design matrix A (N \u00d7 8)\n    A = np.column_stack([\n        np.ones_like(log_lr),\n        log_Np,\n        log_D,\n        log_bsz,\n        log_bsz**2,\n        log_lr,\n        log_lr**2,\n        log_lr * log_bsz\n    ])\n\n    # normal equations\n    AtA = A.T.dot(A)                      # shape (8,8)\n    AtY = A.T.dot(log_y)                 # shape (8,)\n\n    # regularization: no penalty on intercept (index 0)\n    trace = np.trace(AtA)\n    lam   = 1e-6 * (trace / (AtA.shape[0] - 1))\n    R = np.eye(AtA.shape[0]) * lam\n    R[0, 0] = 0.0\n\n    # solve (AtA + R) w = AtY\n    params = np.linalg.solve(AtA + R, AtY)\n    return params\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n\n### Program D2 (Score: 3.5807)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nExtended robust power\u2010law scaling\u2010law model for LLM training loss.\nWe augment the standard power\u2010law with a quadratic term in learning\u2010rate:\n    log L \u2248 a0 \n           + a1\u00b7log(lr) \n           + a2\u00b7(log(lr))^2 \n           + a3\u00b7log(bsz) \n           + a4\u00b7log(data_size) \n           + a5\u00b7log(non_embedding_param_size)\n\nWe fit by ridge\u2010regularized IRLS in log\u2010space with Huber weights\nfor robustness and small\u2010ridge for numerical stability.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using the extended power\u2010law.\n\n    Args:\n      data_points: array of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size], all > 0.\n      params:      array of shape (6,) or (T,6), where\n                   params[...,0] = intercept a0\n                   params[...,1] = a1 (log lr term)\n                   params[...,2] = a2 (log lr)^2 term\n                   params[...,3] = a3 (log bsz term)\n                   params[...,4] = a4 (log data_size term)\n                   params[...,5] = a5 (log param_size term)\n\n    Returns:\n      preds: shape (N,) if params is (6,), or (N,T) if params is (T,6)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"data_points must be shape (N,4), got {X.shape}\")\n    # all inputs must be positive for log\n    if np.any(X <= 0):\n        raise ValueError(\"All entries of data_points must be strictly positive\")\n    # extract and log\u2010transform features\n    lr, bsz, D, P = X[:,0], X[:,1], X[:,2], X[:,3]\n    log_lr   = np.log(lr)\n    log_lr2  = log_lr * log_lr\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_P    = np.log(P)\n    # stack extended log\u2010features: shape (N,5)\n    Z = np.stack([log_lr, log_lr2, log_bsz, log_D, log_P], axis=1)\n\n    p = np.asarray(params, dtype=float)\n    # single parameter vector\n    if p.ndim == 1:\n        if p.shape[0] != 6:\n            raise ValueError(f\"params must have length 6, got {p.shape[0]}\")\n        a0 = p[0]\n        slopes = p[1:]            # (5,)\n        log_pred = a0 + Z.dot(slopes)\n        return np.exp(log_pred)\n\n    # multiple parameter sets in parallel\n    if p.ndim == 2:\n        T, M = p.shape\n        if M != 6:\n            raise ValueError(f\"params must have shape (T,6), got {p.shape}\")\n        a0 = p[:, 0]              # (T,)\n        slopes = p[:, 1:]         # (T,5)\n        # log_pred: shape (N,T)\n        log_pred = Z.dot(slopes.T) + a0[None, :]\n        return np.exp(log_pred)\n\n    raise ValueError(f\"params array must be 1D or 2D, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter extended power\u2010law by robust IRLS in log\u2010space.\n\n    Args:\n      data_points: (N,4) array of positive hyperparameters.\n      loss_values: (N,) array of positive LM losses.\n\n    Returns:\n      params: (6,) array [a0, a1, a2, a3, a4, a5].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"data_points must be shape (N,4), got {X.shape}\")\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError(\"loss_values must be shape (N,) matching data_points\")\n    # enforce positivity\n    if np.any(X <= 0) or np.any(y <= 0):\n        raise ValueError(\"All data_points and loss_values must be strictly positive\")\n\n    # build extended log\u2010feature matrix: (N,5)\n    lr, bsz, D, P = X[:,0], X[:,1], X[:,2], X[:,3]\n    log_lr   = np.log(lr)\n    log_lr2  = log_lr * log_lr\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_P    = np.log(P)\n    logX_ext = np.stack([log_lr, log_lr2, log_bsz, log_D, log_P], axis=1)\n\n    # target\n    logy = np.log(y)\n\n    N, D_ext = logX_ext.shape  # D_ext == 5\n\n    # standardize features for conditioning\n    mu    = logX_ext.mean(axis=0)\n    sigma = logX_ext.std(axis=0)\n    # guard against zero std\n    sigma[sigma < 1e-8] = 1.0\n    Zs = (logX_ext - mu[None,:]) / sigma[None,:]\n\n    # design matrix with intercept: shape (N, 1 + D_ext) = (N,6)\n    A = np.concatenate([np.ones((N,1)), Zs], axis=1)\n\n    # small ridge for numerical stability\n    alpha = 1e-6\n\n    # initial OLS solution in log\u2010space\n    ATA = A.T.dot(A) + alpha * np.eye(D_ext+1)\n    ATy = A.T.dot(logy)\n    p = np.linalg.solve(ATA, ATy)    # shape (6,)\n\n    # IRLS with Huber weighting\n    max_iter = 12\n    for _ in range(max_iter):\n        logy_pred = A.dot(p)\n        resid = logy_pred - logy\n        # Huber threshold\n        med = np.median(np.abs(resid))\n        delta = 1.345 * max(med, 1e-3)\n        # weights\n        w = np.ones_like(resid)\n        mask = np.abs(resid) > delta\n        w[mask] = delta / np.abs(resid[mask])\n        W = w[:,None]\n        ATA_w = (A * W).T.dot(A) + alpha * np.eye(D_ext+1)\n        ATy_w = (A * W).T.dot(logy)\n        p_new = np.linalg.solve(ATA_w, ATy_w)\n        if np.allclose(p, p_new, rtol=1e-6, atol=1e-6):\n            p = p_new\n            break\n        p = p_new\n\n    # recover original\u2010scale slopes and intercept\n    slopes_std = p[1:]                 # shape (5,)\n    slopes = slopes_std / sigma        # un\u2010standardized slopes\n    intercept = p[0] - np.dot(slopes_std, mu / sigma)\n\n    return np.concatenate([[intercept], slopes])\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9315, Type: High-Performer)\n```python\nimport numpy as np\n\n# Try to import a robust optimizer; otherwise fall back to log\u2010linear with floor init\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language model loss from hyperparameters using a floor\u2010augmented\n    power\u2010law in log\u2010space:\n      log_pred = logsumexp(intercept + w\u00b7logX, u_floor)\n      loss = exp(log_pred)\n    where u_floor = log(floor) ensures floor >= 0.\n    Inputs:\n      data_points: np.ndarray (N,4) of positive hyperparameters\n                   columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:      shape (5,) or (T,5):\n                   params[...,0]    = intercept\n                   params[...,1:5]  = weights w_i\n                   params[...,5]    = u_floor (log floor)\n    Returns:\n      preds: shape (N,) if params is (5,), or (N,T) if params is (T,5)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # Avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # (N,4)\n\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        # our model has 6 parameters: intercept + 4 weights + 1 u_floor\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n    intercept = p[:, 0]           # (T,)\n    weights   = p[:, 1:5]         # (T,4)\n    u_floor   = p[:, 5]           # (T,)\n\n    # compute log_term = intercept + w\u00b7logX, shape (N,T)\n    log_term = logX.dot(weights.T) + intercept[None, :]\n    # pred_log = logsumexp(log_term, u_floor)\n    # numpy.logaddexp does exactly log(exp(a)+exp(b))\n    pred_log = np.logaddexp(log_term, u_floor[None, :])\n    loss_pred = np.exp(pred_log)  # (N,T)\n\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the floor\u2010augmented power\u2010law model by minimizing MSE in log\u2010loss domain:\n      minimize mean((log_pred - log(y))^2) + \u03bb||params||^2\n    where\n      log_pred = logaddexp(intercept + w\u00b7logX, u_floor)\n    Returns:\n      params: np.ndarray shape (6,) = [intercept, w1, w2, w3, w4, u_floor]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # prepare log\u2010features and log\u2010targets\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)   # (N,4)\n    logy = np.log(yc)   # (N,)\n\n    N = X.shape[0]\n    F = 4\n\n    # initial linear fit on log\u2010loss to get intercept and weights\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    w_lin, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    intercept0 = w_lin[0]\n    weights0   = w_lin[1:]\n    # initialize floor small fraction of min loss\n    y_min = np.min(yc)\n    floor0 = max(y_min * 0.05, eps)\n    u0 = np.log(floor0)\n    p0 = np.concatenate([[intercept0], weights0, [u0]])  # shape (6,)\n\n    # objective: MSE in log domain + ridge on all params\n    lambda_reg = 1e-3\n    def _objective(p):\n        # p: (6,)\n        b = p[0]\n        w = p[1:1+F]\n        uf = p[1+F]\n        # log_term: (N,)\n        lt = logX.dot(w) + b\n        pll = np.logaddexp(lt, uf)         # (N,)\n        res = pll - logy\n        mse = np.mean(res*res)\n        reg = lambda_reg * np.sum(p*p)\n        return mse + reg\n\n    # optimize\n    if _HAVE_MINIMIZE:\n        res = minimize(_objective, p0,\n                       method='L-BFGS-B',\n                       options={'maxiter':500, 'ftol':1e-9})\n        if res.success:\n            return res.x\n        else:\n            return p0\n    else:\n        # fallback: no optimizer\n        return p0\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (4.545), Excellent nmae (2.546)\n\n\n### Inspiration 2 (Score: 3.7843, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nImproved scaling\u2010law model for LLM training loss using a log\u2010linear (power\u2010law) form.\nPredicted loss: L = exp(a0) * lr^{a1} * bsz^{a2} * data_size^{a3} * param_size^{a4}\n\nWe fit this by linear least squares in log\u2010space:\n  log(L) = a0 + a1*log(lr) + a2*log(bsz) + a3*log(data_size) + a4*log(param_size)\n\nThis yields a simple, stable, and efficient 5-parameter model.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a log-linear scaling law.\n\n    Inputs:\n      data_points: array of shape (N,4) with columns [lr, bsz, data_size, non_embedding_param_size]\n      params:      array of shape (5,) or (T,5):\n                     params[...,0] = intercept a0\n                     params[...,1:] = exponents [a1, a2, a3, a4]\n\n    Returns:\n      If params is (5,), returns array of shape (N,) with predicted losses.\n      If params is (T,5), returns array of shape (N,T) with predictions per parameter set.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n\n    # Avoid log(0) by clipping to a small positive epsilon\n    eps = 1e-12\n    X_clipped = np.clip(X, eps, None)\n    logX = np.log(X_clipped)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    # Single parameter vector\n    if p.ndim == 1:\n        if p.shape[0] != 5:\n            raise ValueError(f\"Expected params of length 5, got {p.shape[0]}\")\n        a0 = p[0]\n        slopes = p[1:]              # shape (4,)\n        log_pred = a0 + logX.dot(slopes)  # shape (N,)\n        return np.exp(log_pred)\n\n    # Multiple parameter sets\n    if p.ndim == 2:\n        T, P = p.shape\n        if P != 5:\n            raise ValueError(f\"Expected params.shape[1]==5, got {P}\")\n        a0 = p[:, 0]                # shape (T,)\n        slopes = p[:, 1:]           # shape (T,4)\n        # Compute log predictions: (N, T)\n        log_pred = logX.dot(slopes.T) + a0[None, :]\n        return np.exp(log_pred)\n\n    raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 5-parameter log-linear model via least-squares in log-space:\n      minimize || [1, log(lr), log(bsz), log(data_size), log(param_size)] @ params - log(loss) ||^2\n\n    Inputs:\n      data_points: (N,4) array of hyperparameters\n      loss_values: (N,) array of observed LM losses\n\n    Returns:\n      params: 1D array of length 5 = [a0, a1, a2, a3, a4]\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n\n    # Clip to avoid log(0)\n    eps = 1e-12\n    X_clipped = np.clip(X, eps, None)\n    y_clipped = np.clip(y, eps, None)\n\n    # Build design matrix A = [1 | log(lr) log(bsz) log(data_size) log(param_size)]\n    logX = np.log(X_clipped)          # shape (N,4)\n    logy = np.log(y_clipped)          # shape (N,)\n    N = X.shape[0]\n    A = np.concatenate([np.ones((N, 1)), logX], axis=1)  # shape (N,5)\n\n    # Solve linear least squares: minimize ||A @ params - logy||^2\n    params, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    return params  # shape (5,)\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (112.988), Excellent nmae (14.128)\n\n\n### Inspiration 3 (Score: 3.5631, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nExtended quadratic\u2013interaction log\u2010law for LLM training loss:\n  log\u2009L = a0\n        + a1\u00b7log(N)\n        + a2\u00b7log(D)\n        + a3\u00b7log(B)\n        + a4\u00b7[log(B)]^2\n        + a5\u00b7log(\u03b7)\n        + a6\u00b7[log(\u03b7)]^2\n        + a7\u00b7[log(\u03b7)\u00b7log(B)]\n\nwhere\n  \u03b7 = learning rate\n  B = batch size\n  D = data_size (tokens)\n  N = non_embedding_param_size\n\nWe fit via ridge\u2010regularized least squares in log\u2010space for numerical stability,\nwith no penalty on the intercept and small penalty on other coefficients.\nSupports both single and parallel parameter sets.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using the 8\u2010parameter\n    quadratic\u2013interaction log\u2010law.\n\n    Args:\n      data_points: array of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 8, or 2D array of shape (T,8)\n\n    Returns:\n      If params.ndim==1: (N,) array of predicted losses.\n      If params.ndim==2: (N,T) array of predicted losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # unpack and safe\u2010clip\n    lr   = np.clip(X[:, 0], 1e-12, None)\n    bsz  = np.clip(X[:, 1], 1e-12, None)\n    D    = np.clip(X[:, 2], 1e-12, None)\n    Np   = np.clip(X[:, 3], 1e-12, None)\n\n    # logs\n    log_lr   = np.log(lr)\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_Np   = np.log(Np)\n\n    # build design matrix (N \u00d7 8)\n    A = np.column_stack([\n        np.ones_like(log_lr),           # intercept\n        log_Np,                         # model size effect\n        log_D,                          # data size effect\n        log_bsz,                        # batch size linear\n        log_bsz**2,                     # batch size quadratic\n        log_lr,                         # learning rate linear\n        log_lr**2,                      # learning rate quadratic\n        log_lr * log_bsz               # cross term\n    ])  # shape (N,8)\n\n    p = np.asarray(params, dtype=float)\n    # Single parameter vector\n    if p.ndim == 1:\n        if p.shape[0] != 8:\n            raise ValueError(f\"Expected params length 8, got {p.shape[0]}\")\n        log_pred = A.dot(p)            # shape (N,)\n        return np.exp(log_pred)\n\n    # Multiple parameter sets in parallel\n    if p.ndim == 2:\n        T, M = p.shape\n        if M != 8:\n            raise ValueError(f\"Expected params.shape[1]==8, got {M}\")\n        log_pred = A.dot(p.T)          # shape (N,T)\n        return np.exp(log_pred)\n\n    raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 8\u2010parameter quadratic\u2013interaction log\u2010law by solving\n    a ridge\u2010regularized least squares in log\u2010space:\n\n      minimize ||A\u00b7w \u2013 log(y)||^2 + w\u1d40\u00b7R\u00b7w\n\n    where A columns =\n      [1,\n       log(N),\n       log(D),\n       log(B),\n       (log(B))^2,\n       log(\u03b7),\n       (log(\u03b7))^2,\n       log(\u03b7)\u00b7log(B)]\n\n    and R = diag([0, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb]) (no penalty on intercept).\n\n    Returns:\n      params: 1D array of length 8 = [a0, a1, \u2026, a7]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n\n    # unpack and safe\u2010clip\n    lr   = np.clip(X[:, 0], 1e-12, None)\n    bsz  = np.clip(X[:, 1], 1e-12, None)\n    D    = np.clip(X[:, 2], 1e-12, None)\n    Np   = np.clip(X[:, 3], 1e-12, None)\n    y_clipped = np.clip(y, 1e-12, None)\n\n    # logs\n    log_lr   = np.log(lr)\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_Np   = np.log(Np)\n    log_y    = np.log(y_clipped)\n\n    # build design matrix A (N \u00d7 8)\n    A = np.column_stack([\n        np.ones_like(log_lr),\n        log_Np,\n        log_D,\n        log_bsz,\n        log_bsz**2,\n        log_lr,\n        log_lr**2,\n        log_lr * log_bsz\n    ])\n\n    # normal equations\n    AtA = A.T.dot(A)                      # shape (8,8)\n    AtY = A.T.dot(log_y)                 # shape (8,)\n\n    # regularization: no penalty on intercept (index 0)\n    trace = np.trace(AtA)\n    lam   = 1e-6 * (trace / (AtA.shape[0] - 1))\n    R = np.eye(AtA.shape[0]) * lam\n    R[0, 0] = 0.0\n\n    # solve (AtA + R) w = AtY\n    params = np.linalg.solve(AtA + R, AtY)\n    return params\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (98.577), Excellent nmae (13.242)\n\n\n### Inspiration 4 (Score: 2.4152, Type: High-Performer)\n```python\nimport numpy as np\n\n# Try to import a robust optimizer; otherwise fall back to closed\u2010form init\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Floor\u2010augmented power\u2010law scaling:\n      log_pred = logaddexp(a0 + w\u00b7logX, u_floor)\n      loss_pred = exp(log_pred)\n    where\n      X = [lr, bsz, data_size, non_embedding_param_size]\n      params = [a0, w1, w2, w3, w4, u_floor]\n    Supports broadcasting over multiple param sets.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    # ensure shape (T,6)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n    a0     = p[:, 0]       # (T,)\n    weights= p[:, 1:5]     # (T,4)\n    ufloor = p[:, 5]       # (T,)\n\n    # compute a0 + w\u00b7logX, shape (N,T)\n    log_term = logX.dot(weights.T) + a0[None, :]\n    # apply floor in log\u2010space\n    pred_log = np.logaddexp(log_term, ufloor[None, :])\n    loss_pred = np.exp(pred_log)\n\n    # if single param set, return (N,)\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6-parameter floor\u2010augmented power\u2010law by minimizing\n      MSE(log_pred, log(y)) + \u03bb||params||^2\n    in log\u2010domain with L-BFGS-B.\n    Returns params = [a0, w1, w2, w3, w4, u_floor].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # clip to avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)       # (N,4)\n    logy = np.log(yc)       # (N,)\n\n    N = X.shape[0]\n    # 1) Linear least\u2010squares init for a0 and w\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    lin_p, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    a0_0    = lin_p[0]\n    w0      = lin_p[1:]         # (4,)\n\n    # 2) Floor init: a bit below minimum observed log-loss\n    min_logy = np.min(logy)\n    # floor roughly at 1/10 of min loss in log\u2010space\n    uf0 = min_logy - np.log(10.0)\n\n    # initial 6-vector\n    p0 = np.concatenate([[a0_0], w0, [uf0]])  # shape (6,)\n\n    # small regularization to prevent over\u2010fitting\n    lambda_reg = 1e-4\n\n    # objective in log\u2010domain\n    def _obj(p):\n        a0 = p[0]\n        w  = p[1:5]\n        uf = p[5]\n        lt = logX.dot(w) + a0           # (N,)\n        pll= np.logaddexp(lt, uf)       # (N,)\n        res= pll - logy\n        mse= np.mean(res * res)\n        reg= lambda_reg * np.sum(p * p)\n        return mse + reg\n\n    # bounds: keep u_floor \u2264 min_logy for stability\n    bounds = [(None, None)] * 5 + [(None, min_logy)]\n\n    if _HAVE_MINIMIZE:\n        res = minimize(\n            _obj, p0,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter':1000, 'ftol':1e-10}\n        )\n        if res.success:\n            return res.x\n    # fallback to initial guess if optimizer unavailable or fails\n    return p0\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (43.217), Excellent nmae (8.638)\n\n\n### Inspiration 5 (Score: 1.0411, Type: High-Performer)\n```python\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using an 8\u2010parameter log\u2010law:\n      log L = a0\n            + a1\u00b7log(N)\n            + a2\u00b7log(D)\n            + a3\u00b7log(B)\n            + a4\u00b7log(\u03b7)\n            + a5\u00b7[log(\u03b7)]^2\n            + a6\u00b7[log(\u03b7)\u00b7log(B)]\n            + a7\u00b7[log(N)\u00b7log(D)]\n\n    Inputs:\n      data_points: array of shape (N,4), columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:       1D array of length 8 = [a0, a1, ..., a7]\n\n    Returns:\n      preds: array of shape (N,) of predicted LM losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    p = np.asarray(params, dtype=float).ravel()\n    if p.shape[0] != 8:\n        raise ValueError(f\"Expected params length 8, got {p.shape[0]}\")\n\n    # Clip to avoid log(0)\n    eps = 1e-12\n    X_clipped = np.clip(X, eps, None)\n    # Extract and log\u2010transform features\n    log_lr, log_bsz, log_D, log_N = np.log(X_clipped).T\n\n    # Build design matrix F (N \u00d7 8)\n    F = np.column_stack([\n        np.ones_like(log_lr),        # intercept\n        log_N,                       # model size\n        log_D,                       # data size\n        log_bsz,                     # batch size\n        log_lr,                      # learning rate\n        log_lr**2,                   # (log \u03b7)^2\n        log_lr * log_bsz,            # interaction \u03b7\u00b7B\n        log_N * log_D                # interaction N\u00b7D\n    ])\n\n    # Compute log\u2010loss and exponentiate\n    log_pred = F.dot(p)\n    return np.exp(log_pred)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 8\u2010parameter log\u2010law by ridge\u2010regularized least squares in log\u2010space:\n      minimize ||F\u00b7params \u2013 log(y)||^2 + \u03bb||params||^2\n\n    where F columns = [1, log N, log D, log B, log \u03b7, (log \u03b7)^2, log \u03b7\u00b7log B, log N\u00b7log D].\n\n    Inputs:\n      data_points: array of shape (N,4)\n      loss_values: array of shape (N,)\n\n    Returns:\n      params: 1D array of length 8\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError(\"loss_values must be a 1D array of same length as data_points\")\n\n    # Clip to avoid log(0)\n    eps = 1e-12\n    X_clipped = np.clip(X, eps, None)\n    y_clipped = np.clip(y, eps, None)\n\n    # Log\u2010transform features and target\n    log_lr, log_bsz, log_D, log_N = np.log(X_clipped).T\n    log_y = np.log(y_clipped)\n\n    # Build design matrix F (N \u00d7 8)\n    F = np.column_stack([\n        np.ones_like(log_lr),\n        log_N,\n        log_D,\n        log_bsz,\n        log_lr,\n        log_lr**2,\n        log_lr * log_bsz,\n        log_N * log_D\n    ])\n\n    # Normal equations with ridge regularization\n    FtF = F.T.dot(F)\n    # Regularization strength: small fraction of average diagonal\n    avg_diag = np.trace(FtF) / FtF.shape[0]\n    lam = 1e-5 * avg_diag\n    reg = lam * np.eye(FtF.shape[0])\n\n    rhs = F.T.dot(log_y)\n    params = np.linalg.solve(FtF + reg, rhs)\n    return params\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (7.282), Excellent nmae (3.044)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nImproved scaling\u2010law model for LLM training loss using a log\u2010linear (power\u2010law) form\nPredicted loss L = exp(a0) * lr^{a1} * bsz^{a2} * data_size^{a3} * param_size^{a4}\nFitting is done via a linear least\u2010squares in log\u2010space for stability and speed.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    params: 1D array of length 5 (or 2D array of shape (T,5) for T parallel models)\n            params[0] = intercept a0\n            params[1:] = slopes [a1, a2, a3, a4]\n    Returns: \n      If params is 1D, returns (N,) array of predicted losses.\n      If params is 2D (T,5), returns (N,T) array of predicted losses for each param set.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n    # Transform features to log\u2010space\n    logX = np.log(X)\n    p = np.asarray(params, dtype=float)\n    \n    # Single parameter set\n    if p.ndim == 1:\n        if p.shape[0] != 5:\n            raise ValueError(f\"Expected params of length 5, got length {p.shape[0]}\")\n        a0 = p[0]\n        slopes = p[1:]            # length 4\n        log_pred = a0 + logX.dot(slopes)\n        return np.exp(log_pred)\n    \n    # Multiple parameter sets in parallel (shape (T,5))\n    elif p.ndim == 2:\n        T, P = p.shape\n        if P != 5:\n            raise ValueError(f\"Expected params.shape[1]==5, got {P}\")\n        a0 = p[:, 0]              # shape (T,)\n        slopes = p[:, 1:]         # shape (T,4)\n        # logX @ slopes.T -> (N,T)\n        log_pred = logX.dot(slopes.T) + a0[None, :]\n        return np.exp(log_pred)\n    \n    else:\n        raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the 5-parameter log\u2010linear model by solving a linear least\u2010squares\n    problem in log\u2010space:\n      minimize || [1, log(lr), log(bsz), log(data_size), log(param_size)] @ params - log(loss) ||^2\n    Returns:\n      params: 1D array of length 5 = [a0, a1, a2, a3, a4]\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n    \n    # Small epsilon to avoid log(0)\n    eps = 1e-12\n    logX = np.log(X)\n    logy = np.log(y + eps)\n    \n    # Build design matrix A = [1 | log(lr) log(bsz) log(data_size) log(param_size)]\n    N = X.shape[0]\n    A = np.concatenate([np.ones((N, 1)), logX], axis=1)  # shape (N,5)\n    \n    # Solve least squares: minimize ||A @ p - logy||^2\n    params, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    # params is shape (5,)\n    return params\n# EVOLVE-BLOCK-END\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["```python\nimport numpy as np\n\n# Try to import a robust optimizer; otherwise we'll fall back to the OLS initialization\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Floor\u2010augmented power\u2010law scaling for LM loss:\n      log_pred = logaddexp(a0 + w\u00b7logX, u_floor)\n      loss_pred = exp(log_pred)\n\n    Args:\n      data_points: array of shape (N,4), columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:       1D array of length 6 = [a0, w1, w2, w3, w4, u_floor],\n                    or 2D array of shape (T,6) for T parallel parameter sets.\n\n    Returns:\n      If params is (6,), returns (N,) predicted losses.\n      If params is (T,6), returns (N,T) predicted losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # Clip to avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    # ensure we have shape (T,6)\n    if p.ndim == 1:\n        if p.shape[0] != 6:\n            raise ValueError(f\"Expected params of length 6, got {p.shape[0]}\")\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n\n    # Unpack parameters\n    a0     = p[:, 0]       # shape (T,)\n    weights= p[:, 1:5]     # shape (T,4)\n    u_floor= p[:, 5]       # shape (T,)\n\n    # Compute un\u2010floored log\u2010prediction: (N,T)\n    log_term = logX.dot(weights.T) + a0[None, :]\n    # Apply the floor in log\u2010space\n    pred_log = np.logaddexp(log_term, u_floor[None, :])\n    loss_pred = np.exp(pred_log)\n\n    # If only one parameter set, return a 1D array\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter floor\u2010augmented power\u2010law by minimizing\n      MSE(log_pred, log(y)) + \u03bb * ||params||^2\n    in the log\u2010domain with L-BFGS-B (if available), otherwise return the\n    OLS\u2010initialized parameters.\n\n    Args:\n      data_points: array of shape (N,4)\n      loss_values: array of shape (N,)\n\n    Returns:\n      params: 1D array of length 6 = [a0, w1, w2, w3, w4, u_floor]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # Clip and take logs\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)   # (N,4)\n    logy = np.log(yc)   # (N,)\n\n    N = X.shape[0]\n\n    # 1) OLS init for a0 and w\n    A_lin = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    lin_p, *_ = np.linalg.lstsq(A_lin, logy, rcond=None)\n    a0_0    = lin_p[0]\n    w0      = lin_p[1:]         # (4,)\n\n    # 2) Floor init: a bit below the minimum observed log\u2010loss\n    min_logy = np.min(logy)\n    # set floor ~ 5% of min(y) in linear space => log(floor) = log(min_y) + log(0.05)\n    u0 = min_logy + np.log(0.05)\n\n    # Initial parameter vector\n    p0 = np.concatenate([[a0_0], w0, [u0]])  # shape (6,)\n\n    # Regularization strength\n    lambda_reg = 1e-3\n\n    # Objective: MSE in log domain + ridge\n    def _objective(p):\n        a0 = p[0]\n        w  = p[1:5]\n        uf = p[5]\n        lt = logX.dot(w) + a0                  # (N,)\n        pll = np.logaddexp(lt, uf)             # (N,)\n        res = pll - logy\n        mse = np.mean(res * res)\n        reg = lambda_reg * np.sum(p * p)\n        return mse + reg\n\n    # If optimizer is available, run L-BFGS-B\n    if _HAVE_MINIMIZE:\n        res = minimize(\n            _objective, p0,\n            method=\"L-BFGS-B\",\n            options={\"maxiter\": 500, \"ftol\": 1e-9}\n        )\n        if res.success:\n            return res.x\n\n    # Fallback to initial guess\n    return p0\n```\nThis implementation upgrades the simple log\u2010linear model to a six\u2010parameter floor\u2010augmented power\u2010law:\n- Introduces a \u201cfloor\u201d in log\u2010space to avoid overly small extrapolations.\n- Fits intercept and slopes by OLS in log\u2010space, then refines via L-BFGS-B minimizing MSE(log_pred, log_true) with mild ridge regularization.\n- Works with 1D or batched 2D parameter arrays.\n- Maintains the original API signature and improves accuracy across a wide hyperparameter range."]}}}