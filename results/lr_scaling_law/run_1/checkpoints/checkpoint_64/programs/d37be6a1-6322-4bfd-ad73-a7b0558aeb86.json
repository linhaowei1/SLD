{"id": "d37be6a1-6322-4bfd-ad73-a7b0558aeb86", "code": "import numpy as np\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Floor\u2010augmented power\u2010law scaling:\n      log_pred = logaddexp(a0 + w\u00b7logX, u_floor)\n      loss_pred = exp(log_pred)\n\n    Inputs:\n      data_points: np.ndarray (N,4) of positive hyperparameters\n                   columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:      array of shape (6,) or (T,6):\n                     params[...,0] = intercept a0\n                     params[...,1:5] = weights w for [log(lr), log(bsz), log(data_size), log(param_size)]\n                     params[...,5] = u_floor (log of loss floor)\n    Returns:\n      If params is (6,), returns (N,) array of predictions.\n      If params is (T,6), returns (N,T) array of predictions.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n\n    a0     = p[:, 0]       # (T,)\n    weights= p[:, 1:5]     # (T,4)\n    u_floor= p[:, 5]       # (T,)\n\n    # compute a0 + w\u00b7logX \u2192 shape (N,T)\n    log_term = logX.dot(weights.T) + a0[None, :]\n    # apply floor in log\u2010space\n    pred_log = np.logaddexp(log_term, u_floor[None, :])\n    loss_pred = np.exp(pred_log)  # (N,T)\n\n    # if single parameter set, return (N,)\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter floor\u2010augmented power\u2010law by minimizing\n      MSE(log_pred, log(y)) + \u03bb\u00b7||w||^2\n    in log\u2010domain with L-BFGS-B, bounding the floor \u2264 min(log y).\n    Returns params = [a0, w1, w2, w3, w4, u_floor].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # clip to avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)   # (N,4)\n    logy = np.log(yc)   # (N,)\n\n    N = X.shape[0]\n    # 1) Linear LS init for a0, w\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    lin_p, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    a0_0 = lin_p[0]\n    w0   = lin_p[1:]   # (4,)\n\n    # 2) Floor init: small fraction of min observed loss\n    y_min = np.min(yc)\n    floor0 = max(y_min * 0.05, eps)\n    u0 = np.log(floor0)\n\n    p0 = np.concatenate([[a0_0], w0, [u0]])  # shape (6,)\n\n    # 3) Objective: MSE in log\u2010space + ridge on weights\n    lambda_reg = 1e-3\n    def _obj(p):\n        a0 = p[0]\n        w  = p[1:5]\n        uf = p[5]\n        lt = logX.dot(w) + a0              # (N,)\n        pll = np.logaddexp(lt, uf)        # (N,)\n        res = pll - logy\n        mse = np.mean(res * res)\n        reg = lambda_reg * np.sum(w * w)\n        return mse + reg\n\n    # 4) Bounds: u_floor \u2264 min(log y)\n    min_logy = np.min(logy)\n    bounds = [(None, None)] * 5 + [(None, min_logy)]\n\n    # 5) Optimize\n    if _HAVE_MINIMIZE:\n        res = minimize(_obj, p0,\n                       method='L-BFGS-B',\n                       bounds=bounds,\n                       options={'maxiter':500, 'ftol':1e-9})\n        if res.success:\n            return res.x\n    # fallback\n    return p0\n# EVOLVE-BLOCK-END", "language": "python", "parent_id": "9ffb907a-e96e-43dd-9fe7-b1a59365d373", "generation": 2, "timestamp": 1754802272.0973322, "iteration_found": 54, "metrics": {"nmse": 103.62308127983549, "nmae": 13.52235975701271, "r2": -102.62308127983549, "combined_score": 0.009558120328393873}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"nmse": 112.9881536359387, "nmae": 14.128417620739429, "r2": -111.9881536359387, "combined_score": 0.008772841458541842}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are an expert in scaling laws and machine learning who specializes in discovering and improving scaling law functions for different LLM training scenarios. Your task is to evolve both the `scaling_law_func` function (currently a naive power law) and the `fit_scaling_law` optimization algorithm (currently a naive BFGS) to better model the relationship between learning rate, batch size, data size, model parameters and training loss.\n\nFocus on mathematical accuracy across different hyperparameter scales, cross-configuration generalization, parameter efficiency (simple forms that can be fitted with limited data), and numerical/theoretical stability.\n\n**DATA CHARACTERISTICS:**\n- Features: [lr, bsz, data_size, non_embedding_param_size] - 4D input\n- Labels: lm loss - scalar output\n- lr: learning rate\n- bsz: batch size\n- data_size: amount of training data\n- non_embedding_param_size: number of non-embedding parameters in the model\n- lm loss: language model loss (filtered to <= 4.0 to remove outliers)\n- Data explores the relationship between training hyperparameters and model performance\n- Test set consists of points with the largest data_size or non_embedding_param_size\n\nThe function signatures must remain:\n\n```python\ndef scaling_law_func(data_points, params):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # lr: Array of learning rates\n    # bsz: Array of batch sizes\n    # data_size: Array of data sizes\n    # non_embedding_param_size: Array of non-embedding parameter sizes\n    # Returns: Predicted lm loss values\n    - Model parameters (N) range: ~214M to ~1B parameters\n    - Training tokens (D) range: 4B to 100B tokens\n    - Learning rates range: 1.2e-4 to 2.2e-2\n    - Batch sizes range: 16 to 4096\n\ndef fit_scaling_law(data_points, loss_values):\n    # data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    # loss_values: Array of corresponding lm loss values\n    # Returns: Optimized parameters \n```\n\nWrite all improvements between # EVOLVE-BLOCK-START and # EVOLVE-BLOCK-END markers.\n\nYou are not allowed to use input-dependent feature in scaling_law_func, e.g., median / min / max / etc.\n", "user": "# Current Program Information\n- Current performance metrics: - nmse: 112.9882\n- nmae: 14.1284\n- r2: -111.9882\n- combined_score: 0.0088\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: nmse, nmae. Consider continuing with similar changes.\n- Metrics showing regression: r2, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: nmse: 98.5771, nmae: 13.2424, r2: -97.5771, combined_score: 0.0100\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: nmse: 43.2172, nmae: 8.6380, r2: -42.2172, combined_score: 0.0226\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: nmse: 4.5452, nmae: 2.5458, r2: -3.5452, combined_score: 0.1803\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9315)\n```python\nimport numpy as np\n\n# Try to import a robust optimizer; otherwise fall back to log\u2010linear with floor init\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language model loss from hyperparameters using a floor\u2010augmented\n    power\u2010law in log\u2010space:\n      log_pred = logsumexp(intercept + w\u00b7logX, u_floor)\n      loss = exp(log_pred)\n    where u_floor = log(floor) ensures floor >= 0.\n    Inputs:\n      data_points: np.ndarray (N,4) of positive hyperparameters\n                   columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:      shape (5,) or (T,5):\n                   params[...,0]    = intercept\n                   params[...,1:5]  = weights w_i\n                   params[...,5]    = u_floor (log floor)\n    Returns:\n      preds: shape (N,) if params is (5,), or (N,T) if params is (T,5)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # Avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # (N,4)\n\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        # our model has 6 parameters: intercept + 4 weights + 1 u_floor\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n    intercept = p[:, 0]           # (T,)\n    weights   = p[:, 1:5]         # (T,4)\n    u_floor   = p[:, 5]           # (T,)\n\n    # compute log_term = intercept + w\u00b7logX, shape (N,T)\n    log_term = logX.dot(weights.T) + intercept[None, :]\n    # pred_log = logsumexp(log_term, u_floor)\n    # numpy.logaddexp does exactly log(exp(a)+exp(b))\n    pred_log = np.logaddexp(log_term, u_floor[None, :])\n    loss_pred = np.exp(pred_log)  # (N,T)\n\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the floor\u2010augmented power\u2010law model by minimizing MSE in log\u2010loss domain:\n      minimize mean((log_pred - log(y))^2) + \u03bb||params||^2\n    where\n      log_pred = logaddexp(intercept + w\u00b7logX, u_floor)\n    Returns:\n      params: np.ndarray shape (6,) = [intercept, w1, w2, w3, w4, u_floor]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # prepare log\u2010features and log\u2010targets\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)   # (N,4)\n    logy = np.log(yc)   # (N,)\n\n    N = X.shape[0]\n    F = 4\n\n    # initial linear fit on log\u2010loss to get intercept and weights\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    w_lin, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    intercept0 = w_lin[0]\n    weights0   = w_lin[1:]\n    # initialize floor small fraction of min loss\n    y_min = np.min(yc)\n    floor0 = max(y_min * 0.05, eps)\n    u0 = np.log(floor0)\n    p0 = np.concatenate([[intercept0], weights0, [u0]])  # shape (6,)\n\n    # objective: MSE in log domain + ridge on all params\n    lambda_reg = 1e-3\n    def _objective(p):\n        # p: (6,)\n        b = p[0]\n        w = p[1:1+F]\n        uf = p[1+F]\n        # log_term: (N,)\n        lt = logX.dot(w) + b\n        pll = np.logaddexp(lt, uf)         # (N,)\n        res = pll - logy\n        mse = np.mean(res*res)\n        reg = lambda_reg * np.sum(p*p)\n        return mse + reg\n\n    # optimize\n    if _HAVE_MINIMIZE:\n        res = minimize(_objective, p0,\n                       method='L-BFGS-B',\n                       options={'maxiter':500, 'ftol':1e-9})\n        if res.success:\n            return res.x\n        else:\n            return p0\n    else:\n        # fallback: no optimizer\n        return p0\n# EVOLVE-BLOCK-END\n```\nKey features: Performs well on nmse (4.5452), Performs well on nmae (2.5458), Performs well on r2 (-3.5452), Performs well on combined_score (0.1803)\n\n\n### Program 2 (Score: 2.4152)\n```python\nimport numpy as np\n\n# Try to import a robust optimizer; otherwise fall back to closed\u2010form init\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Floor\u2010augmented power\u2010law scaling:\n      log_pred = logaddexp(a0 + w\u00b7logX, u_floor)\n      loss_pred = exp(log_pred)\n    where\n      X = [lr, bsz, data_size, non_embedding_param_size]\n      params = [a0, w1, w2, w3, w4, u_floor]\n    Supports broadcasting over multiple param sets.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    # ensure shape (T,6)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n    a0     = p[:, 0]       # (T,)\n    weights= p[:, 1:5]     # (T,4)\n    ufloor = p[:, 5]       # (T,)\n\n    # compute a0 + w\u00b7logX, shape (N,T)\n    log_term = logX.dot(weights.T) + a0[None, :]\n    # apply floor in log\u2010space\n    pred_log = np.logaddexp(log_term, ufloor[None, :])\n    loss_pred = np.exp(pred_log)\n\n    # if single param set, return (N,)\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6-parameter floor\u2010augmented power\u2010law by minimizing\n      MSE(log_pred, log(y)) + \u03bb||params||^2\n    in log\u2010domain with L-BFGS-B.\n    Returns params = [a0, w1, w2, w3, w4, u_floor].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # clip to avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)       # (N,4)\n    logy = np.log(yc)       # (N,)\n\n    N = X.shape[0]\n    # 1) Linear least\u2010squares init for a0 and w\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    lin_p, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    a0_0    = lin_p[0]\n    w0      = lin_p[1:]         # (4,)\n\n    # 2) Floor init: a bit below minimum observed log-loss\n    min_logy = np.min(logy)\n    # floor roughly at 1/10 of min loss in log\u2010space\n    uf0 = min_logy - np.log(10.0)\n\n    # initial 6-vector\n    p0 = np.concatenate([[a0_0], w0, [uf0]])  # shape (6,)\n\n    # small regularization to prevent over\u2010fitting\n    lambda_reg = 1e-4\n\n    # objective in log\u2010domain\n    def _obj(p):\n        a0 = p[0]\n        w  = p[1:5]\n        uf = p[5]\n        lt = logX.dot(w) + a0           # (N,)\n        pll= np.logaddexp(lt, uf)       # (N,)\n        res= pll - logy\n        mse= np.mean(res * res)\n        reg= lambda_reg * np.sum(p * p)\n        return mse + reg\n\n    # bounds: keep u_floor \u2264 min_logy for stability\n    bounds = [(None, None)] * 5 + [(None, min_logy)]\n\n    if _HAVE_MINIMIZE:\n        res = minimize(\n            _obj, p0,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter':1000, 'ftol':1e-10}\n        )\n        if res.success:\n            return res.x\n    # fallback to initial guess if optimizer unavailable or fails\n    return p0\n# EVOLVE-BLOCK-END\n```\nKey features: Performs well on nmse (43.2172), Performs well on nmae (8.6380), Performs well on r2 (-42.2172), Performs well on combined_score (0.0226)\n\n\n### Program 3 (Score: 3.5631)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nExtended quadratic\u2013interaction log\u2010law for LLM training loss:\n  log\u2009L = a0\n        + a1\u00b7log(N)\n        + a2\u00b7log(D)\n        + a3\u00b7log(B)\n        + a4\u00b7[log(B)]^2\n        + a5\u00b7log(\u03b7)\n        + a6\u00b7[log(\u03b7)]^2\n        + a7\u00b7[log(\u03b7)\u00b7log(B)]\n\nwhere\n  \u03b7 = learning rate\n  B = batch size\n  D = data_size (tokens)\n  N = non_embedding_param_size\n\nWe fit via ridge\u2010regularized least squares in log\u2010space for numerical stability,\nwith no penalty on the intercept and small penalty on other coefficients.\nSupports both single and parallel parameter sets.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using the 8\u2010parameter\n    quadratic\u2013interaction log\u2010law.\n\n    Args:\n      data_points: array of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 8, or 2D array of shape (T,8)\n\n    Returns:\n      If params.ndim==1: (N,) array of predicted losses.\n      If params.ndim==2: (N,T) array of predicted losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # unpack and safe\u2010clip\n    lr   = np.clip(X[:, 0], 1e-12, None)\n    bsz  = np.clip(X[:, 1], 1e-12, None)\n    D    = np.clip(X[:, 2], 1e-12, None)\n    Np   = np.clip(X[:, 3], 1e-12, None)\n\n    # logs\n    log_lr   = np.log(lr)\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_Np   = np.log(Np)\n\n    # build design matrix (N \u00d7 8)\n    A = np.column_stack([\n        np.ones_like(log_lr),           # intercept\n        log_Np,                         # model size effect\n        log_D,                          # data size effect\n        log_bsz,                        # batch size linear\n        log_bsz**2,                     # batch size quadratic\n        log_lr,                         # learning rate linear\n        log_lr**2,                      # learning rate quadratic\n        log_lr * log_bsz               # cross term\n    ])  # shape (N,8)\n\n    p = np.asarray(params, dtype=float)\n    # Single parameter vector\n    if p.ndim == 1:\n        if p.shape[0] != 8:\n            raise ValueError(f\"Expected params length 8, got {p.shape[0]}\")\n        log_pred = A.dot(p)            # shape (N,)\n        return np.exp(log_pred)\n\n    # Multiple parameter sets in parallel\n    if p.ndim == 2:\n        T, M = p.shape\n        if M != 8:\n            raise ValueError(f\"Expected params.shape[1]==8, got {M}\")\n        log_pred = A.dot(p.T)          # shape (N,T)\n        return np.exp(log_pred)\n\n    raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 8\u2010parameter quadratic\u2013interaction log\u2010law by solving\n    a ridge\u2010regularized least squares in log\u2010space:\n\n      minimize ||A\u00b7w \u2013 log(y)||^2 + w\u1d40\u00b7R\u00b7w\n\n    where A columns =\n      [1,\n       log(N),\n       log(D),\n       log(B),\n       (log(B))^2,\n       log(\u03b7),\n       (log(\u03b7))^2,\n       log(\u03b7)\u00b7log(B)]\n\n    and R = diag([0, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb]) (no penalty on intercept).\n\n    Returns:\n      params: 1D array of length 8 = [a0, a1, \u2026, a7]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n\n    # unpack and safe\u2010clip\n    lr   = np.clip(X[:, 0], 1e-12, None)\n    bsz  = np.clip(X[:, 1], 1e-12, None)\n    D    = np.clip(X[:, 2], 1e-12, None)\n    Np   = np.clip(X[:, 3], 1e-12, None)\n    y_clipped = np.clip(y, 1e-12, None)\n\n    # logs\n    log_lr   = np.log(lr)\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_Np   = np.log(Np)\n    log_y    = np.log(y_clipped)\n\n    # build design matrix A (N \u00d7 8)\n    A = np.column_stack([\n        np.ones_like(log_lr),\n        log_Np,\n        log_D,\n        log_bsz,\n        log_bsz**2,\n        log_lr,\n        log_lr**2,\n        log_lr * log_bsz\n    ])\n\n    # normal equations\n    AtA = A.T.dot(A)                      # shape (8,8)\n    AtY = A.T.dot(log_y)                 # shape (8,)\n\n    # regularization: no penalty on intercept (index 0)\n    trace = np.trace(AtA)\n    lam   = 1e-6 * (trace / (AtA.shape[0] - 1))\n    R = np.eye(AtA.shape[0]) * lam\n    R[0, 0] = 0.0\n\n    # solve (AtA + R) w = AtY\n    params = np.linalg.solve(AtA + R, AtY)\n    return params\n# EVOLVE-BLOCK-END\n```\nKey features: Performs well on nmse (98.5771), Performs well on nmae (13.2424), Performs well on r2 (-97.5771), Performs well on combined_score (0.0100)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 3.5807)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nExtended robust power\u2010law scaling\u2010law model for LLM training loss.\nWe augment the standard power\u2010law with a quadratic term in learning\u2010rate:\n    log L \u2248 a0 \n           + a1\u00b7log(lr) \n           + a2\u00b7(log(lr))^2 \n           + a3\u00b7log(bsz) \n           + a4\u00b7log(data_size) \n           + a5\u00b7log(non_embedding_param_size)\n\nWe fit by ridge\u2010regularized IRLS in log\u2010space with Huber weights\nfor robustness and small\u2010ridge for numerical stability.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using the extended power\u2010law.\n\n    Args:\n      data_points: array of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size], all > 0.\n      params:      array of shape (6,) or (T,6), where\n                   params[...,0] = intercept a0\n                   params[...,1] = a1 (log lr term)\n                   params[...,2] = a2 (log lr)^2 term\n                   params[...,3] = a3 (log bsz term)\n                   params[...,4] = a4 (log data_size term)\n                   params[...,5] = a5 (log param_size term)\n\n    Returns:\n      preds: shape (N,) if params is (6,), or (N,T) if params is (T,6)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"data_points must be shape (N,4), got {X.shape}\")\n    # all inputs must be positive for log\n    if np.any(X <= 0):\n        raise ValueError(\"All entries of data_points must be strictly positive\")\n    # extract and log\u2010transform features\n    lr, bsz, D, P = X[:,0], X[:,1], X[:,2], X[:,3]\n    log_lr   = np.log(lr)\n    log_lr2  = log_lr * log_lr\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_P    = np.log(P)\n    # stack extended log\u2010features: shape (N,5)\n    Z = np.stack([log_lr, log_lr2, log_bsz, log_D, log_P], axis=1)\n\n    p = np.asarray(params, dtype=float)\n    # single parameter vector\n    if p.ndim == 1:\n        if p.shape[0] != 6:\n            raise ValueError(f\"params must have length 6, got {p.shape[0]}\")\n        a0 = p[0]\n        slopes = p[1:]            # (5,)\n        log_pred = a0 + Z.dot(slopes)\n        return np.exp(log_pred)\n\n    # multiple parameter sets in parallel\n    if p.ndim == 2:\n        T, M = p.shape\n        if M != 6:\n            raise ValueError(f\"params must have shape (T,6), got {p.shape}\")\n        a0 = p[:, 0]              # (T,)\n        slopes = p[:, 1:]         # (T,5)\n        # log_pred: shape (N,T)\n        log_pred = Z.dot(slopes.T) + a0[None, :]\n        return np.exp(log_pred)\n\n    raise ValueError(f\"params array must be 1D or 2D, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter extended power\u2010law by robust IRLS in log\u2010space.\n\n    Args:\n      data_points: (N,4) array of positive hyperparameters.\n      loss_values: (N,) array of positive LM losses.\n\n    Returns:\n      params: (6,) array [a0, a1, a2, a3, a4, a5].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"data_points must be shape (N,4), got {X.shape}\")\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError(\"loss_values must be shape (N,) matching data_points\")\n    # enforce positivity\n    if np.any(X <= 0) or np.any(y <= 0):\n        raise ValueError(\"All data_points and loss_values must be strictly positive\")\n\n    # build extended log\u2010feature matrix: (N,5)\n    lr, bsz, D, P = X[:,0], X[:,1], X[:,2], X[:,3]\n    log_lr   = np.log(lr)\n    log_lr2  = log_lr * log_lr\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_P    = np.log(P)\n    logX_ext = np.stack([log_lr, log_lr2, log_bsz, log_D, log_P], axis=1)\n\n    # target\n    logy = np.log(y)\n\n    N, D_ext = logX_ext.shape  # D_ext == 5\n\n    # standardize features for conditioning\n    mu    = logX_ext.mean(axis=0)\n    sigma = logX_ext.std(axis=0)\n    # guard against zero std\n    sigma[sigma < 1e-8] = 1.0\n    Zs = (logX_ext - mu[None,:]) / sigma[None,:]\n\n    # design matrix with intercept: shape (N, 1 + D_ext) = (N,6)\n    A = np.concatenate([np.ones((N,1)), Zs], axis=1)\n\n    # small ridge for numerical stability\n    alpha = 1e-6\n\n    # initial OLS solution in log\u2010space\n    ATA = A.T.dot(A) + alpha * np.eye(D_ext+1)\n    ATy = A.T.dot(logy)\n    p = np.linalg.solve(ATA, ATy)    # shape (6,)\n\n    # IRLS with Huber weighting\n    max_iter = 12\n    for _ in range(max_iter):\n        logy_pred = A.dot(p)\n        resid = logy_pred - logy\n        # Huber threshold\n        med = np.median(np.abs(resid))\n        delta = 1.345 * max(med, 1e-3)\n        # weights\n        w = np.ones_like(resid)\n        mask = np.abs(resid) > delta\n        w[mask] = delta / np.abs(resid[mask])\n        W = w[:,None]\n        ATA_w = (A * W).T.dot(A) + alpha * np.eye(D_ext+1)\n        ATy_w = (A * W).T.dot(logy)\n        p_new = np.linalg.solve(ATA_w, ATy_w)\n        if np.allclose(p, p_new, rtol=1e-6, atol=1e-6):\n            p = p_new\n            break\n        p = p_new\n\n    # recover original\u2010scale slopes and intercept\n    slopes_std = p[1:]                 # shape (5,)\n    slopes = slopes_std / sigma        # un\u2010standardized slopes\n    intercept = p[0] - np.dot(slopes_std, mu / sigma)\n\n    return np.concatenate([[intercept], slopes])\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n\n### Program D2 (Score: 3.6003)\n```python\nimport numpy as np\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LLM loss with an extended log\u2010quadratic scaling law:\n      log L = p0 \n            + p1\u00b7log(lr) + p2\u00b7log(bsz) \n            + p3\u00b7log(data_size) + p4\u00b7log(param_size)\n            + p5\u00b7[log(lr)]^2 + p6\u00b7[log(bsz)]^2\n\n    data_points: (N,4) array [lr, bsz, data_size, non_embedding_param_size]\n    params: \n      - 1D array of length 7, or\n      - 2D array of shape (T,7) for T parallel models\n    Returns:\n      - If params is 1D: (N,) array of predicted losses\n      - If params is 2D: (N,T) array of predicted losses\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n\n    # Split features\n    lr       = X[:, 0]\n    bsz      = X[:, 1]\n    data_sz  = X[:, 2]\n    param_sz = X[:, 3]\n\n    # Numerical stability eps for logs\n    eps = 1e-12\n    log_lr    = np.log(lr + eps)\n    log_bsz   = np.log(bsz + eps)\n    log_data  = np.log(data_sz + eps)\n    log_param = np.log(param_sz + eps)\n\n    # Build design matrix with quadratic lr and bsz terms\n    # phi has shape (N,7): [1, log_lr, log_bsz, log_data, log_param, log_lr^2, log_bsz^2]\n    ones     = np.ones_like(log_lr)\n    phi = np.stack([\n        ones,\n        log_lr,\n        log_bsz,\n        log_data,\n        log_param,\n        log_lr**2,\n        log_bsz**2\n    ], axis=1)\n\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        # Single parameter vector\n        if p.shape[0] != phi.shape[1]:\n            raise ValueError(f\"Expected params length {phi.shape[1]}, got {p.shape[0]}\")\n        log_pred = phi.dot(p)\n        return np.exp(log_pred)\n\n    elif p.ndim == 2:\n        # Multiple parameter sets\n        T, P = p.shape\n        if P != phi.shape[1]:\n            raise ValueError(f\"Expected params.shape[1]=={phi.shape[1]}, got {P}\")\n        # phi @ p.T -> (N,T)\n        log_pred = phi.dot(p.T)\n        return np.exp(log_pred)\n\n    else:\n        raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the extended log\u2010quadratic scaling law via ridge\u2010regularized\n    least squares in log\u2010space:\n      minimize || phi @ p - log(loss) ||^2 + \u03bb\u00b7||p_excluding_intercept||^2\n\n    Returns:\n      params: 1D array of length 7\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n\n    # Split and log-transform features\n    eps = 1e-12\n    lr       = X[:, 0]\n    bsz      = X[:, 1]\n    data_sz  = X[:, 2]\n    param_sz = X[:, 3]\n\n    log_lr    = np.log(lr + eps)\n    log_bsz   = np.log(bsz + eps)\n    log_data  = np.log(data_sz + eps)\n    log_param = np.log(param_sz + eps)\n    logy      = np.log(y + eps)\n\n    # Build design matrix phi (N,7)\n    ones     = np.ones_like(log_lr)\n    phi = np.stack([\n        ones,\n        log_lr,\n        log_bsz,\n        log_data,\n        log_param,\n        log_lr**2,\n        log_bsz**2\n    ], axis=1)\n\n    # Ridge regularization parameter (scaled by N)\n    N, F = phi.shape\n    lam = 1e-6 * N\n\n    # Build normal equations: (phi^T phi + lam\u00b7R) p = phi^T logy\n    # R is identity but do not regularize intercept term\n    R = np.eye(F, dtype=float)\n    R[0, 0] = 0.0\n\n    A = phi.T.dot(phi) + lam * R\n    b = phi.T.dot(logy)\n\n    # Solve for params\n    params = np.linalg.solve(A, b)\n    return params\n# EVOLVE-BLOCK-END\n```\nKey features: Alternative approach to nmse, Alternative approach to nmae\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9315, Type: High-Performer)\n```python\nimport numpy as np\n\n# Try to import a robust optimizer; otherwise fall back to log\u2010linear with floor init\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict language model loss from hyperparameters using a floor\u2010augmented\n    power\u2010law in log\u2010space:\n      log_pred = logsumexp(intercept + w\u00b7logX, u_floor)\n      loss = exp(log_pred)\n    where u_floor = log(floor) ensures floor >= 0.\n    Inputs:\n      data_points: np.ndarray (N,4) of positive hyperparameters\n                   columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:      shape (5,) or (T,5):\n                   params[...,0]    = intercept\n                   params[...,1:5]  = weights w_i\n                   params[...,5]    = u_floor (log floor)\n    Returns:\n      preds: shape (N,) if params is (5,), or (N,T) if params is (T,5)\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # Avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # (N,4)\n\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        # our model has 6 parameters: intercept + 4 weights + 1 u_floor\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n    intercept = p[:, 0]           # (T,)\n    weights   = p[:, 1:5]         # (T,4)\n    u_floor   = p[:, 5]           # (T,)\n\n    # compute log_term = intercept + w\u00b7logX, shape (N,T)\n    log_term = logX.dot(weights.T) + intercept[None, :]\n    # pred_log = logsumexp(log_term, u_floor)\n    # numpy.logaddexp does exactly log(exp(a)+exp(b))\n    pred_log = np.logaddexp(log_term, u_floor[None, :])\n    loss_pred = np.exp(pred_log)  # (N,T)\n\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the floor\u2010augmented power\u2010law model by minimizing MSE in log\u2010loss domain:\n      minimize mean((log_pred - log(y))^2) + \u03bb||params||^2\n    where\n      log_pred = logaddexp(intercept + w\u00b7logX, u_floor)\n    Returns:\n      params: np.ndarray shape (6,) = [intercept, w1, w2, w3, w4, u_floor]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # prepare log\u2010features and log\u2010targets\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)   # (N,4)\n    logy = np.log(yc)   # (N,)\n\n    N = X.shape[0]\n    F = 4\n\n    # initial linear fit on log\u2010loss to get intercept and weights\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    w_lin, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    intercept0 = w_lin[0]\n    weights0   = w_lin[1:]\n    # initialize floor small fraction of min loss\n    y_min = np.min(yc)\n    floor0 = max(y_min * 0.05, eps)\n    u0 = np.log(floor0)\n    p0 = np.concatenate([[intercept0], weights0, [u0]])  # shape (6,)\n\n    # objective: MSE in log domain + ridge on all params\n    lambda_reg = 1e-3\n    def _objective(p):\n        # p: (6,)\n        b = p[0]\n        w = p[1:1+F]\n        uf = p[1+F]\n        # log_term: (N,)\n        lt = logX.dot(w) + b\n        pll = np.logaddexp(lt, uf)         # (N,)\n        res = pll - logy\n        mse = np.mean(res*res)\n        reg = lambda_reg * np.sum(p*p)\n        return mse + reg\n\n    # optimize\n    if _HAVE_MINIMIZE:\n        res = minimize(_objective, p0,\n                       method='L-BFGS-B',\n                       options={'maxiter':500, 'ftol':1e-9})\n        if res.success:\n            return res.x\n        else:\n            return p0\n    else:\n        # fallback: no optimizer\n        return p0\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (4.545), Excellent nmae (2.546)\n\n\n### Inspiration 2 (Score: 3.7843, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nImproved scaling\u2010law model for LLM training loss using a log\u2010linear (power\u2010law) form.\nPredicted loss: L = exp(a0) * lr^{a1} * bsz^{a2} * data_size^{a3} * param_size^{a4}\n\nWe fit this by linear least squares in log\u2010space:\n  log(L) = a0 + a1*log(lr) + a2*log(bsz) + a3*log(data_size) + a4*log(param_size)\n\nThis yields a simple, stable, and efficient 5-parameter model.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters via a log-linear scaling law.\n\n    Inputs:\n      data_points: array of shape (N,4) with columns [lr, bsz, data_size, non_embedding_param_size]\n      params:      array of shape (5,) or (T,5):\n                     params[...,0] = intercept a0\n                     params[...,1:] = exponents [a1, a2, a3, a4]\n\n    Returns:\n      If params is (5,), returns array of shape (N,) with predicted losses.\n      If params is (T,5), returns array of shape (N,T) with predictions per parameter set.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n\n    # Avoid log(0) by clipping to a small positive epsilon\n    eps = 1e-12\n    X_clipped = np.clip(X, eps, None)\n    logX = np.log(X_clipped)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    # Single parameter vector\n    if p.ndim == 1:\n        if p.shape[0] != 5:\n            raise ValueError(f\"Expected params of length 5, got {p.shape[0]}\")\n        a0 = p[0]\n        slopes = p[1:]              # shape (4,)\n        log_pred = a0 + logX.dot(slopes)  # shape (N,)\n        return np.exp(log_pred)\n\n    # Multiple parameter sets\n    if p.ndim == 2:\n        T, P = p.shape\n        if P != 5:\n            raise ValueError(f\"Expected params.shape[1]==5, got {P}\")\n        a0 = p[:, 0]                # shape (T,)\n        slopes = p[:, 1:]           # shape (T,4)\n        # Compute log predictions: (N, T)\n        log_pred = logX.dot(slopes.T) + a0[None, :]\n        return np.exp(log_pred)\n\n    raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 5-parameter log-linear model via least-squares in log-space:\n      minimize || [1, log(lr), log(bsz), log(data_size), log(param_size)] @ params - log(loss) ||^2\n\n    Inputs:\n      data_points: (N,4) array of hyperparameters\n      loss_values: (N,) array of observed LM losses\n\n    Returns:\n      params: 1D array of length 5 = [a0, a1, a2, a3, a4]\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n\n    # Clip to avoid log(0)\n    eps = 1e-12\n    X_clipped = np.clip(X, eps, None)\n    y_clipped = np.clip(y, eps, None)\n\n    # Build design matrix A = [1 | log(lr) log(bsz) log(data_size) log(param_size)]\n    logX = np.log(X_clipped)          # shape (N,4)\n    logy = np.log(y_clipped)          # shape (N,)\n    N = X.shape[0]\n    A = np.concatenate([np.ones((N, 1)), logX], axis=1)  # shape (N,5)\n\n    # Solve linear least squares: minimize ||A @ params - logy||^2\n    params, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    return params  # shape (5,)\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (112.988), Excellent nmae (14.128)\n\n\n### Inspiration 3 (Score: 3.5631, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nExtended quadratic\u2013interaction log\u2010law for LLM training loss:\n  log\u2009L = a0\n        + a1\u00b7log(N)\n        + a2\u00b7log(D)\n        + a3\u00b7log(B)\n        + a4\u00b7[log(B)]^2\n        + a5\u00b7log(\u03b7)\n        + a6\u00b7[log(\u03b7)]^2\n        + a7\u00b7[log(\u03b7)\u00b7log(B)]\n\nwhere\n  \u03b7 = learning rate\n  B = batch size\n  D = data_size (tokens)\n  N = non_embedding_param_size\n\nWe fit via ridge\u2010regularized least squares in log\u2010space for numerical stability,\nwith no penalty on the intercept and small penalty on other coefficients.\nSupports both single and parallel parameter sets.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict LM loss from hyperparameters using the 8\u2010parameter\n    quadratic\u2013interaction log\u2010law.\n\n    Args:\n      data_points: array of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size]\n      params:      1D array of length 8, or 2D array of shape (T,8)\n\n    Returns:\n      If params.ndim==1: (N,) array of predicted losses.\n      If params.ndim==2: (N,T) array of predicted losses.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # unpack and safe\u2010clip\n    lr   = np.clip(X[:, 0], 1e-12, None)\n    bsz  = np.clip(X[:, 1], 1e-12, None)\n    D    = np.clip(X[:, 2], 1e-12, None)\n    Np   = np.clip(X[:, 3], 1e-12, None)\n\n    # logs\n    log_lr   = np.log(lr)\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_Np   = np.log(Np)\n\n    # build design matrix (N \u00d7 8)\n    A = np.column_stack([\n        np.ones_like(log_lr),           # intercept\n        log_Np,                         # model size effect\n        log_D,                          # data size effect\n        log_bsz,                        # batch size linear\n        log_bsz**2,                     # batch size quadratic\n        log_lr,                         # learning rate linear\n        log_lr**2,                      # learning rate quadratic\n        log_lr * log_bsz               # cross term\n    ])  # shape (N,8)\n\n    p = np.asarray(params, dtype=float)\n    # Single parameter vector\n    if p.ndim == 1:\n        if p.shape[0] != 8:\n            raise ValueError(f\"Expected params length 8, got {p.shape[0]}\")\n        log_pred = A.dot(p)            # shape (N,)\n        return np.exp(log_pred)\n\n    # Multiple parameter sets in parallel\n    if p.ndim == 2:\n        T, M = p.shape\n        if M != 8:\n            raise ValueError(f\"Expected params.shape[1]==8, got {M}\")\n        log_pred = A.dot(p.T)          # shape (N,T)\n        return np.exp(log_pred)\n\n    raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 8\u2010parameter quadratic\u2013interaction log\u2010law by solving\n    a ridge\u2010regularized least squares in log\u2010space:\n\n      minimize ||A\u00b7w \u2013 log(y)||^2 + w\u1d40\u00b7R\u00b7w\n\n    where A columns =\n      [1,\n       log(N),\n       log(D),\n       log(B),\n       (log(B))^2,\n       log(\u03b7),\n       (log(\u03b7))^2,\n       log(\u03b7)\u00b7log(B)]\n\n    and R = diag([0, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb, \u03bb]) (no penalty on intercept).\n\n    Returns:\n      params: 1D array of length 8 = [a0, a1, \u2026, a7]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n\n    # unpack and safe\u2010clip\n    lr   = np.clip(X[:, 0], 1e-12, None)\n    bsz  = np.clip(X[:, 1], 1e-12, None)\n    D    = np.clip(X[:, 2], 1e-12, None)\n    Np   = np.clip(X[:, 3], 1e-12, None)\n    y_clipped = np.clip(y, 1e-12, None)\n\n    # logs\n    log_lr   = np.log(lr)\n    log_bsz  = np.log(bsz)\n    log_D    = np.log(D)\n    log_Np   = np.log(Np)\n    log_y    = np.log(y_clipped)\n\n    # build design matrix A (N \u00d7 8)\n    A = np.column_stack([\n        np.ones_like(log_lr),\n        log_Np,\n        log_D,\n        log_bsz,\n        log_bsz**2,\n        log_lr,\n        log_lr**2,\n        log_lr * log_bsz\n    ])\n\n    # normal equations\n    AtA = A.T.dot(A)                      # shape (8,8)\n    AtY = A.T.dot(log_y)                 # shape (8,)\n\n    # regularization: no penalty on intercept (index 0)\n    trace = np.trace(AtA)\n    lam   = 1e-6 * (trace / (AtA.shape[0] - 1))\n    R = np.eye(AtA.shape[0]) * lam\n    R[0, 0] = 0.0\n\n    # solve (AtA + R) w = AtY\n    params = np.linalg.solve(AtA + R, AtY)\n    return params\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (98.577), Excellent nmae (13.242)\n\n\n### Inspiration 4 (Score: 3.7471, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling\u2010law model for LLM training loss:\n  Loss_pred = c0 + exp(a0 + a1*log(lr) + a2*log(bsz)\n                       + a3*log(data_size) + a4*log(non_embedding_param_size))\nFitting uses a two\u2010stage approach:\n  1) Linear least\u2010squares in log\u2010space (ignoring c0) for initial slopes.\n  2) Nonlinear L-BFGS-B refinement on all 6 parameters minimizing a\n     blend of MSE in output\u2010space and log\u2010space for balanced accuracy.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict lm loss from hyperparameters with an additive floor.\n\n    Args:\n      data_points: array of shape (N,4) with columns\n                   [lr, bsz, data_size, non_embedding_param_size].\n      params:      1D array of length 6 or 2D array of shape (T,6):\n                   params[...,0] = c0 (additive floor, >= 0)\n                   params[...,1] = a0 (log\u2010scale intercept)\n                   params[...,2:] = [a1,a2,a3,a4] (slopes for log\u2010features)\n\n    Returns:\n      If params is 1D, returns (N,) array of predictions.\n      If params is 2D (T,6), returns (N,T) array of predictions.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[None, :]\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 columns, got {X.shape[1]}\")\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.shape[1] != 6:\n        raise ValueError(f\"Expected params with 6 elements, got {p.shape[1]}\")\n    # avoid log(0)\n    eps = 1e-12\n    logX = np.log(np.clip(X, eps, None))    # (N,4)\n    c0 = p[:, 0]                            # (T,)\n    a0 = p[:, 1]                            # (T,)\n    slopes = p[:, 2:]                       # (T,4)\n    # compute log\u2010prediction and final output\n    log_pred = logX.dot(slopes.T) + a0[None, :]  # (N,T)\n    output = c0[None, :] + np.exp(log_pred)      # (N,T)\n    # if single parameter set, return 1D\n    return output.ravel() if params.ndim == 1 else output\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter scaling law by two\u2010stage optimization:\n      1) Linear least squares in log\u2010space (ignoring c0) to get [a0,a1..a4].\n      2) L-BFGS-B refinement on [c0,a0,a1..a4], minimizing a blend of\n         MSE in y and MSE in log(y) for balanced errors.\n\n    Returns:\n      params: 1D array of length 6 = [c0, a0, a1, a2, a3, a4].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(\"data_points must be shape (N,4)\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"Mismatch between data_points and loss_values lengths\")\n    # Clip to avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    y_clipped = np.clip(y, eps, None)\n    logX = np.log(Xc)     # (N,4)\n    logy = np.log(y_clipped)\n    # Stage 1: solve linear LS for intercept+slopes (no c0)\n    N = X.shape[0]\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    p_ls, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    # p_ls = [a0, a1, a2, a3, a4]\n    # initialize c0 as small fraction of min loss\n    c0_init = max(0.0, 0.1 * y_clipped.min())\n    init = np.concatenate([[c0_init], p_ls])  # shape (6,)\n    # Stage 2: nonlinear refinement\n    alpha = 0.5  # blend factor between y-space and log-space MSE\n    def objective(p):\n        c0, a0 = p[0], p[1]\n        slopes = p[2:]\n        log_pred = a0 + logX.dot(slopes)\n        pred = c0 + np.exp(log_pred)\n        # MSE in y-space\n        err_y = pred - y_clipped\n        mse_y = np.mean(err_y * err_y)\n        # MSE in log-space\n        log_pred_clip = np.log(np.clip(pred, eps, None))\n        err_l = log_pred_clip - logy\n        mse_l = np.mean(err_l * err_l)\n        return alpha * mse_y + (1 - alpha) * mse_l\n    # bounds: c0>=0, slopes limited to avoid runaway\n    bounds = [(0.0, None), (None, None)] + [(-10.0, 10.0)] * 4\n    res = minimize(objective, init, method='L-BFGS-B', bounds=bounds,\n                   options={'ftol':1e-9, 'gtol':1e-6, 'maxiter':5000})\n    p_opt = res.x if res.success else init\n    return p_opt\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (110.645), Excellent nmae (13.979)\n\n\n### Inspiration 5 (Score: 3.7843, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nImproved scaling\u2010law model for LLM training loss using a log\u2010linear (power\u2010law) form\nPredicted loss L = exp(a0) * lr^{a1} * bsz^{a2} * data_size^{a3} * param_size^{a4}\nFitting is done via a linear least\u2010squares in log\u2010space for stability and speed.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    params: 1D array of length 5 (or 2D array of shape (T,5) for T parallel models)\n            params[0] = intercept a0\n            params[1:] = slopes [a1, a2, a3, a4]\n    Returns: \n      If params is 1D, returns (N,) array of predicted losses.\n      If params is 2D (T,5), returns (N,T) array of predicted losses for each param set.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n    # Transform features to log\u2010space\n    logX = np.log(X)\n    p = np.asarray(params, dtype=float)\n    \n    # Single parameter set\n    if p.ndim == 1:\n        if p.shape[0] != 5:\n            raise ValueError(f\"Expected params of length 5, got length {p.shape[0]}\")\n        a0 = p[0]\n        slopes = p[1:]            # length 4\n        log_pred = a0 + logX.dot(slopes)\n        return np.exp(log_pred)\n    \n    # Multiple parameter sets in parallel (shape (T,5))\n    elif p.ndim == 2:\n        T, P = p.shape\n        if P != 5:\n            raise ValueError(f\"Expected params.shape[1]==5, got {P}\")\n        a0 = p[:, 0]              # shape (T,)\n        slopes = p[:, 1:]         # shape (T,4)\n        # logX @ slopes.T -> (N,T)\n        log_pred = logX.dot(slopes.T) + a0[None, :]\n        return np.exp(log_pred)\n    \n    else:\n        raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the 5-parameter log\u2010linear model by solving a linear least\u2010squares\n    problem in log\u2010space:\n      minimize || [1, log(lr), log(bsz), log(data_size), log(param_size)] @ params - log(loss) ||^2\n    Returns:\n      params: 1D array of length 5 = [a0, a1, a2, a3, a4]\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n    \n    # Small epsilon to avoid log(0)\n    eps = 1e-12\n    logX = np.log(X)\n    logy = np.log(y + eps)\n    \n    # Build design matrix A = [1 | log(lr) log(bsz) log(data_size) log(param_size)]\n    N = X.shape[0]\n    A = np.concatenate([np.ones((N, 1)), logX], axis=1)  # shape (N,5)\n    \n    # Solve least squares: minimize ||A @ p - logy||^2\n    params, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    # params is shape (5,)\n    return params\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Excellent nmse (112.988), Excellent nmae (14.128)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nImproved scaling\u2010law model for LLM training loss using a log\u2010linear (power\u2010law) form\nPredicted loss L = exp(a0) * lr^{a1} * bsz^{a2} * data_size^{a3} * param_size^{a4}\nFitting is done via a linear least\u2010squares in log\u2010space for stability and speed.\n\"\"\"\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    data_points: (N,4) array with columns [lr, bsz, data_size, non_embedding_param_size]\n    params: 1D array of length 5 (or 2D array of shape (T,5) for T parallel models)\n            params[0] = intercept a0\n            params[1:] = slopes [a1, a2, a3, a4]\n    Returns: \n      If params is 1D, returns (N,) array of predicted losses.\n      If params is 2D (T,5), returns (N,T) array of predicted losses for each param set.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n    # Transform features to log\u2010space\n    logX = np.log(X)\n    p = np.asarray(params, dtype=float)\n    \n    # Single parameter set\n    if p.ndim == 1:\n        if p.shape[0] != 5:\n            raise ValueError(f\"Expected params of length 5, got length {p.shape[0]}\")\n        a0 = p[0]\n        slopes = p[1:]            # length 4\n        log_pred = a0 + logX.dot(slopes)\n        return np.exp(log_pred)\n    \n    # Multiple parameter sets in parallel (shape (T,5))\n    elif p.ndim == 2:\n        T, P = p.shape\n        if P != 5:\n            raise ValueError(f\"Expected params.shape[1]==5, got {P}\")\n        a0 = p[:, 0]              # shape (T,)\n        slopes = p[:, 1:]         # shape (T,4)\n        # logX @ slopes.T -> (N,T)\n        log_pred = logX.dot(slopes.T) + a0[None, :]\n        return np.exp(log_pred)\n    \n    else:\n        raise ValueError(f\"params must be 1D or 2D array, got ndim={p.ndim}\")\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the 5-parameter log\u2010linear model by solving a linear least\u2010squares\n    problem in log\u2010space:\n      minimize || [1, log(lr), log(bsz), log(data_size), log(param_size)] @ params - log(loss) ||^2\n    Returns:\n      params: 1D array of length 5 = [a0, a1, a2, a3, a4]\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    if X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points with 4 features, got shape {X.shape}\")\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"Number of loss values must match number of data points\")\n    \n    # Small epsilon to avoid log(0)\n    eps = 1e-12\n    logX = np.log(X)\n    logy = np.log(y + eps)\n    \n    # Build design matrix A = [1 | log(lr) log(bsz) log(data_size) log(param_size)]\n    N = X.shape[0]\n    A = np.concatenate([np.ones((N, 1)), logX], axis=1)  # shape (N,5)\n    \n    # Solve least squares: minimize ||A @ p - logy||^2\n    params, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    # params is shape (5,)\n    return params\n# EVOLVE-BLOCK-END\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["```python\nimport numpy as np\ntry:\n    from scipy.optimize import minimize\n    _HAVE_MINIMIZE = True\nexcept ImportError:\n    _HAVE_MINIMIZE = False\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Floor\u2010augmented power\u2010law scaling:\n      log_pred = logaddexp(a0 + w\u00b7logX, u_floor)\n      loss_pred = exp(log_pred)\n\n    Inputs:\n      data_points: np.ndarray (N,4) of positive hyperparameters\n                   columns = [lr, bsz, data_size, non_embedding_param_size]\n      params:      array of shape (6,) or (T,6):\n                     params[...,0] = intercept a0\n                     params[...,1:5] = weights w for [log(lr), log(bsz), log(data_size), log(param_size)]\n                     params[...,5] = u_floor (log of loss floor)\n    Returns:\n      If params is (6,), returns (N,) array of predictions.\n      If params is (T,6), returns (N,T) array of predictions.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    # avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    logX = np.log(Xc)  # shape (N,4)\n\n    p = np.asarray(params, dtype=float)\n    if p.ndim == 1:\n        p = p[None, :]\n    if p.ndim != 2 or p.shape[1] != 6:\n        raise ValueError(f\"Expected params shape (6,) or (T,6), got {p.shape}\")\n\n    a0     = p[:, 0]       # (T,)\n    weights= p[:, 1:5]     # (T,4)\n    u_floor= p[:, 5]       # (T,)\n\n    # compute a0 + w\u00b7logX \u2192 shape (N,T)\n    log_term = logX.dot(weights.T) + a0[None, :]\n    # apply floor in log\u2010space\n    pred_log = np.logaddexp(log_term, u_floor[None, :])\n    loss_pred = np.exp(pred_log)  # (N,T)\n\n    # if single parameter set, return (N,)\n    if loss_pred.shape[1] == 1:\n        return loss_pred[:, 0]\n    return loss_pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 6\u2010parameter floor\u2010augmented power\u2010law by minimizing\n      MSE(log_pred, log(y)) + \u03bb\u00b7||w||^2\n    in log\u2010domain with L-BFGS-B, bounding the floor \u2264 min(log y).\n    Returns params = [a0, w1, w2, w3, w4, u_floor].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float).ravel()\n    if X.ndim != 2 or X.shape[1] != 4:\n        raise ValueError(f\"Expected data_points shape (N,4), got {X.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"data_points and loss_values must have same length\")\n\n    # clip to avoid log(0)\n    eps = 1e-12\n    Xc = np.clip(X, eps, None)\n    yc = np.clip(y, eps, None)\n    logX = np.log(Xc)   # (N,4)\n    logy = np.log(yc)   # (N,)\n\n    N = X.shape[0]\n    # 1) Linear LS init for a0, w\n    A = np.concatenate([np.ones((N,1)), logX], axis=1)  # (N,5)\n    lin_p, *_ = np.linalg.lstsq(A, logy, rcond=None)\n    a0_0 = lin_p[0]\n    w0   = lin_p[1:]   # (4,)\n\n    # 2) Floor init: small fraction of min observed loss\n    y_min = np.min(yc)\n    floor0 = max(y_min * 0.05, eps)\n    u0 = np.log(floor0)\n\n    p0 = np.concatenate([[a0_0], w0, [u0]])  # shape (6,)\n\n    # 3) Objective: MSE in log\u2010space + ridge on weights\n    lambda_reg = 1e-3\n    def _obj(p):\n        a0 = p[0]\n        w  = p[1:5]\n        uf = p[5]\n        lt = logX.dot(w) + a0              # (N,)\n        pll = np.logaddexp(lt, uf)        # (N,)\n        res = pll - logy\n        mse = np.mean(res * res)\n        reg = lambda_reg * np.sum(w * w)\n        return mse + reg\n\n    # 4) Bounds: u_floor \u2264 min(log y)\n    min_logy = np.min(logy)\n    bounds = [(None, None)] * 5 + [(None, min_logy)]\n\n    # 5) Optimize\n    if _HAVE_MINIMIZE:\n        res = minimize(_obj, p0,\n                       method='L-BFGS-B',\n                       bounds=bounds,\n                       options={'maxiter':500, 'ftol':1e-9})\n        if res.success:\n            return res.x\n    # fallback\n    return p0\n# EVOLVE-BLOCK-END\n```"]}}}